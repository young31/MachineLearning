# 미로.ipynb
- '야 그냥 이런거 인공지능 돌리지 누가 짜고 있냐'에 대한 구현
- q-learning 응용해야 작업하였음
- 가치행렬 갱신과정에서 몇 가지 잡음이 있었음
- 재현성에 대한 부분 확인 안됨 >> copy파일에서 확인
## q-learning
- 100회 반복 시 2.29s 확인 >> print 없으니 0.11s
- 100회 중 79회는 완전히 바로 찾아감
- 첫 회 초반 회차에서만 탐색과정이 작용하고 나머지는 빠르게 수렴 

## SARSA
- q-leaerning보다 느린 완료 시간
- 수렴하는 모습도 거의 보이지 않음
- 그만큼 다양한 경로를 탐색하기 좋은 방법

- 큰 환경에서는 수렴하지 않는 현상 발견
  - 찾아가긴하나 그 경우의 수가 수렴하지 않음
  - 탐색과정을 통해 학습되기전에 끝내서인 것으로 추정

--------- 예정 -------
# game에 적용
(오목까지?)
## 틱택토
- negamax 법 사용x 상태
- 실습하면서 상태가치를 승무패로 나눈 부분이 차이가 나는 부분인데 이부분에서 문제가 생긴 것일지도?
- 아니면 상대수를 고려해서 알고리즘을 짜야 하나?

- MinMax
  - 완전탐색인데 생각보다 성적이 확연하지는 않음
  - 알파베타법으로 시간 단축 적용 가능
  
- MCS(Monte Carlo Simulation)
  - 랜덤으로 수를 놓아 가면서 시뮬레이션 후 가장 값이 점수를 많이 얻은 수를 선택
  - 시뮬레이션 횟수에 따라서 성능이 달라질 수 있으며 시간도 이에 의존함
  - 랜덤과 비교하면 선수시 99%, 후수시 90%정도의 승률을 보여줌
  - MinMax법에 비해서 수행속도도 압도적인 
  - MCT로 확장 가능(알파제로에서 사용)

# 복잡한 문제 적용(주식추천 등)
- 학습 기법 + 탐색 기법을 적용하여 인공지능을 만들어 보자!

# 로봇어드바이저
- DQN기술을 활용하여 한 종목에 대하여 살지 말지를 판단하는 모형을 구성해 봄
- output = [buy, sell]
- 모형 구성시 분류문제로 바꿔 풀기(softmax, crossentropy 사용)
- 아직 완전히 수렴하지 않는 문제
  - 초기 값에 따라 지역해 (all sell) 문제
  - 과적합 문제 발생(위의 결과와 연결)
  
