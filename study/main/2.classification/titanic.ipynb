{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## 기본 library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "## model library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from keras import models, layers, optimizers\n",
    "## helper library\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "data = pd.read_csv('../1.clustering/titanic.csv')\n",
    "\n",
    "target = data['Survived']\n",
    "data = data.drop(['PassengerId', 'Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data.copy()\n",
    "categorical = [\n",
    "    'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'] = pd.DataFrame(MinMaxScaler().fit_transform(data['Age'].values.reshape(-1,1)))\n",
    "data['Fare'] = pd.DataFrame(StandardScaler().fit_transform(data['Fare'].values.reshape(-1,1)))\n",
    "\n",
    "lbe = LabelEncoder()\n",
    "data['Sex'] = lbe.fit_transform(data['Sex'])\n",
    "sex = lbe.classes_\n",
    "data['Embarked'] = lbe.fit_transform(data['Embarked'])\n",
    "emb = lbe.classes_\n",
    "\n",
    "for c in categorical:\n",
    "    data = pd.concat([data, pd.get_dummies(data[c], prefix=c)], axis=1)\n",
    "    data = data.drop(c, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799265306122449\n"
     ]
    }
   ],
   "source": [
    "## logistic\n",
    "acc = 0\n",
    "for tr_idx, val_idx in kfold.split(train_X):\n",
    "    tr_X, tr_y = train_X.iloc[tr_idx, :], train_y.iloc[tr_idx]\n",
    "    val_X, val_y = train_X.iloc[val_idx, :], train_y.iloc[val_idx]\n",
    "    lr = LogisticRegression(random_state=0).fit(tr_X, tr_y)\n",
    "    acc += accuracy_score(val_y, lr.predict(val_X))/10\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7992653061224491\n"
     ]
    }
   ],
   "source": [
    "## 조금 더 간단히 할 수 있음\n",
    "lr  = LogisticRegression(random_state=0)\n",
    "acc = cross_val_score(lr, train_X, train_y, cv=kfold, n_jobs=1, scoring='accuracy')\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## knn\n",
    "opt = []\n",
    "for n in range(2, 30):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    acc = []\n",
    "    for tr_idx, val_idx in kfold.split(train_X):\n",
    "        tr_X, tr_y = train_X.iloc[tr_idx, :], train_y.iloc[tr_idx]\n",
    "        val_X, val_y = train_X.iloc[val_idx, :], train_y.iloc[val_idx]\n",
    "        knn.fit(tr_X, tr_y)\n",
    "        acc.append(accuracy_score(val_y, knn.predict(val_X)))\n",
    "    opt.append(np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7811020408163265, 0.8093061224489796, 0.7791836734693878, 0.7951428571428572, 0.7952653061224491, 0.7953469387755102, 0.7973469387755102, 0.7953061224489797, 0.8033061224489797, 0.7952244897959184, 0.7972244897959184, 0.793265306122449, 0.7892653061224489, 0.7933469387755101, 0.8053469387755102, 0.8053877551020407, 0.7933061224489796, 0.7973469387755102, 0.7893469387755102, 0.8013061224489796, 0.7853469387755101, 0.7953877551020407, 0.7913877551020408, 0.7953061224489797, 0.7893469387755102, 0.7892653061224489, 0.7892244897959183, 0.791265306122449]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(opt)\n",
    "print(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## knn's n_neighbor = 3\n",
    "knn = KNeighborsClassifier(n_neighbors=3).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7832653061224489\n"
     ]
    }
   ],
   "source": [
    "## RF with no paramter tuning\n",
    "acc = 0\n",
    "for tr_idx, val_idx in kfold.split(train_X):\n",
    "    tr_X, tr_y = train_X.iloc[tr_idx, :], train_y.iloc[tr_idx]\n",
    "    val_X, val_y = train_X.iloc[val_idx, :], train_y.iloc[val_idx]\n",
    "    rf = RandomForestClassifier(random_state=0).fit(tr_X, tr_y)\n",
    "    acc += accuracy_score(val_y, rf.predict(val_X))/10\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=0).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## xgboost\n",
    "xgb_bounds = {\n",
    "    'max_depth': (3, 12),\n",
    "    'subsample': (0.3, 1),\n",
    "    'colsample_bytree': (0.3, 1),\n",
    "    'reg_alpha': (0.1, 3),\n",
    "    'reg_lamda': (0.1, 3)\n",
    "}\n",
    "\n",
    "best_ns = []\n",
    "\n",
    "def bayes_xgb(max_depth, subsample, colsample_bytree, reg_alpha, reg_lamda):\n",
    "    global best_ns\n",
    "    params = {\n",
    "        'n_estimators': 1000,\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 0,\n",
    "        'n_jobs': -1,\n",
    "        'max_depth': int(max_depth),\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lamda': reg_lamda\n",
    "    }\n",
    "    \n",
    "    acc = 0\n",
    "    best_n = 0\n",
    "    for tr_idx, val_idx in kfold.split(train_X):\n",
    "        tr_X, tr_y = train_X.iloc[tr_idx, :], train_y.iloc[tr_idx]\n",
    "        val_X, val_y = train_X.iloc[val_idx, :], train_y.iloc[val_idx]\n",
    "        clf = XGBClassifier(**params).fit(tr_X, tr_y, eval_metric = 'error', eval_set=[[val_X, val_y]], early_stopping_rounds=100, verbose=0)\n",
    "        acc += accuracy_score(val_y, clf.predict(val_X))/10\n",
    "        best_n += clf.best_iteration/10\n",
    "    best_ns.append(best_n)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | max_depth | reg_alpha | reg_lamda | subsample |\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (0.6841694527491273, 9.436704297351774, 1.8480137906077674, 1.680161230691001, 0.5965583595372332)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-e16bd8b20745>\u001b[0m in \u001b[0;36mbayes_xgb\u001b[1;34m(max_depth, subsample, colsample_bytree, reg_alpha, reg_lamda)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mtr_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtr_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mbest_n\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1109\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1110\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = BayesianOptimization(bayes_xgb, xgb_bounds, random_state=0)\n",
    "\n",
    "init_points = 10\n",
    "n_iter = 20\n",
    "\n",
    "optimizer.maximize(init_points=init_points, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('best_params.bin', 'rb') as f:\n",
    "    best_params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-87e387590966>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'params'"
     ]
    }
   ],
   "source": [
    "optimizer.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'n_estimators': int(best_ns[12]),\n",
    "    'booster': 'gbtree',\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 0,\n",
    "    'n_jobs': -1,\n",
    "    'max_depth': int(optimizer.max['params']['max_depth']),\n",
    "    'subsample': optimizer.max['params']['subsample'],\n",
    "    'colsample_bytree': optimizer.max['params']['colsample_bytree'],\n",
    "    'reg_alpha': optimizer.max['params']['reg_alpha'],\n",
    "    'reg_lamda': optimizer.max['params']['reg_lamda'],\n",
    "    'metric': 'error'\n",
    "}\n",
    "\n",
    "clf = XGBClassifier(**best_params).fit(train_X, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8073061224489797\n"
     ]
    }
   ],
   "source": [
    "## voting\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "v_lr = LogisticRegression(random_state=0)\n",
    "v_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "v_rf = RandomForestClassifier(max_depth=9, random_state=0)\n",
    "\n",
    "vclf = VotingClassifier(estimators=[\n",
    "    ('lr', v_lr),\n",
    "    ('knn', v_knn),\n",
    "    ('rf', v_rf)\n",
    "], voting='soft')\n",
    "\n",
    "acc = 0\n",
    "for tr_idx, val_idx in kfold.split(train_X):\n",
    "    tr_X, tr_y = train_X.iloc[tr_idx, :], train_y.iloc[tr_idx]\n",
    "    val_X, val_y = train_X.iloc[val_idx, :], train_y.iloc[val_idx]\n",
    "    vclf.fit(tr_X, tr_y)\n",
    "    acc += accuracy_score(val_y, vclf.predict(val_X))/10\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개별 모델들과 앙상블모형을 통틀어 xgboost가 가장 높은 성능을 보여줄 것으로 기대된다.  \n",
    "실제 test셋의 결과는 어떨까?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.7710280373831776\n",
      "knn: 0.7523364485981309\n",
      "rf: 0.7663551401869159\n",
      "xgboost: 0.780373831775701\n",
      "voting: 0.7897196261682243\n"
     ]
    }
   ],
   "source": [
    "print('lr:', accuracy_score(test_y, lr.predict(test_X)))\n",
    "print('knn:', accuracy_score(test_y, knn.predict(test_X)))\n",
    "print('rf:', accuracy_score(test_y, rf.predict(test_X)))\n",
    "print('xgboost:', accuracy_score(test_y, clf.predict(test_X)))\n",
    "print('voting:', accuracy_score(test_y, vclf.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nn 및 stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nn\n",
    "input_layer = layers.Input(shape=((train_X.shape[1], )))\n",
    "\n",
    "layer1 = layers.Dense(64, activation='relu')(input_layer)\n",
    "# drop1 = layers.Dropout(0.1)(layer1)\n",
    "layer2 = layers.Dense(32, activation='relu')(layer1)\n",
    "\n",
    "out_layer = layers.Dense(1, activation='sigmoid')(layer2)\n",
    "\n",
    "model = models.Model(input_layer, out_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 150 samples\n",
      "Epoch 1/200\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.7098 - acc: 0.4569 - val_loss: 0.6548 - val_acc: 0.7067\n",
      "Epoch 2/200\n",
      "348/348 [==============================] - 0s 103us/step - loss: 0.6333 - acc: 0.6695 - val_loss: 0.5910 - val_acc: 0.7400\n",
      "Epoch 3/200\n",
      "348/348 [==============================] - 0s 102us/step - loss: 0.5868 - acc: 0.7184 - val_loss: 0.5537 - val_acc: 0.7600\n",
      "Epoch 4/200\n",
      "348/348 [==============================] - 0s 102us/step - loss: 0.5500 - acc: 0.7299 - val_loss: 0.5254 - val_acc: 0.7467\n",
      "Epoch 5/200\n",
      "348/348 [==============================] - 0s 105us/step - loss: 0.5150 - acc: 0.7730 - val_loss: 0.5074 - val_acc: 0.7733\n",
      "Epoch 6/200\n",
      "348/348 [==============================] - 0s 100us/step - loss: 0.4831 - acc: 0.7960 - val_loss: 0.4868 - val_acc: 0.7733\n",
      "Epoch 7/200\n",
      "348/348 [==============================] - 0s 104us/step - loss: 0.4557 - acc: 0.8103 - val_loss: 0.4694 - val_acc: 0.8067\n",
      "Epoch 8/200\n",
      "348/348 [==============================] - 0s 105us/step - loss: 0.4363 - acc: 0.8132 - val_loss: 0.4607 - val_acc: 0.7667\n",
      "Epoch 9/200\n",
      "348/348 [==============================] - 0s 103us/step - loss: 0.4202 - acc: 0.8103 - val_loss: 0.4638 - val_acc: 0.7667\n",
      "Epoch 10/200\n",
      "348/348 [==============================] - 0s 112us/step - loss: 0.4099 - acc: 0.8132 - val_loss: 0.4619 - val_acc: 0.7733\n",
      "Epoch 11/200\n",
      "348/348 [==============================] - 0s 109us/step - loss: 0.3974 - acc: 0.8333 - val_loss: 0.4553 - val_acc: 0.7733\n",
      "Epoch 12/200\n",
      "348/348 [==============================] - 0s 109us/step - loss: 0.3929 - acc: 0.8190 - val_loss: 0.4568 - val_acc: 0.7733\n",
      "Epoch 13/200\n",
      "348/348 [==============================] - 0s 112us/step - loss: 0.3825 - acc: 0.8362 - val_loss: 0.4610 - val_acc: 0.7600\n",
      "Epoch 14/200\n",
      "348/348 [==============================] - 0s 104us/step - loss: 0.3760 - acc: 0.8391 - val_loss: 0.4607 - val_acc: 0.7733\n",
      "Epoch 15/200\n",
      "348/348 [==============================] - 0s 106us/step - loss: 0.3697 - acc: 0.8448 - val_loss: 0.4641 - val_acc: 0.7667\n",
      "Epoch 16/200\n",
      "348/348 [==============================] - 0s 111us/step - loss: 0.3652 - acc: 0.8391 - val_loss: 0.4654 - val_acc: 0.7867\n",
      "Epoch 17/200\n",
      "348/348 [==============================] - 0s 106us/step - loss: 0.3617 - acc: 0.8420 - val_loss: 0.4654 - val_acc: 0.7800\n",
      "Epoch 18/200\n",
      "348/348 [==============================] - 0s 106us/step - loss: 0.3566 - acc: 0.8448 - val_loss: 0.4725 - val_acc: 0.7733\n",
      "Epoch 19/200\n",
      "348/348 [==============================] - 0s 103us/step - loss: 0.3512 - acc: 0.8477 - val_loss: 0.4786 - val_acc: 0.7733\n",
      "Epoch 20/200\n",
      "348/348 [==============================] - 0s 101us/step - loss: 0.3446 - acc: 0.8506 - val_loss: 0.4708 - val_acc: 0.7867\n",
      "Epoch 21/200\n",
      "348/348 [==============================] - 0s 107us/step - loss: 0.3427 - acc: 0.8420 - val_loss: 0.4727 - val_acc: 0.7867\n",
      "Epoch 22/200\n",
      "348/348 [==============================] - 0s 104us/step - loss: 0.3363 - acc: 0.8391 - val_loss: 0.4821 - val_acc: 0.7867\n",
      "Epoch 23/200\n",
      "348/348 [==============================] - 0s 108us/step - loss: 0.3330 - acc: 0.8534 - val_loss: 0.4807 - val_acc: 0.7800\n",
      "Epoch 24/200\n",
      "348/348 [==============================] - 0s 107us/step - loss: 0.3290 - acc: 0.8448 - val_loss: 0.4776 - val_acc: 0.7800\n",
      "Epoch 25/200\n",
      "348/348 [==============================] - 0s 112us/step - loss: 0.3250 - acc: 0.8506 - val_loss: 0.4880 - val_acc: 0.7733\n",
      "Epoch 26/200\n",
      "348/348 [==============================] - 0s 107us/step - loss: 0.3219 - acc: 0.8534 - val_loss: 0.4897 - val_acc: 0.7800\n",
      "Epoch 27/200\n",
      "348/348 [==============================] - 0s 120us/step - loss: 0.3184 - acc: 0.8563 - val_loss: 0.4913 - val_acc: 0.7867\n",
      "Epoch 28/200\n",
      "348/348 [==============================] - 0s 104us/step - loss: 0.3145 - acc: 0.8592 - val_loss: 0.4909 - val_acc: 0.7867\n",
      "Epoch 29/200\n",
      "348/348 [==============================] - 0s 104us/step - loss: 0.3116 - acc: 0.8649 - val_loss: 0.4963 - val_acc: 0.7867\n",
      "Epoch 30/200\n",
      "348/348 [==============================] - 0s 110us/step - loss: 0.3170 - acc: 0.8621 - val_loss: 0.5086 - val_acc: 0.7867\n",
      "Epoch 31/200\n",
      "348/348 [==============================] - 0s 111us/step - loss: 0.3249 - acc: 0.8534 - val_loss: 0.4931 - val_acc: 0.7800\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2b78ff85d08>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y,\n",
    "          epochs=200,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 0s 50us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5076733777456195, 0.7476635575294495]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## self cv 예시\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def make_model():\n",
    "    K.clear_session()\n",
    "    input_layer = layers.Input(shape=((train_X.shape[1], )))\n",
    "\n",
    "    layer1 = layers.Dense(32, activation='relu')(input_layer)\n",
    "    drop1 = layers.Dropout(0.1)(layer1)\n",
    "    layer2 = layers.Dense(16, activation='relu')(drop1)\n",
    "    \n",
    "    out_layer = layers.Dense(1, activation='sigmoid')(layer2)\n",
    "\n",
    "    model = models.Model(input_layer, out_layer)\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 77us/step\n",
      "50/50 [==============================] - 0s 77us/step\n",
      "50/50 [==============================] - 0s 72us/step\n",
      "50/50 [==============================] - 0s 71us/step\n",
      "50/50 [==============================] - 0s 84us/step\n",
      "50/50 [==============================] - 0s 73us/step\n",
      "50/50 [==============================] - 0s 81us/step\n",
      "50/50 [==============================] - 0s 71us/step\n",
      "49/49 [==============================] - 0s 93us/step\n",
      "49/49 [==============================] - 0s 77us/step\n",
      "0.7912652969360352\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for tr_idx, val_idx in kfold.split(train_X):\n",
    "    tr_X, tr_y = train_X.iloc[tr_idx, :], train_y.iloc[tr_idx]\n",
    "    val_X, val_y = train_X.iloc[val_idx, :], train_y.iloc[val_idx]\n",
    "    model = make_model()\n",
    "    model.fit(tr_X, tr_y, \n",
    "            epochs=40, \n",
    "#           callbacks = [es], \n",
    "            validation_data=[val_X, val_y],\n",
    "            verbose=0)\n",
    "    acc += model.evaluate(val_X, val_y)[1]/10\n",
    "\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7912652969360352"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 0s 68us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5108383331343392, 0.7429906725883484]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## meta_training\n",
    "meta_train = pd.DataFrame()\n",
    "meta_train['lr'] = list(zip(*lr.predict_proba(train_X)))[0]\n",
    "meta_train['knn'] = list(zip(*knn.predict_proba(train_X)))[0]\n",
    "meta_train['rf'] = list(zip(*rf.predict_proba(train_X)))[0]\n",
    "meta_train['xgboost'] = list(zip(*clf.predict_proba(train_X)))[0]\n",
    "meta_train['voting'] = list(zip(*vclf.predict_proba(train_X)))[0]\n",
    "\n",
    "## meta_test\n",
    "meta_test = pd.DataFrame()\n",
    "meta_test['lr'] = list(zip(*lr.predict_proba(test_X)))[0]\n",
    "meta_test['knn'] = list(zip(*knn.predict_proba(test_X)))[0]\n",
    "meta_test['rf'] = list(zip(*rf.predict_proba(test_X)))[0]\n",
    "meta_test['xgboost'] = list(zip(*clf.predict_proba(test_X)))[0]\n",
    "meta_test['voting'] = list(zip(*vclf.predict_proba(test_X)))[0]\n",
    "# knn\n",
    "# rf\n",
    "# clf\n",
    "# vclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>knn</th>\n",
       "      <th>rf</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>voting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.912940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.884549</td>\n",
       "      <td>0.934846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.799155</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.832210</td>\n",
       "      <td>0.792607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.321217</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.657710</td>\n",
       "      <td>0.597356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.174382</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>0.525560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.745927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.787516</td>\n",
       "      <td>0.869335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.632150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.711787</td>\n",
       "      <td>0.830666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.806150</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.832210</td>\n",
       "      <td>0.794290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.722263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.828759</td>\n",
       "      <td>0.905067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.822041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.874082</td>\n",
       "      <td>0.928753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.320300</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.319402</td>\n",
       "      <td>0.310777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lr       knn    rf   xgboost    voting\n",
       "0    0.912940  1.000000  0.97  0.884549  0.934846\n",
       "1    0.799155  0.666667  0.89  0.832210  0.792607\n",
       "2    0.321217  0.666667  0.81  0.657710  0.597356\n",
       "3    0.174382  0.666667  0.86  0.415983  0.525560\n",
       "4    0.745927  1.000000  1.00  0.787516  0.869335\n",
       "..        ...       ...   ...       ...       ...\n",
       "493  0.632150  1.000000  0.82  0.711787  0.830666\n",
       "494  0.806150  0.666667  0.85  0.832210  0.794290\n",
       "495  0.722263  1.000000  0.98  0.828759  0.905067\n",
       "496  0.822041  1.000000  0.99  0.874082  0.928753\n",
       "497  0.320300  0.333333  0.11  0.319402  0.310777\n",
       "\n",
       "[498 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "def stack_net():\n",
    "    K.clear_session()\n",
    "    \n",
    "    input_layer = layers.Input(shape=((meta_train.shape[1], )))\n",
    "\n",
    "    layer1 = layers.Dense(4, activation='relu')(input_layer)\n",
    "#     layer2 = layers.Dense(8, activation='relu')(layer1)\n",
    "    \n",
    "    out_layer = layers.Dense(1, activation='sigmoid')(layer1)\n",
    "\n",
    "    model = models.Model(input_layer, out_layer)\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 150 samples\n",
      "Epoch 1/200\n",
      "348/348 [==============================] - 0s 369us/step - loss: 0.9097 - acc: 0.4167 - val_loss: 0.9089 - val_acc: 0.3600\n",
      "Epoch 2/200\n",
      "348/348 [==============================] - 0s 104us/step - loss: 0.8740 - acc: 0.4167 - val_loss: 0.8704 - val_acc: 0.3600\n",
      "Epoch 3/200\n",
      "348/348 [==============================] - 0s 95us/step - loss: 0.8393 - acc: 0.4109 - val_loss: 0.8355 - val_acc: 0.3600\n",
      "Epoch 4/200\n",
      "348/348 [==============================] - 0s 98us/step - loss: 0.8082 - acc: 0.4023 - val_loss: 0.8014 - val_acc: 0.3533\n",
      "Epoch 5/200\n",
      "348/348 [==============================] - 0s 95us/step - loss: 0.7774 - acc: 0.4109 - val_loss: 0.7692 - val_acc: 0.3533\n",
      "Epoch 6/200\n",
      "348/348 [==============================] - 0s 97us/step - loss: 0.7480 - acc: 0.4080 - val_loss: 0.7382 - val_acc: 0.3467\n",
      "Epoch 7/200\n",
      "348/348 [==============================] - 0s 97us/step - loss: 0.7199 - acc: 0.3908 - val_loss: 0.7082 - val_acc: 0.3600\n",
      "Epoch 8/200\n",
      "348/348 [==============================] - 0s 92us/step - loss: 0.6924 - acc: 0.4080 - val_loss: 0.6798 - val_acc: 0.7333\n",
      "Epoch 9/200\n",
      "348/348 [==============================] - 0s 95us/step - loss: 0.6660 - acc: 0.8851 - val_loss: 0.6543 - val_acc: 0.9333\n",
      "Epoch 10/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.6429 - acc: 0.9397 - val_loss: 0.6311 - val_acc: 0.9333\n",
      "Epoch 11/200\n",
      "348/348 [==============================] - 0s 99us/step - loss: 0.6210 - acc: 0.9454 - val_loss: 0.6080 - val_acc: 0.9400\n",
      "Epoch 12/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.5988 - acc: 0.9368 - val_loss: 0.5851 - val_acc: 0.9333\n",
      "Epoch 13/200\n",
      "348/348 [==============================] - 0s 95us/step - loss: 0.5772 - acc: 0.9339 - val_loss: 0.5619 - val_acc: 0.9400\n",
      "Epoch 14/200\n",
      "348/348 [==============================] - 0s 92us/step - loss: 0.5549 - acc: 0.9454 - val_loss: 0.5399 - val_acc: 0.9533\n",
      "Epoch 15/200\n",
      "348/348 [==============================] - 0s 94us/step - loss: 0.5335 - acc: 0.9511 - val_loss: 0.5179 - val_acc: 0.9533\n",
      "Epoch 16/200\n",
      "348/348 [==============================] - 0s 91us/step - loss: 0.5122 - acc: 0.9483 - val_loss: 0.4960 - val_acc: 0.9533\n",
      "Epoch 17/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.4913 - acc: 0.9483 - val_loss: 0.4757 - val_acc: 0.9533\n",
      "Epoch 18/200\n",
      "348/348 [==============================] - 0s 98us/step - loss: 0.4714 - acc: 0.9598 - val_loss: 0.4566 - val_acc: 0.9533\n",
      "Epoch 19/200\n",
      "348/348 [==============================] - 0s 100us/step - loss: 0.4527 - acc: 0.9626 - val_loss: 0.4387 - val_acc: 0.9533\n",
      "Epoch 20/200\n",
      "348/348 [==============================] - 0s 91us/step - loss: 0.4353 - acc: 0.9626 - val_loss: 0.4219 - val_acc: 0.9533\n",
      "Epoch 21/200\n",
      "348/348 [==============================] - 0s 93us/step - loss: 0.4184 - acc: 0.9655 - val_loss: 0.4053 - val_acc: 0.9533\n",
      "Epoch 22/200\n",
      "348/348 [==============================] - 0s 92us/step - loss: 0.4018 - acc: 0.9655 - val_loss: 0.3892 - val_acc: 0.9533\n",
      "Epoch 23/200\n",
      "348/348 [==============================] - 0s 91us/step - loss: 0.3855 - acc: 0.9684 - val_loss: 0.3739 - val_acc: 0.9600\n",
      "Epoch 24/200\n",
      "348/348 [==============================] - 0s 97us/step - loss: 0.3700 - acc: 0.9713 - val_loss: 0.3589 - val_acc: 0.9600\n",
      "Epoch 25/200\n",
      "348/348 [==============================] - 0s 97us/step - loss: 0.3547 - acc: 0.9713 - val_loss: 0.3441 - val_acc: 0.9600\n",
      "Epoch 26/200\n",
      "348/348 [==============================] - 0s 97us/step - loss: 0.3398 - acc: 0.9713 - val_loss: 0.3304 - val_acc: 0.9600\n",
      "Epoch 27/200\n",
      "348/348 [==============================] - 0s 91us/step - loss: 0.3256 - acc: 0.9741 - val_loss: 0.3170 - val_acc: 0.9600\n",
      "Epoch 28/200\n",
      "348/348 [==============================] - 0s 91us/step - loss: 0.3119 - acc: 0.9741 - val_loss: 0.3042 - val_acc: 0.9600\n",
      "Epoch 29/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.2989 - acc: 0.9741 - val_loss: 0.2918 - val_acc: 0.9600\n",
      "Epoch 30/200\n",
      "348/348 [==============================] - 0s 103us/step - loss: 0.2863 - acc: 0.9741 - val_loss: 0.2798 - val_acc: 0.9667\n",
      "Epoch 31/200\n",
      "348/348 [==============================] - 0s 98us/step - loss: 0.2744 - acc: 0.9741 - val_loss: 0.2687 - val_acc: 0.9667\n",
      "Epoch 32/200\n",
      "348/348 [==============================] - 0s 97us/step - loss: 0.2630 - acc: 0.9799 - val_loss: 0.2589 - val_acc: 0.9800\n",
      "Epoch 33/200\n",
      "348/348 [==============================] - 0s 100us/step - loss: 0.2521 - acc: 0.9828 - val_loss: 0.2481 - val_acc: 0.9800\n",
      "Epoch 34/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.2418 - acc: 0.9799 - val_loss: 0.2388 - val_acc: 0.9800\n",
      "Epoch 35/200\n",
      "348/348 [==============================] - 0s 97us/step - loss: 0.2320 - acc: 0.9856 - val_loss: 0.2298 - val_acc: 0.9800\n",
      "Epoch 36/200\n",
      "348/348 [==============================] - 0s 95us/step - loss: 0.2228 - acc: 0.9856 - val_loss: 0.2213 - val_acc: 0.9800\n",
      "Epoch 37/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.2140 - acc: 0.9885 - val_loss: 0.2133 - val_acc: 0.9800\n",
      "Epoch 38/200\n",
      "348/348 [==============================] - 0s 97us/step - loss: 0.2058 - acc: 0.9885 - val_loss: 0.2057 - val_acc: 0.9800\n",
      "Epoch 39/200\n",
      "348/348 [==============================] - 0s 92us/step - loss: 0.1979 - acc: 0.9914 - val_loss: 0.1986 - val_acc: 0.9800\n",
      "Epoch 40/200\n",
      "348/348 [==============================] - 0s 93us/step - loss: 0.1906 - acc: 0.9914 - val_loss: 0.1916 - val_acc: 0.9800\n",
      "Epoch 41/200\n",
      "348/348 [==============================] - 0s 94us/step - loss: 0.1836 - acc: 0.9914 - val_loss: 0.1854 - val_acc: 0.9800\n",
      "Epoch 42/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.1771 - acc: 0.9943 - val_loss: 0.1794 - val_acc: 0.9800\n",
      "Epoch 43/200\n",
      "348/348 [==============================] - 0s 93us/step - loss: 0.1707 - acc: 0.9971 - val_loss: 0.1736 - val_acc: 0.9800\n",
      "Epoch 44/200\n",
      "348/348 [==============================] - 0s 101us/step - loss: 0.1651 - acc: 0.9971 - val_loss: 0.1677 - val_acc: 0.9800\n",
      "Epoch 45/200\n",
      "348/348 [==============================] - 0s 97us/step - loss: 0.1593 - acc: 0.9971 - val_loss: 0.1631 - val_acc: 0.9867\n",
      "Epoch 46/200\n",
      "348/348 [==============================] - 0s 95us/step - loss: 0.1539 - acc: 0.9971 - val_loss: 0.1581 - val_acc: 0.9867\n",
      "Epoch 47/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.1489 - acc: 0.9971 - val_loss: 0.1536 - val_acc: 0.9867\n",
      "Epoch 48/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.1441 - acc: 0.9971 - val_loss: 0.1492 - val_acc: 0.9867\n",
      "Epoch 49/200\n",
      "348/348 [==============================] - 0s 94us/step - loss: 0.1396 - acc: 0.9971 - val_loss: 0.1450 - val_acc: 0.9867\n",
      "Epoch 50/200\n",
      "348/348 [==============================] - 0s 103us/step - loss: 0.1353 - acc: 0.9971 - val_loss: 0.1411 - val_acc: 0.9867\n",
      "Epoch 51/200\n",
      "348/348 [==============================] - 0s 104us/step - loss: 0.1311 - acc: 0.9971 - val_loss: 0.1371 - val_acc: 0.9867\n",
      "Epoch 52/200\n",
      "348/348 [==============================] - 0s 97us/step - loss: 0.1273 - acc: 0.9971 - val_loss: 0.1335 - val_acc: 0.9867\n",
      "Epoch 53/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.1238 - acc: 0.9971 - val_loss: 0.1303 - val_acc: 0.9867\n",
      "Epoch 54/200\n",
      "348/348 [==============================] - 0s 94us/step - loss: 0.1201 - acc: 0.9971 - val_loss: 0.1269 - val_acc: 0.9867\n",
      "Epoch 55/200\n",
      "348/348 [==============================] - 0s 95us/step - loss: 0.1167 - acc: 0.9971 - val_loss: 0.1236 - val_acc: 0.9867\n",
      "Epoch 56/200\n",
      "348/348 [==============================] - 0s 94us/step - loss: 0.1135 - acc: 0.9971 - val_loss: 0.1207 - val_acc: 0.9867\n",
      "Epoch 57/200\n",
      "348/348 [==============================] - 0s 95us/step - loss: 0.1106 - acc: 0.9971 - val_loss: 0.1181 - val_acc: 0.9867\n",
      "Epoch 58/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.1078 - acc: 0.9971 - val_loss: 0.1152 - val_acc: 0.9867\n",
      "Epoch 59/200\n",
      "348/348 [==============================] - 0s 101us/step - loss: 0.1048 - acc: 0.9971 - val_loss: 0.1127 - val_acc: 0.9867\n",
      "Epoch 60/200\n",
      "348/348 [==============================] - 0s 105us/step - loss: 0.1025 - acc: 0.9971 - val_loss: 0.1106 - val_acc: 0.9867\n",
      "Epoch 61/200\n",
      "348/348 [==============================] - 0s 108us/step - loss: 0.0995 - acc: 0.9971 - val_loss: 0.1077 - val_acc: 0.9867\n",
      "Epoch 62/200\n",
      "348/348 [==============================] - 0s 98us/step - loss: 0.0971 - acc: 0.9971 - val_loss: 0.1054 - val_acc: 0.9867\n",
      "Epoch 63/200\n",
      "348/348 [==============================] - 0s 95us/step - loss: 0.0948 - acc: 0.9971 - val_loss: 0.1031 - val_acc: 0.9867\n",
      "Epoch 64/200\n",
      "348/348 [==============================] - 0s 92us/step - loss: 0.0924 - acc: 0.9971 - val_loss: 0.1012 - val_acc: 0.9867\n",
      "Epoch 65/200\n",
      "348/348 [==============================] - 0s 96us/step - loss: 0.0904 - acc: 0.9971 - val_loss: 0.0994 - val_acc: 0.9867\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00065: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2b79d223b48>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_acc', patience=20, restore_best_weights=True, verbose=1)\n",
    "stack = stack_net()\n",
    "stack.fit(meta_train, train_y,\n",
    "          epochs=200,\n",
    "          validation_split = 0.3,\n",
    "          callbacks = [es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 0s 51us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5577698377805336, 0.7429906725883484]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack.evaluate(meta_test, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.7710280373831776\n",
      "knn: 0.7523364485981309\n",
      "rf: 0.7663551401869159\n",
      "xgboost: 0.780373831775701\n",
      "voting: 0.7897196261682243\n",
      "214/214 [==============================] - 0s 79us/step\n",
      "nn: 0.7429906725883484\n",
      "214/214 [==============================] - 0s 74us/step\n",
      "stacking: 0.7429906725883484\n"
     ]
    }
   ],
   "source": [
    "print('lr:', accuracy_score(test_y, lr.predict(test_X)))\n",
    "print('knn:', accuracy_score(test_y, knn.predict(test_X)))\n",
    "print('rf:', accuracy_score(test_y, rf.predict(test_X)))\n",
    "print('xgboost:', accuracy_score(test_y, clf.predict(test_X)))\n",
    "print('voting:', accuracy_score(test_y, vclf.predict(test_X)))\n",
    "print('nn:', model.evaluate(test_X, test_y)[1])\n",
    "print('stacking:', stack.evaluate(meta_test, test_y)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
