{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keep-steady.tistory.com/37\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('./data/train_data.csv', index_col='index')\n",
    "te = pd.read_csv('./data/test_data.csv', index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자만 남기고 제거\n",
    "import re\n",
    "\n",
    "reg = re.compile('[가-힣a-zA-Z]+')\n",
    "tr['title'] =  tr['title'].map(lambda x: ' '.join(reg.findall(x)))\n",
    "te['title'] = te['title'].map(lambda x: ' '.join(reg.findall(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/sent.txt', 'w', encoding='utf-8') as f:\n",
    "    for s in tr['title'].values:\n",
    "        f.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "mecab = konlpy.tag.Mecab('C:\\mecab')\n",
    "mecab_tokenizer = mecab.morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sent.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "total_morph=[]\n",
    "for sentence in data:\n",
    "    # 문장단위 mecab 적용\n",
    "    morph_sentence = mecab_tokenizer(sentence)\n",
    "#     morph_sentence = list(filter(lambda x: x not in stopwords, morph_sentence))\n",
    "    # 문장단위 저장\n",
    "    total_morph.append(morph_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_morph[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/after_mecab.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in total_morph:\n",
    "        f.write(' '.join(line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "how_to_tokenize = BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(strip_accents=False, \n",
    "                                   lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file   = ['data/after_mecab.txt']  # data path\n",
    "vocab_size    = 32000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,  # 단어의 최소 발생 빈도, 5\n",
    "               limit_alphabet=limit_alphabet,  # ByteLevelBPETokenizer 학습시엔 주석처리 필요\n",
    "               show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model('vocab') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('vocab',\n",
    "                                                       strip_accents=False,  # Must be False if cased model\n",
    "                                                       lowercase=False)  # 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertForSequenceClassification\n",
    "\n",
    "config = DistilBertConfig(vocab_size=tokenizer.vocab_size)\n",
    "print(config)\n",
    "model = DistilBertForSequenceClassification(config)\n",
    "model.classifier = nn.Linear(768, 7)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_len=40):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        record = self.data.iloc[index]\n",
    "        document, label = str(record['title']), int(record['topic_idx'])\n",
    "        inputs = self.tokenizer(\n",
    "            document, \n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=self.max_seq_len,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "            )\n",
    "        \n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "            \n",
    "        return {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': np.array(label, dtype=np.int_)}\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_len=40):\n",
    "        self.data = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        record = self.data.iloc[index]\n",
    "        document = str(record['title'])\n",
    "        inputs = self.tokenizer(\n",
    "            document, \n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=self.max_seq_len,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "            )\n",
    "        \n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "            \n",
    "        return {'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize 결과를 바탕으로 문장 max_seq_len 결정\n",
    "token_len = tr['title'].map(tokenizer.tokenize)\n",
    "token_len.map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train parameters\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "max_seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold로 validation 하시면 더 좋을 것 같습니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(tr, test_size=0.15, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loader\n",
    "train_ds = TrainDataset(train, tokenizer, max_seq_len=max_seq_len)\n",
    "tr_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "\n",
    "val_ds = TrainDataset(val, tokenizer, max_seq_len=max_seq_len)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "test_ds = TestDataset(te, tokenizer, max_seq_len=max_seq_len)\n",
    "test_loader = DataLoader(test_ds, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 5, )\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_score = -float('inf')\n",
    "best_model = None\n",
    "\n",
    "for e in range(epochs):\n",
    "    tr_loss = []\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    for batch in tr_loader:\n",
    "        optimizer.zero_grad()\n",
    "        ids, atts, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        ids = torch.tensor(ids).long().cuda()\n",
    "        atts = torch.tensor(atts).float().cuda()\n",
    "        labels = torch.tensor(labels).long().cuda()\n",
    "        pred = model(ids, attention_mask=atts)\n",
    "        loss = loss_fn(pred[0], labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += [loss.item()]\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    for batch in val_loader:\n",
    "        optimizer.zero_grad()\n",
    "        ids, atts, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        ids = torch.tensor(ids).long().cuda()\n",
    "        atts = torch.tensor(atts).float().cuda()\n",
    "        trues += list(labels.numpy())\n",
    "        pred = model(ids, attention_mask=atts)\n",
    "        preds += list(np.argmax(pred[0].detach().cpu().numpy(), 1))\n",
    "        \n",
    "    trues = np.array(trues)\n",
    "    preds = np.array(preds)\n",
    "    acc = np.sum(trues == preds) / len(trues)\n",
    "    \n",
    "    if best_score < acc:\n",
    "        best_score = acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    print(e, 'tr_loss:', np.mean(tr_loss), 'val_score:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise('eo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "model.eval()\n",
    "\n",
    "for b in tqdm(test_loader):\n",
    "    ids, atts = b['input_ids'], b['attention_mask']\n",
    "    ids = torch.tensor(ids).long().cuda()\n",
    "    atts = torch.tensor(atts).float().cuda()\n",
    "    pred = best_model(ids, attention_mask=atts)\n",
    "    preds += list(np.argmax(pred[0].detach().cpu().numpy(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./data/sample_submission.csv', index_col='index')\n",
    "sub['topic_idx'] = preds\n",
    "sub.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub.to_csv('./custom_Distilbert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
