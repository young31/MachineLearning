{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def seed_all(SEED=42):\n",
    "    np.random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "    random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/train/train.csv')\n",
    "valid_cols = ['DHI', 'DNI', 'T', 'TARGET'] #  'WS', 'RH',\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_hour = []\n",
    "for i in range(24):\n",
    "    if data[data['Hour']==i].sum()['TARGET'] == 0:\n",
    "        zero_hour.append(i)\n",
    "zero_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[valid_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[valid_cols].copy()\n",
    "target = train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# sc = StandardScaler()\n",
    "tr_sc  = MinMaxScaler()\n",
    "tar_sc = MinMaxScaler()\n",
    "tr_sc.fit(train)\n",
    "tar_sc.fit(target.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr_sc.transform(train)\n",
    "target = tar_sc.transform(target.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 3\n",
    "window = days*48\n",
    "# valid_min = max(48*2, window)\n",
    "\n",
    "available_idx = np.array(list(range(0, train.shape[0]-window-48*2, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_generator():\n",
    "    for el in available_idx:\n",
    "        yield train[el:el+window], target[el+window:el+window+48*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers, callbacks, layers, losses\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation, Add, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Conv1DTranspose,\\\n",
    "concatenate, Input, UpSampling1D, GlobalMaxPooling1D, Permute, LSTM, GRU\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(x):\n",
    "    return x*tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "def decay(epochs):\n",
    "    init = 1e-3\n",
    "    drop = 10\n",
    "    ratio = 0.9\n",
    "    return max(5e-5, (init * (ratio ** (epochs//drop))))\n",
    "\n",
    "es = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "lrs = callbacks.LearningRateScheduler(decay, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds = tf.data.Dataset.from_generator(tr_generator,\n",
    "                                    (tf.float32, tf.float32), \n",
    "                                    )\n",
    "tr_loader = tr_ds.shuffle(1024, reshuffle_each_iteration=True).batch(128).prefetch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,y in tr_generator():\n",
    "    print(d.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inputs = Input(shape=(window, 6))\n",
    "#     x = Permute((2, 1),)(inputs)\n",
    "\n",
    "    h = LSTM(128, return_sequences=True)(inputs)\n",
    "    h = LSTM(128, return_sequences=False)(h)\n",
    "\n",
    "    outputs = Dense(48*2)(h)\n",
    "#     outputs = tf.expand_dims(outputs, 1)\n",
    "#     outputs = Add()([outputs, tf.expand_dims(inputs[:,-1], 1)])\n",
    "    \n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn = build_model()\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = './tmp/checkpoint'\n",
    "ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    save_best_only=True)\n",
    "\n",
    "nn.compile(loss='mse', optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4))\n",
    "nn.fit(tr_loader,\n",
    "      epochs=5,\n",
    "      callbacks=[ckpt]\n",
    "      )\n",
    "nn.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "test_files = glob.glob('./data/test/*.csv')\n",
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def post_processing(pred):\n",
    "    res = pred.copy()\n",
    "    for i in zero_hour:\n",
    "        res[2*i] = 0\n",
    "        res[2*i+1] = 0\n",
    "        res[48 + 2*i] = 0\n",
    "        res[48 + 2*i+1] = 0\n",
    "        \n",
    "    res = np.clip(res, 0, float('inf'))\n",
    "    res = smoothing(res)\n",
    "    return res\n",
    "\n",
    "def smoothing(x):\n",
    "    for i in range(1, len(x)-1):\n",
    "        if x[i-1] == 0 and x[i+1] == 0:\n",
    "            x[i] = 0\n",
    "    return x\n",
    "        \n",
    "post_processing(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_X = test[valid_cols].values[-48*2:]\n",
    "test_X = tr_sc.transform(test_X)\n",
    "test_X = np.expand_dims(test_X, 0)\n",
    "pred = nn.predict(test_X)\n",
    "pred = tar_sc.inverse_transform(pred.flatten().reshape(-1, 1))\n",
    "pred = post_processing(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred)\n",
    "plt.plot(post_processing(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import MeanAbsoluteError, MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration  = 100\n",
    "preds = {}\n",
    "for seed in tqdm(range(iteration)):\n",
    "    seed_all(seed)\n",
    "    nn = build_model()\n",
    "    nn.compile(loss='mse', optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4))\n",
    "    nn.fit(tr_loader,\n",
    "          epochs=10,\n",
    "          verbose = 0,\n",
    "          callbacks=[ckpt]\n",
    "          )\n",
    "    nn.load_weights(checkpoint_filepath)\n",
    "\n",
    "    for i, test_file in enumerate(test_files):\n",
    "        if seed == 0:\n",
    "            preds[test_file] = []\n",
    "            \n",
    "        test = pd.read_csv(test_file)\n",
    "        test_X = test[valid_cols].values[-48*2:]\n",
    "        test_X = tr_sc.transform(test_X)\n",
    "        test_X = np.expand_dims(test_X, 0)\n",
    "        pred = nn.predict(test_X)\n",
    "        pred = tar_sc.inverse_transform(pred.flatten().reshape(-1, 1))\n",
    "        pred = post_processing(pred)\n",
    "        preds[test_file].append(pred)\n",
    "        \n",
    "        if seed == iteration-1:\n",
    "            preds[test_file] = np.array(preds[test_file])\n",
    "\n",
    "print('train done')\n",
    "for key in preds.keys():\n",
    "    file = key.split('\\\\')[1]\n",
    "    idx = sub[sub['id'].map(lambda x: x.split('_')[0]) == file].index\n",
    "    for i in range(1, 10):\n",
    "        q = i/10\n",
    "        sub.loc[idx[0]:idx[-1]][f'q_{q}'] = np.quantile(preds[key], q, axis=0).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {}\n",
    "for seed in tqdm(range(100)):\n",
    "    seed_all(seed)\n",
    "    rf = RandomForestRegressor(n_jobs=-1, random_state=seed, n_estimators=100, min_samples_split=10)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    for i, test_file in enumerate(test_files):\n",
    "        if seed == 0:\n",
    "            preds[test_file] = []\n",
    "        test = pd.read_csv(test_file)\n",
    "        pred = rf.predict(test[test['Day']>=6][test['Day']<=6][valid_cols].values.flatten().reshape(1, -1))\n",
    "        preds[test_file].append(pred)\n",
    "        \n",
    "        if seed == 99:\n",
    "            preds[test_file] = np.array(preds[test_file])\n",
    "\n",
    "print('train done')\n",
    "for key in preds.keys():\n",
    "    file = key.split('\\\\')[1]\n",
    "    idx = sub[sub['id'].map(lambda x: x.split('_')[0]) == file].index\n",
    "    for i in range(1, 10):\n",
    "        q = i/10\n",
    "        sub.loc[idx[0]:idx[-1]][f'q_{q}'] = np.quantile(preds[key], q, axis=0).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_file in tqdm(test_files):\n",
    "    test = pd.read_csv(test_file)\n",
    "    preds = []\n",
    "    for es in rf.estimators_:\n",
    "        pred = es.predict(test[test['Day']>=5][test['Day']<=6][valid_cols].values.flatten().reshape(1, -1))\n",
    "        preds.append(pred)\n",
    "    preds = np.array(preds)\n",
    "    \n",
    "    file = test_file.split('\\\\')[1]\n",
    "    idx = sub[sub['id'].map(lambda x: x.split('_')[0]) == file].index\n",
    "    for i in range(1, 10):\n",
    "        q = i/10\n",
    "        sub.loc[idx[0]:idx[-1]][f'q_{q}'] = np.quantile(preds, q, axis=0).reshape(-1, 1)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub.iloc[30:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
