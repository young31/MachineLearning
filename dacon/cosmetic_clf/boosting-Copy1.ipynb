{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./new_cols.bin', 'rb')\n",
    "cols = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('./data/train.csv')\n",
    "te = pd.read_csv('./data/test.csv')\n",
    "\n",
    "sub = pd.read_csv('./data/sample_submission.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_lbe = LabelEncoder().fit(target)\n",
    "\n",
    "# t = target_lbe.transform(target)\n",
    "\n",
    "column_number = {}\n",
    "for i, column in enumerate(sub.columns):\n",
    "    column_number[column] = i\n",
    "    \n",
    "def to_number(x, dic):\n",
    "    return dic[x]\n",
    "\n",
    "tr['type_num'] = tr['type'].apply(lambda x: to_number(x, column_number))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tr['type_num']\n",
    "t = target.copy()\n",
    "\n",
    "train_X = tr.drop(['id', 'type', 'type_num', 'fiberID'], axis=1)\n",
    "test_X = te.drop(['id','fiberID'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X['index'] = np.ones(len(train_X))\n",
    "test_X['index'] = np.ones(len(test_X))*2\n",
    "\n",
    "merge = pd.concat([train_X, test_X], ignore_index=True)\n",
    "\n",
    "k = train_X.columns[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.concat([merge, pd.get_dummies(merge['fiberID'], prefix='fiberID')], axis=1)\n",
    "merge = merge.drop('fiberID', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = merge[merge['index'] == 1]\n",
    "train_X = train_X.drop('index', axis=1)\n",
    "test_X = merge[merge['index'] == 2]\n",
    "test_X = test_X.drop('index', axis=1)\n",
    "test_X.index = range(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X = train_X[k]\n",
    "te_X = test_X[k]\n",
    "\n",
    "tr_X = (tr_X - np.mean(tr_X))/np.std(tr_X)\n",
    "te_X = (te_X - np.mean(te_X))/np.std(te_X)\n",
    "\n",
    "train_X[k] = tr_X\n",
    "test_X[k] = te_X\n",
    "\n",
    "te_X = test_X.copy()\n",
    "tr_X = train_X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X = tr_X[cols]\n",
    "te_X = te_X[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X = (train_X - np.mean(train_X))/np.std(train_X)\n",
    "te_X = (test_X - np.mean(test_X))/np.std(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(tr_X, t, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGB_bayesian(\n",
    "    #learning_rate,\n",
    "    num_leaves, \n",
    "    bagging_fraction,\n",
    "    feature_fraction,\n",
    "    min_child_weight, \n",
    "    min_data_in_leaf,\n",
    "    max_depth,\n",
    "    reg_alpha,\n",
    "    reg_lambda\n",
    "     ):\n",
    "    # LightGBM expects next three parameters need to be integer. \n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "    \n",
    "\n",
    "    params = {\n",
    "              'num_leaves': num_leaves, \n",
    "              'min_data_in_leaf': min_data_in_leaf,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "#               'learning_rate' : 0.03,\n",
    "              'max_depth': max_depth,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'objective': 'binary',\n",
    "              'save_binary': True,\n",
    "              'seed': 12,\n",
    "              'feature_fraction_seed': 12,\n",
    "              'bagging_seed': 12,\n",
    "              'drop_seed': 12,\n",
    "              'data_random_seed': 12,\n",
    "              'boosting': 'gbdt', ## some get better result using 'dart'\n",
    "              'verbose': 1,\n",
    "              'is_unbalance': False,\n",
    "              'boost_from_average': True,\n",
    "              'metric':'multi_logloss'}    \n",
    "    \n",
    "    ## set clf options\n",
    "    clf = lgb.LGBMClassifier(**params).fit(train_X, train_y, early_stopping_rounds=100,eval_set=[(test_X, test_y)], eval_metric='multi_logloss', verbose=0)\n",
    "    \n",
    "    score = accuracy_score(test_y, clf.predict(test_X))\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_LGB = {\n",
    "    'num_leaves': (300, 1000), \n",
    "    'min_data_in_leaf': (0, 150),\n",
    "    'bagging_fraction' : (0.3, 0.9),\n",
    "    'feature_fraction' : (0.3, 0.9),\n",
    "#     'learning_rate': (0.01, 0.3),\n",
    "    'min_child_weight': (0.001, 3),   \n",
    "    'reg_alpha': (0.1, 3), \n",
    "    'reg_lambda': (0.1, 3),\n",
    "    'max_depth':(10, 30),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_points = 10\n",
    "n_iter = 20\n",
    "\n",
    "optimizer.maximize(init_points=init_points, n_iter=n_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lgb = {\n",
    "        'min_data_in_leaf': int(optimizer.max['params']['min_data_in_leaf']), \n",
    "        'num_leaves': int(optimizer.max['params']['num_leaves']), \n",
    "        #'learning_rate': LGB_BO.max['params']['learning_rate'],\n",
    "        'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "        'bagging_fraction': optimizer.max['params']['bagging_fraction'], \n",
    "        'feature_fraction': optimizer.max['params']['feature_fraction'],\n",
    "        'reg_lambda': optimizer.max['params']['reg_lambda'],\n",
    "        'reg_alpha': optimizer.max['params']['reg_alpha'],\n",
    "        'max_depth': int(optimizer.max['params']['max_depth']), \n",
    "        'objective': 'binary',\n",
    "        'save_binary': True,\n",
    "        'seed': 12,\n",
    "        'feature_fraction_seed': 12,\n",
    "        'bagging_seed': 12,\n",
    "        'drop_seed': 12,\n",
    "        'data_random_seed': 12,\n",
    "        'boosting_type': 'gbdt',  # also consider 'dart'\n",
    "        'verbose': 1,\n",
    "        'is_unbalance': False,\n",
    "        'boost_from_average': True,\n",
    "        'metric':'multi_logloss'\n",
    "    }\n",
    "\n",
    "params = param_lgb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('best_params_sh.bin', 'wb')\n",
    "# pickle.dump(params, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('best_params_sh.bin', 'rb')\n",
    "params = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.8133797481154059, bagging_seed=12,\n",
       "               boost_from_average=True, boosting_type='gbdt', class_weight=None,\n",
       "               colsample_bytree=1.0, data_random_seed=12, drop_seed=12,\n",
       "               early_stoppong_rounds=100, feature_fraction=0.42744912354487063,\n",
       "               feature_fraction_seed=12, importance_type='split',\n",
       "               is_unbalance=False, learning_rate=0.1, max_depth=29,\n",
       "               metric='multi_logloss', min_child_samples=20,\n",
       "               min_child_weight=2.312439152145239, min_data_in_leaf=2,\n",
       "               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=302,\n",
       "               objective='binary', random_state=None,\n",
       "               reg_alpha=1.9121592091690662, reg_lambda=0.4835878331894956,\n",
       "               save_binary=True, seed=12, silent=True, ...)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(**params, early_stoppong_rounds = 100)\n",
    "lgb_clf.fit(tr_X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = lgb_clf.predict_proba(te_X)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/lgb7.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'############################'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parmas for xgboost\n",
    "params_fx = {'min_data_in_leaf': params['min_data_in_leaf'],\n",
    "             'num_leaves': params['num_leaves'],\n",
    "             'min_child_weight': params['min_child_weight'],\n",
    "             'bagging_fraction': params['bagging_fraction'],\n",
    "             'feature_fraction': params['feature_fraction'],\n",
    "             'reg_lambda': params['reg_lambda'],\n",
    "             'reg_alpha': params['reg_alpha'],\n",
    "             'max_depth': params['max_depth']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(**params, early_stoppong_rounds = 100)\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "            **params_fx,\n",
    "            n_estimators=300,\n",
    "            tree_method = 'hist',\n",
    "            booster = 'gbtree',\n",
    "#             eval_metric = 'mlogloss',\n",
    "#             objective = 'multi:softprob',\n",
    "#             num_class = 19,\n",
    "            early_stoppong_rounds = 100\n",
    "            \n",
    "    )\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200,\n",
    "                               max_depth=13,\n",
    "                               min_samples_split=5,\n",
    "                               min_samples_leaf=5,\n",
    "                               min_impurity_decrease = 0.001,\n",
    "                               max_features=None,\n",
    "                               oob_score=True,\n",
    "                               random_state=42,)\n",
    "\n",
    "pca = PCA(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[lgb_clf, rf, pca], \n",
    "          [rf]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Start of Level 0 ======================\n",
      "Input Dimensionality 20 at Level 0 \n",
      "3 models included in Level 0 \n",
      "Fold 1/4 , model 0 , logloss===0.393031 \n",
      "Fold 1/4 , model 1 , logloss===0.623155 \n",
      "=========== end of fold 1 in level 0 ===========\n",
      "Fold 2/4 , model 0 , logloss===0.385407 \n",
      "Fold 2/4 , model 1 , logloss===0.628474 \n",
      "=========== end of fold 2 in level 0 ===========\n",
      "Fold 3/4 , model 0 , logloss===0.382767 \n",
      "Fold 3/4 , model 1 , logloss===0.615601 \n",
      "=========== end of fold 3 in level 0 ===========\n",
      "Fold 4/4 , model 0 , logloss===0.388745 \n",
      "Fold 4/4 , model 1 , logloss===0.625972 \n",
      "=========== end of fold 4 in level 0 ===========\n",
      "Level 0, model 0 , logloss===0.387487 \n",
      "Level 0, model 1 , logloss===0.623300 \n",
      "Output dimensionality of level 0 is 50 \n",
      "====================== End of Level 0 ======================\n",
      " level 0 lasted 2090.965102 seconds \n",
      "====================== Start of Level 1 ======================\n",
      "Input Dimensionality 50 at Level 1 \n",
      "1 models included in Level 1 \n",
      "Fold 1/4 , model 0 , logloss===0.474300 \n",
      "=========== end of fold 1 in level 1 ===========\n",
      "Fold 2/4 , model 0 , logloss===0.472525 \n",
      "=========== end of fold 2 in level 1 ===========\n",
      "Fold 3/4 , model 0 , logloss===0.465748 \n",
      "=========== end of fold 3 in level 1 ===========\n",
      "Fold 4/4 , model 0 , logloss===0.474813 \n",
      "=========== end of fold 4 in level 1 ===========\n",
      "Level 1, model 0 , logloss===0.471846 \n",
      "Output dimensionality of level 1 is 19 \n",
      "====================== End of Level 1 ======================\n",
      " level 1 lasted 2879.732177 seconds \n",
      "====================== End of fit ======================\n",
      " fit() lasted 4970.715796 seconds \n"
     ]
    }
   ],
   "source": [
    "model = StackNetClassifier(models, \n",
    "                           metric=\"logloss\", \n",
    "                           folds=3,\n",
    "                           restacking=False,\n",
    "                           use_retraining=True,\n",
    "                           use_proba=True, # To use predict_proba after training\n",
    "                           random_state=42,\n",
    "                           n_jobs=-1, \n",
    "                           verbose=1)\n",
    "\n",
    "model.fit(tr_X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Start of Level 0 ======================\n",
      "1 estimators included in Level 0 \n",
      "====================== Start of Level 1 ======================\n",
      "1 estimators included in Level 1 \n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_proba(te_X)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/stk3.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kk = pd.DataFrame(lgb_clf.predict_proba(te_X), columns=target_lbe.classes_)\n",
    "# sub[sub.columns[1:]] = kk[sub.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub.to_csv('./sub/stk1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_imp = lgb_clf.feature_importances_\n",
    "\n",
    "lgb_imp_idx = []\n",
    "cols = tr_X.columns\n",
    "for i, imp in enumerate(lgb_imp):\n",
    "    if imp > 0:\n",
    "        lgb_imp_idx.append(i)\n",
    "        \n",
    "new_cols_imp = cols[lgb_imp_idx]\n",
    "new_cols_imp = list(new_cols_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(tr_X.columns), len(new_cols_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./new_cols.bin', 'wb')\n",
    "pickle.dump(new_cols_imp, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
