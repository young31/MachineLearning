{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os,sys,inspect\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, callbacks, layers, losses\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation, Add, BatchNormalization, Dropout, Input, Embedding, Flatten, Multiply\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "        \n",
    "def mish(x):\n",
    "    return x*tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "def leakyrelu(x, factor=0.2):\n",
    "    return tf.maximum(x, factor*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('./data/train.pkl')\n",
    "test = pd.read_pickle('./data/test.pkl')\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expressions = {\n",
    "#     r'\\d+:\\d+:\\d+\\S': '<TIME>',\n",
    "#     r'\\d+:\\d+': '<RANGE>',\n",
    "#     r'\\d+-\\d+-\\d+\\S': '<DAY>',\n",
    "#     r'\\S\\S+[a-z] [\\s\\d]\\d': '<MON> <DATE>',\n",
    "#     r'\\d+': '<NUM>',\n",
    "#     'js:': 'js',\n",
    "#     r'\\\\n': ' ',\n",
    "#     '\\t': ' ',\n",
    "#     '\"': '',\n",
    "#     r':+\\s': ' ',\n",
    "#     ':': '=',\n",
    "#     '{': '',\n",
    "#     '}': '',\n",
    "#     '(': '',\n",
    "#     ')': '',\n",
    "#     ',': ' ',\n",
    "#     r'\\s+': ' '\n",
    "# }\n",
    "\n",
    "expressions = {\n",
    "    r'\\d+:\\d+:\\d+\\S': '<TIME>',\n",
    "    r'\\d+:\\d+': '<RANGE>',\n",
    "    r'\\d+-\\d+-\\d+\\S': '<DAY>',\n",
    "    r'\\S\\S+[a-z] [\\s\\d]\\d': '<MON> <DATE>',\n",
    "    r'\\d+': '<NUM>',\n",
    "#     'js:': 'js',\n",
    "#     r'\\\\n|\\s+|,|:+\\s': ' ',\n",
    "    r'\\\\n|,|[[]|[]]|[=]|[:]': ' ',\n",
    "    r'[{]|[}]|[(]|[)]|[\"]|[\\\\]+': '',\n",
    "#     ':': '=',\n",
    "}\n",
    "\n",
    "def strip_strs(x):\n",
    "    phrases = re.findall(r'\"+[\\S\\s]+?\"', x)\n",
    "    for ph in phrases:\n",
    "        x = x.replace(ph, ph.replace(' ', ''))\n",
    "    return x\n",
    "\n",
    "def convert(x):\n",
    "    for f, t in expressions.items():\n",
    "        x = re.sub(f, t, x)\n",
    "    return x\n",
    "\n",
    "def convert_df(df_, col='full_log'):\n",
    "    df = df_.copy()\n",
    "    df[col] = df[col].map(strip_strs)\n",
    "    for f, t in expressions.items():\n",
    "        df[col] = df[col].str.replace(f, t)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = convert_df(train, 'full_log')\n",
    "test_X = convert_df(test)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_sent = list(map(list, map(lambda x: filter(lambda y: len(y)>0, x.split(' ')), df['full_log'].values)))\n",
    "test_sent = list(map(list, map(lambda x: filter(lambda y: len(y)>0, x.split(' ')), test_X['full_log'].values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "emb_dim = 128\n",
    "w2v = gensim.models.Word2Vec(tr_sent, vector_size =emb_dim, sg=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.build_vocab(np.array(tr_sent))\n",
    "w2v.train(np.array(tr_sent),\n",
    "         total_examples = w2v.corpus_count,\n",
    "         epochs=100,\n",
    "         compute_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(sents, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(sents),num_features),dtype=\"float32\")\n",
    "    for sent in tqdm(sents):\n",
    "        reviewFeatureVecs[counter] = featureVecMethod(sent, model, num_features)\n",
    "        counter += 1\n",
    "\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb_X = getAvgFeatureVecs(tr_sent, w2v, emb_dim)\n",
    "emb_test_X = getAvgFeatureVecs(test_sent, w2v, emb_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE=0.2\n",
    "\n",
    "tr_X, val_X, tr_y, val_y=train_test_split(emb_X, df['level'], test_size=TEST_SIZE, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential([\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(7, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.Adam(2e-4))\n",
    "es = callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "\n",
    "nn.fit(emb_X, tr_y,\n",
    "      epochs=10,\n",
    "      validation_data=(val_X, val_y),\n",
    "      callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=nn.predict(val_X)\n",
    "# probas=forest.predict_proba(val_X)\n",
    "\n",
    "f1_score(val_y, np.argmax(preds, 1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.bw = None\n",
    "        self.best_score = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pred = self.model.generator.predict(val_X)\n",
    "        pred = np.argmax(pred, 1)\n",
    "        score = f1_score(val_y, pred, average='macro')\n",
    "        if score > self.best_score:\n",
    "            self.bw = self.model.generator.get_weights()\n",
    "            \n",
    "    def on_train_end(self, epoch, logs=None):\n",
    "        self.model.generator.set_weights(self.bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.models.Model):\n",
    "    def __init__(self, x_dim, y_dim, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "    def compile(self, adv_loss_fn, pred_loss_fn, g_optim, d_optim):\n",
    "        super().compile()\n",
    "        self.adv_loss_fn = adv_loss_fn\n",
    "        self.pred_loss_fn = pred_loss_fn\n",
    "        self.g_optim = g_optim\n",
    "        self.d_optim = d_optim\n",
    "        \n",
    "        \n",
    "    def build_generator(self):\n",
    "        activation = 'relu'\n",
    "        inputs = Input(shape=(self.x_dim, ))\n",
    "        \n",
    "        h = Dense(128)(inputs)\n",
    "        h = Activation(activation)(h)\n",
    "        h = Dense(64)(h)\n",
    "        h = Activation(activation)(h)\n",
    "        \n",
    "#         h = Add()([inputs, h])\n",
    "        \n",
    "        outputs = Dense(self.y_dim)(h)\n",
    "        outputs = Activation('softmax')(outputs)\n",
    "        \n",
    "        return Model(inputs, outputs, name='generator')\n",
    "        \n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        activation = 'relu'\n",
    "        inputs_x = Input(shape=(self.x_dim, ))\n",
    "        inputs_y = Input(shape=(self.y_dim, ))\n",
    "        \n",
    "        inputs = Concatenate()([inputs_x, inputs_y])\n",
    "        \n",
    "        h = Dense(64)(inputs)\n",
    "        h = Activation(activation)(h)\n",
    "        h = Dense(32)(h)\n",
    "        h = Activation(activation)(h)\n",
    "    \n",
    "        outputs = Dense(1)(h)\n",
    "        \n",
    "        return Model([inputs_x, inputs_y], outputs, name='discriminator')\n",
    "    \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        global batch_size\n",
    "        x = data['X']\n",
    "        y = data['y']\n",
    "        \n",
    "        fake_labels = tf.ones((self.batch_size, 1))\n",
    "        real_labels = tf.ones((self.batch_size, 1))*0\n",
    "        labels = tf.concat([real_labels, fake_labels], 0)\n",
    "\n",
    "        # discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake = self.generator(x)\n",
    "            all_y = tf.concat([y, fake], 0)\n",
    "            all_x = tf.concat([x, x], 0)\n",
    "            preds = self.discriminator([all_x, all_y])\n",
    "\n",
    "            d_loss = self.adv_loss_fn(labels, preds)\n",
    "\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optim.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "        # generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake = self.generator(x)\n",
    "            preds = self.discriminator([x, fake])\n",
    "            \n",
    "            adv_loss = self.adv_loss_fn(real_labels, preds)\n",
    "            pred_loss = self.pred_loss_fn(y, fake)\n",
    "            \n",
    "            g_loss = adv_loss + 100*pred_loss\n",
    "            \n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optim.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        return {'d_loss': d_loss, 'g_loss': g_loss, 'adv_loss': adv_loss, 'pred_loss': pred_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "tr_ds = tf.data.Dataset.from_tensor_slices({'X': emb_X, 'y': to_categorical(df['level'].values).astype(np.float32)})\n",
    "tr_loader = tr_ds.batch(batch_size, drop_remainder=True).shuffle(buffer_size=5000, reshuffle_each_iteration=True).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(64, 7, batch_size)\n",
    "gan.compile(\n",
    "    adv_loss_fn = losses.BinaryCrossentropy(from_logits=True),\n",
    "    pred_loss_fn = losses.CategoricalCrossentropy(),\n",
    "    g_optim = optimizers.Adam(2e-4),\n",
    "    d_optim = optimizers.Adam(2e-4)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='pred_loss')\n",
    "\n",
    "gan.fit(tr_loader,\n",
    "      epochs=20,\n",
    "      callbacks=[ValCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gan.generator.predict(val_X)\n",
    "f1_score(val_y, np.argmax(pred, 1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.argmax(pred, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(df['level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(64, 7, batch_size)\n",
    "gan.compile(\n",
    "#     adv_loss_fn = losses.BinaryCrossentropy(from_logits=True),\n",
    "    adv_loss_fn = losses.MeanSquaredError(),\n",
    "    pred_loss_fn = losses.CategoricalCrossentropy(),\n",
    "    g_optim = optimizers.Adam(2e-4),\n",
    "    d_optim = optimizers.Adam(2e-4)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan.fit(tr_loader,\n",
    "      epochs=20,\n",
    "      callbacks=[ValCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gan.generator.predict(val_X)\n",
    "f1_score(val_y, np.argmax(pred, 1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D2GAN(tf.keras.models.Model):\n",
    "    def __init__(self, x_dim, y_dim, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.alpha = 1\n",
    "        self.beta = 1\n",
    "        \n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator_1 = self.build_discriminator()\n",
    "        self.discriminator_2 = self.build_discriminator()\n",
    "        \n",
    "    def compile(self, adv_loss_fn, pred_loss_fn, g_optim, d_optim_1, d_optim_2):\n",
    "        super().compile()\n",
    "        self.adv_loss_fn = adv_loss_fn\n",
    "        self.pred_loss_fn = pred_loss_fn\n",
    "        self.g_optim = g_optim\n",
    "        self.d_optim_1 = d_optim_1\n",
    "        self.d_optim_2 = d_optim_2\n",
    "        \n",
    "        \n",
    "    def build_generator(self):\n",
    "        activation = 'relu'\n",
    "        inputs = Input(shape=(self.x_dim, ))\n",
    "        \n",
    "        h = Dense(128)(inputs)\n",
    "        h = Activation(activation)(h)\n",
    "        h = Dense(64)(h)\n",
    "        h = Activation(activation)(h)\n",
    "        \n",
    "        outputs = Dense(self.y_dim)(h)\n",
    "        outputs = Activation('softmax')(outputs)\n",
    "        \n",
    "        return Model(inputs, outputs, name='generator')\n",
    "        \n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        activation = 'relu'\n",
    "        inputs_x = Input(shape=(self.x_dim, ))\n",
    "        inputs_y = Input(shape=(self.y_dim, ))\n",
    "        \n",
    "        inputs = Concatenate()([inputs_x, inputs_y])\n",
    "        \n",
    "        h = Dense(64)(inputs)\n",
    "        h = Activation(activation)(h)\n",
    "        h = Dense(32)(h)\n",
    "        h = Activation(activation)(h)\n",
    "    \n",
    "        outputs = Dense(1, activation='softplus')(h)\n",
    "        \n",
    "        return Model([inputs_x, inputs_y], outputs, name='discriminator')\n",
    "    \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x = data['X']\n",
    "        y = data['y']\n",
    "        \n",
    "        fake_labels = tf.ones((self.batch_size, 1))\n",
    "        real_labels = tf.ones((self.batch_size, 1))*0\n",
    "        labels = tf.concat([real_labels, fake_labels], 0)\n",
    "\n",
    "        # discriminator\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            fake = self.generator(x)\n",
    "\n",
    "            d1_pred_x = self.discriminator_1([x, y])\n",
    "            d1_pred_fake = self.discriminator_1([x, fake])\n",
    "            \n",
    "            d1_loss = tf.reduce_mean(-self.alpha*tf.math.log(d1_pred_x) + d1_pred_fake)\n",
    "            \n",
    "            d2_pred_x = self.discriminator_2([x, y])\n",
    "            d2_pred_fake = self.discriminator_2([x, fake])\n",
    "            \n",
    "            d2_loss = tf.reduce_mean(d2_pred_x - self.beta*tf.math.log(d2_pred_fake))\n",
    "\n",
    "        grads1 = tape1.gradient(d1_loss, self.discriminator_1.trainable_weights)\n",
    "        self.d_optim_1.apply_gradients(zip(grads1, self.discriminator_1.trainable_weights))\n",
    "        \n",
    "        grads2 = tape2.gradient(d2_loss, self.discriminator_2.trainable_weights)\n",
    "        self.d_optim_2.apply_gradients(zip(grads2, self.discriminator_2.trainable_weights))\n",
    "        \n",
    "        \n",
    "        # generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake = self.generator(x)\n",
    "            pred_1 = self.discriminator_1([x, fake])\n",
    "            pred_2 = self.discriminator_2([x, fake])\n",
    "            \n",
    "            adv_loss = tf.reduce_mean(-pred_1 + self.beta*tf.math.log(pred_2))\n",
    "            pred_loss = self.pred_loss_fn(y, fake)\n",
    "            \n",
    "            g_loss = adv_loss + 100*pred_loss\n",
    "            \n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optim.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        return {'d1_loss': d1_loss, 'd2_loss':d2_loss, 'g_loss': g_loss, 'adv_loss': adv_loss, 'pred_loss': pred_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2gan = D2GAN(64, 7, batch_size)\n",
    "d2gan.compile(\n",
    "    adv_loss_fn = losses.BinaryCrossentropy(from_logits=True),\n",
    "    pred_loss_fn = losses.CategoricalCrossentropy(),\n",
    "    g_optim = optimizers.Adam(2e-4),\n",
    "    d_optim_1 = optimizers.Adam(2e-4),\n",
    "    d_optim_2 = optimizers.Adam(2e-4)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='pred_loss')\n",
    "\n",
    "d2gan.fit(tr_loader,\n",
    "      epochs=20,\n",
    "      callbacks=[ValCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = d2gan.generator.predict(emb_X)\n",
    "f1_score(df['level'].values, np.argmax(pred, 1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.argmax(pred, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = d2gan.generator.predict(emb_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.argmax(pred, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[np.where(np.max(pred, axis=1) < 0.7)] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "sub['level']=results\n",
    "sub.to_csv('./sample_d2gan_no7.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
