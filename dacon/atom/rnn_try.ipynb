{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "print(f'python {python_version()}')\n",
    "print(f'pandas {pd.__version__}')\n",
    "print(f'numpy {np.__version__}')\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import lightgbm as lgb\n",
    "from scipy import integrate\n",
    "import seaborn as sns\n",
    "print(f'lgb {lgb.__version__}')\n",
    "import operator\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from scipy.stats import norm, kurtosis\n",
    "from sklearn.metrics import make_scorer\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import Dense, Activation, BatchNormalization, AlphaDropout, Dropout, Add, Concatenate, Flatten, Lambda\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPooling1D, GlobalAveragePooling1D, AveragePooling1D, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential, Model, Input, load_model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(x):\n",
    "    return x*K.tanh(K.softplus(x))\n",
    "\n",
    "def custom_x_nn(y_true, y_pred):\n",
    "    _t, _p = y_true, y_pred\n",
    "    score = 0.5 * K.mean((K.square(_t - _p)/2e+04))\n",
    "    return score\n",
    "\n",
    "def custom_y_nn(y_true, y_pred):\n",
    "    _t, _p = y_true, y_pred\n",
    "    score = 0.5 * K.mean((K.square(_t - _p)/2e+04))\n",
    "    return score\n",
    "\n",
    "def custom_m_nn(y_true, y_pred):\n",
    "    _t, _p = y_true, y_pred\n",
    "    score = 0.5 * K.mean((K.square((_t - _p) / (_t + 1e-06)))) \n",
    "    return score\n",
    "\n",
    "def custom_v_nn(y_true, y_pred):\n",
    "    _t, _p = y_true, y_pred\n",
    "    score = 0.5 * K.mean((K.square((_t - _p) / (_t + 1e-06))))\n",
    "    return score\n",
    "\n",
    "\n",
    "def kaeri_metric_nn(y_true, y_pred):\n",
    "    return 0.5 * E1(y_true, y_pred) + 0.5 * E2(y_true, y_pred)\n",
    "\n",
    "\n",
    "### E1과 E2는 아래에 정의됨 ###\n",
    "\n",
    "def E1(y_true, y_pred):\n",
    "    _t, _ = tf.split(y_true, 2, 1)\n",
    "    _p, _ = tf.split(y_pred, 2, 1)\n",
    "    \n",
    "    return K.mean(K.sum(K.square(_t - _p), axis = 1) / 2e+04)\n",
    "\n",
    "\n",
    "def E2(y_true, y_pred):\n",
    "    _, _t = tf.split(y_true, 2, 1)\n",
    "    _, _p = tf.split(y_pred, 2, 1)\n",
    "    \n",
    "    return K.mean(K.sum(K.square((_t - _p) / (_t + 1e-06)), axis = 1))\n",
    "\n",
    "weight1 = np.array([1,1,0,0])\n",
    "weight2 = np.array([0,0,1,1])\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult))\n",
    "\n",
    "\n",
    "def my_loss_E1(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred)*weight1)/2e+04\n",
    "\n",
    "def my_loss_E2(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*weight2)\n",
    "\n",
    "def get_mfcc_features(train_df):\n",
    "    new_df = pd.DataFrame(index = train_df['id'].unique())\n",
    "    for name, group in tqdm(train_df.groupby(['id'])):\n",
    "        for i in range(1,5):\n",
    "            s_data = group['S' + str(i)].values\n",
    "            #time = get_wave_arrival_time_threshold(s_data, 500)\n",
    "            mfcc_data = get_mfcc_result(s_data)\n",
    "            for j in range(0, len(mfcc_data)):\n",
    "                new_df.loc[name, 'S' + str(i) + 'mfcc' + str(j)] = mfcc_data[j]\n",
    "\n",
    "   \n",
    "    return new_df\n",
    "\n",
    "def repair(data):\n",
    "    x = data.copy()\n",
    "    x[:,0:2] *= 400\n",
    "    x[:,2] *= 100\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.005\n",
    "    drop = 0.5\n",
    "    epochs_drop = 15.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    \n",
    "    lrate = max(1e-4, lrate)\n",
    "    return lrate\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
    "lrs = keras.callbacks.LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('./data/train_features.csv')\n",
    "train_Y = pd.read_csv('./data/train_target.csv')\n",
    "test_features = pd.read_csv('./data/test_features.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_train = get_stats_features(train_features)\n",
    "# rolling_train= get_rolling_features(train_features)\n",
    "# fre_train= get_frequency_features(train_features)\n",
    "mfcc_train = get_mfcc_features(train_features)\n",
    "# time_train = get_every_s2(train_features)\n",
    "\n",
    "# stats_test = get_stats_features(test_features)\n",
    "# rolling_test= get_rolling_features(test_features)\n",
    "# fre_test= get_frequency_features(test_features)\n",
    "mfcc_test = get_mfcc_features(test_features)\n",
    "# time_test = get_every_s2(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = mfcc_train[mfcc_train.columns[mfcc_train.columns.str.contains('S1')]]\n",
    "s2 = mfcc_train[mfcc_train.columns[mfcc_train.columns.str.contains('S2')]]\n",
    "s3 = mfcc_train[mfcc_train.columns[mfcc_train.columns.str.contains('S3')]]\n",
    "s4 = mfcc_train[mfcc_train.columns[mfcc_train.columns.str.contains('S4')]]\n",
    "\n",
    "q = np.zeros((32, 4))\n",
    "for i in mfcc_train.index:\n",
    "    n = np.hstack([s1.loc[i].values.reshape(-1, 1), \n",
    "                   s2.loc[i].values.reshape(-1, 1),\n",
    "                   s3.loc[i].values.reshape(-1, 1),\n",
    "                   s4.loc[i].values.reshape(-1, 1)])\n",
    "    q = np.vstack([q, n])\n",
    "    \n",
    "mfcc_ts_train = q.reshape((2801, 32, 4))[1:]\n",
    "\n",
    "\n",
    "s1 = mfcc_test[mfcc_test.columns[mfcc_test.columns.str.contains('S1')]]\n",
    "s2 = mfcc_test[mfcc_test.columns[mfcc_test.columns.str.contains('S2')]]\n",
    "s3 = mfcc_test[mfcc_test.columns[mfcc_test.columns.str.contains('S3')]]\n",
    "s4 = mfcc_test[mfcc_test.columns[mfcc_test.columns.str.contains('S4')]]\n",
    "\n",
    "q = np.zeros((32, 4))\n",
    "for i in mfcc_test.index:\n",
    "    n = np.hstack([s1.loc[i].values.reshape(-1, 1), \n",
    "                   s2.loc[i].values.reshape(-1, 1),\n",
    "                   s3.loc[i].values.reshape(-1, 1),\n",
    "                   s4.loc[i].values.reshape(-1, 1)])\n",
    "    q = np.vstack([q, n])\n",
    "    \n",
    "mfcc_ts_test = q.reshape((701, 32, 4))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.zeros((32, 4))\n",
    "\n",
    "for i in range(2800):\n",
    "    n1 = (np.fft.fft(train_features['S1'].loc[i*375:(i+1)*375], norm='ortho').imag[:32].reshape(-1, 1))\n",
    "    n2 = (np.fft.fft(train_features['S2'].loc[i*375:(i+1)*375], norm='ortho').imag[:32].reshape(-1, 1))\n",
    "    n3 =(np.fft.fft(train_features['S3'].loc[i*375:(i+1)*375], norm='ortho').imag[:32].reshape(-1, 1))\n",
    "    n4 =(np.fft.fft(train_features['S4'].loc[i*375:(i+1)*375], norm='ortho').imag[:32].reshape(-1, 1))\n",
    "    n1 /= np.max(abs(n1))\n",
    "    n2 /= np.max(abs(n2))\n",
    "    n3 /= np.max(abs(n3))\n",
    "    n4 /= np.max(abs(n4))\n",
    "    d = np.hstack([n1, n2, n3, n4])\n",
    "    n = np.vstack([n, d])\n",
    "    \n",
    "fre_ts_imag_train = n.reshape((2801, 32, 4))[1:]\n",
    "\n",
    "n = np.zeros((32, 4))\n",
    "\n",
    "for i in range(700):\n",
    "    n1 = (np.fft.fft(test_features['S1'].values[i*375:(i+1)*375], norm='ortho').imag[:32].reshape(-1, 1))\n",
    "    n2 = (np.fft.fft(test_features['S2'].values[i*375:(i+1)*375], norm='ortho').imag[:32].reshape(-1, 1))\n",
    "    n3 =(np.fft.fft(test_features['S3'].values[i*375:(i+1)*375], norm='ortho').imag[:32].reshape(-1, 1))\n",
    "    n4 =(np.fft.fft(test_features['S4'].values[i*375:(i+1)*375], norm='ortho').imag[:32].reshape(-1, 1))\n",
    "    n1 /= np.max(abs(n1))\n",
    "    n2 /= np.max(abs(n2))\n",
    "    n3 /= np.max(abs(n3))\n",
    "    n4 /= np.max(abs(n4))\n",
    "    d = np.hstack([n1, n2, n3, n4])\n",
    "    n = np.vstack([n, d])\n",
    "\n",
    "fre_ts_imag_test = n.reshape((701, 32, 4))[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.zeros((32, 4))\n",
    "\n",
    "for i in range(2800):\n",
    "    n1 = (np.fft.fft(train_features['S1'].loc[i*375:(i+1)*375], norm='ortho').real[:32].reshape(-1, 1))\n",
    "    n2 = (np.fft.fft(train_features['S2'].loc[i*375:(i+1)*375], norm='ortho').real[:32].reshape(-1, 1))\n",
    "    n3 =(np.fft.fft(train_features['S3'].loc[i*375:(i+1)*375], norm='ortho').real[:32].reshape(-1, 1))\n",
    "    n4 =(np.fft.fft(train_features['S4'].loc[i*375:(i+1)*375], norm='ortho').real[:32].reshape(-1, 1))\n",
    "    n1 /= np.max(abs(n1))\n",
    "    n2 /= np.max(abs(n2))\n",
    "    n3 /= np.max(abs(n3))\n",
    "    n4 /= np.max(abs(n4))\n",
    "    d = np.hstack([n1, n2, n3, n4])\n",
    "    n = np.vstack([n, d])\n",
    "    \n",
    "fre_ts_real_train = n.reshape((2801, 32, 4))[1:]\n",
    "\n",
    "n = np.zeros((32, 4))\n",
    "\n",
    "for i in range(700):\n",
    "    n1 = (np.fft.fft(test_features['S1'].values[i*375:(i+1)*375], norm='ortho').real[:32].reshape(-1, 1))\n",
    "    n2 = (np.fft.fft(test_features['S2'].values[i*375:(i+1)*375], norm='ortho').real[:32].reshape(-1, 1))\n",
    "    n3 =(np.fft.fft(test_features['S3'].values[i*375:(i+1)*375], norm='ortho').real[:32].reshape(-1, 1))\n",
    "    n4 =(np.fft.fft(test_features['S4'].values[i*375:(i+1)*375], norm='ortho').real[:32].reshape(-1, 1))\n",
    "    n1 /= np.max(abs(n1))\n",
    "    n2 /= np.max(abs(n2))\n",
    "    n3 /= np.max(abs(n3))\n",
    "    n4 /= np.max(abs(n4))\n",
    "    d = np.hstack([n1, n2, n3, n4])\n",
    "    n = np.vstack([n, d])\n",
    "\n",
    "fre_ts_real_test = n.reshape((701, 32, 4))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_ts_train /= np.max(np.abs(mfcc_ts_train))\n",
    "mfcc_ts_test /= np.max(np.abs(mfcc_ts_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train = np.dstack([mfcc_ts_train, fre_ts_imag_train, fre_ts_real_train])\n",
    "ts_test = np.dstack([mfcc_ts_test, fre_ts_imag_test, fre_ts_real_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_features.iloc[:,2:].values.reshape((2800,375,4))\n",
    "test_X = test_features.iloc[:,2:].values.reshape((700,375,4))\n",
    "train_y = train_Y.iloc[:,1:].values\n",
    "# train_y[:,0:2]/=400\n",
    "# train_y[:,2]/=100\n",
    "\n",
    "\n",
    "tr_X, te_X, tr_y, te_y = train_test_split(ts_train, train_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_target):\n",
    "    inputs = Input(shape = (32, 12))\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, activation=tf.nn.elu, kernel_initializer='he_normal'))(inputs)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, activation=tf.nn.elu, kernel_initializer='he_normal'))(x)\n",
    "    \n",
    "    x1 = GlobalAveragePooling1D()(x)\n",
    "    x2 = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Concatenate()([x1, x2])\n",
    "    \n",
    "    x = Dense(128, activation=mish, kernel_initializer='he_normal')(x)\n",
    "    x = Dense(64, activation=mish, kernel_initializer='he_normal')(x)\n",
    "    \n",
    "    outputs = Dense(4, activation=mish, kernel_initializer='he_normal')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "\n",
    "    global weight2\n",
    "    if train_target == 1: # only for M\n",
    "        weight2 = np.array([0,0,1,0])\n",
    "    elif train_target == 2: # only for V\n",
    "        weight2 = np.array([0,0,0,1])\n",
    "       \n",
    "    if train_target == 0:\n",
    "        model.compile(#loss='mae',\n",
    "            loss=my_loss_E1,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['mae']\n",
    "                 )\n",
    "    else:\n",
    "        model.compile(#loss='mae',\n",
    "            loss=my_loss_E2,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['mae']\n",
    "                 )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn = build_model(0)\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X,Y):\n",
    "    MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "        \n",
    "    history = model.fit(X, Y,\n",
    "                  epochs=250,\n",
    "#                   batch_size=256,\n",
    "                  shuffle=True,\n",
    "                  validation_split=0.2,\n",
    "                  verbose = 2,\n",
    "                  callbacks=[es, lrs])\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "    plt.show()    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "te_ys = pd.DataFrame(np.zeros_like(te_y), columns=['X', 'Y', 'M', 'V'])\n",
    "\n",
    "models = [None for _ in range(3)]\n",
    "for train_target in range(3):\n",
    "#     model = set_model(train_target)\n",
    "    model = build_model(train_target)\n",
    "    models[train_target] = train(model,tr_X, tr_y)    \n",
    "#     best_model = load_best_model(train_target)\n",
    "\n",
    "    pred_data_test = models[train_target].predict(ts_test)\n",
    "    val_pred =  models[train_target].predict(te_X)\n",
    "\n",
    "    \n",
    "    if train_target == 0: # x,y 학습\n",
    "        sub.iloc[:,1] = pred_data_test[:,0]#*400\n",
    "        sub.iloc[:,2] = pred_data_test[:,1]#*400\n",
    "        te_ys.iloc[:,0] = val_pred[:,0]#*400\n",
    "        te_ys.iloc[:,1] = val_pred[:,1]#*400\n",
    "        \n",
    "    elif train_target == 1: # m 학습\n",
    "        sub.iloc[:,3] = pred_data_test[:,2]#*100\n",
    "        te_ys.iloc[:,2] = val_pred[:,2]#*100\n",
    "\n",
    "    elif train_target == 2: # v 학습\n",
    "        sub.iloc[:,4] = pred_data_test[:,3]\n",
    "        te_ys.iloc[:,3] = val_pred[:,3]\n",
    "        \n",
    "# val_score = kaeri_metric(repair(te_y), te_ys)\n",
    "val_score = kaeri_metric(te_y, te_ys)\n",
    "print(val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
