{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "print(f'python {python_version()}')\n",
    "print(f'pandas {pd.__version__}')\n",
    "print(f'numpy {np.__version__}')\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import lightgbm as lgb\n",
    "from scipy import integrate\n",
    "import seaborn as sns\n",
    "print(f'lgb {lgb.__version__}')\n",
    "import operator\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from scipy.stats import norm, kurtosis\n",
    "from sklearn.metrics import make_scorer\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import Dense, Activation, BatchNormalization, AlphaDropout, Dropout, Add, Concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, Lambda, GlobalMaxPooling2D, SeparableConv2D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling1D, Lambda, GlobalMaxPooling1D, SeparableConv1D\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential, Model, Input, load_model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(x):\n",
    "    return x*K.tanh(K.softplus(x))\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.002\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    \n",
    "    lrate = max(5e-5, lrate)\n",
    "    return lrate\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "lrs = keras.callbacks.LearningRateScheduler(step_decay, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('./data/train_features.csv')\n",
    "train_target = pd.read_csv('./data/train_target.csv')\n",
    "test_features = pd.read_csv('./data/test_features.csv')\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_features.iloc[:,2:].values.reshape((2800,375,4, 1))\n",
    "test_X = test_features.iloc[:,2:].values.reshape((700,375,4, 1))\n",
    "train_y = train_target.iloc[:,1:].values\n",
    "\n",
    "s1 = train_X.squeeze()[:,:,0]\n",
    "s2 = train_X.squeeze()[:,:,1]\n",
    "s3 = train_X.squeeze()[:,:,2]\n",
    "s4 = train_X.squeeze()[:,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "S1 = list(map(lambda x: librosa.feature.melspectrogram(x, sr=375, n_mels=n_mels, hop_length=160), s1))\n",
    "S2 = list(map(lambda x: librosa.feature.melspectrogram(x, sr=375, n_mels=n_mels, hop_length=160), s2))\n",
    "S3 = list(map(lambda x: librosa.feature.melspectrogram(x, sr=375, n_mels=n_mels, hop_length=160), s3))\n",
    "S4 = list(map(lambda x: librosa.feature.melspectrogram(x, sr=375, n_mels=n_mels, hop_length=160), s4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel1 = np.array(list(map(lambda x: librosa.power_to_db(x, ref=np.max), S1)))\n",
    "mel2 = np.array(list(map(lambda x: librosa.power_to_db(x, ref=np.max), S2)))\n",
    "mel3 = np.array(list(map(lambda x: librosa.power_to_db(x, ref=np.max), S3)))\n",
    "mel4 = np.array(list(map(lambda x: librosa.power_to_db(x, ref=np.max), S4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel1 /= mel1.min()\n",
    "mel2 /= mel2.min()\n",
    "mel3 /= mel3.min()\n",
    "mel4 /= mel4.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.dstack([mel1, mel2, mel3, mel4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X, te_X,tr_y, te_y = train_test_split(train_X, train_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm1 = np.expand_dims(mel1, -1)\n",
    "mm2 = np.expand_dims(mel2, -1)\n",
    "mm3 = np.expand_dims(mel3, -1)\n",
    "mm4 = np.expand_dims(mel4, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.dstack([mm1, mm2, mm3, mm4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = train_features.iloc[:,2:].values.reshape((2800,375,4, 1))\n",
    "# test_X = test_features.iloc[:,2:].values.reshape((700,375,4, 1))\n",
    "# train_y = train_target.iloc[:,1:].values\n",
    "\n",
    "# tr_X, te_X,tr_y, te_y = train_test_split(train_X, train_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X /= np.abs(train_X).max()\n",
    "train_X /= np.abs(train_X).max()\n",
    "\n",
    "tr_X, te_X,tr_y, te_y = train_test_split(train_X, train_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = np.array([1,1,0,0])\n",
    "weight2 = np.array([0,0,1,1])\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult))\n",
    "\n",
    "\n",
    "def my_loss_E1(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred)*weight1)/2e+04\n",
    "\n",
    "def my_loss_E2(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "def set_model(train_target, shape):  # 0:x,y, 1:m, 2:v\n",
    "    \n",
    "    activation = tf.nn.swish\n",
    "    padding = 'same'\n",
    "    model = Sequential()\n",
    "    kernel_initializer = 'he_normal'\n",
    "    nf = 64\n",
    "    fs = 2\n",
    "    ps = 1\n",
    "    strides = 2\n",
    "\n",
    "    model.add(Conv2D(nf,fs, padding=padding,input_shape=shape, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=ps, strides=strides))\n",
    "\n",
    "    model.add(Conv2D(nf,fs, padding=padding, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=ps, strides=strides))\n",
    "\n",
    "    model.add(Conv2D(nf*2,fs, padding=padding, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=ps, strides=strides))\n",
    "\n",
    "    model.add(Conv2D(nf*2,fs, padding=padding, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=ps, strides=strides))\n",
    "\n",
    "    model.add(Conv2D(nf*4,fs, padding=padding, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=ps, strides=strides))\n",
    "\n",
    "    model.add(Conv2D(nf*4,fs, padding=padding, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=ps, strides=strides))\n",
    "    \n",
    "    model.add(Conv2D(nf*8,fs, padding=padding, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPooling2D(pool_size=ps, strides=strides))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1024, activation =activation, kernel_initializer=kernel_initializer))\n",
    "    model.add(Dense(512, activation =activation, kernel_initializer=kernel_initializer))\n",
    "    model.add(Dense(256, activation =activation, kernel_initializer=kernel_initializer))\n",
    "    model.add(Dense(128, activation =activation, kernel_initializer=kernel_initializer))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, kernel_initializer=kernel_initializer))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "\n",
    "    global weight2\n",
    "    if train_target == 1: # only for M\n",
    "        weight2 = np.array([0,0,1,0])\n",
    "    else: # only for V\n",
    "        weight2 = np.array([0,0,0,1])\n",
    "       \n",
    "    if train_target==0:\n",
    "        model.compile(loss=my_loss_E1,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    else:\n",
    "        model.compile(loss=my_loss_E2,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "        \n",
    "#     model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn = set_model(0, train_X.shape[1:])\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X,Y):\n",
    "    MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "\n",
    "    history = model.fit(X, Y,\n",
    "                  epochs=500,\n",
    "                  batch_size=32,\n",
    "                  shuffle=True,\n",
    "                  validation_split=0.2,\n",
    "                  verbose = 1,\n",
    "                  callbacks=[es, lrs])\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "    plt.show()    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "te_ys = pd.DataFrame(np.zeros_like(te_y), columns=['X', 'Y', 'M', 'V'])\n",
    "\n",
    "models = [None for _ in range(3)]\n",
    "for train_target in range(3):\n",
    "    model = set_model(train_target, shape=train_X.shape[1:])\n",
    "#     model = build_model(train_target)\n",
    "    models[train_target] = train(model,tr_X, tr_y)    \n",
    "#     best_model = load_best_model(train_target)\n",
    "\n",
    "#     pred_data_test = models[train_target].predict(test_X)\n",
    "    val_pred =  models[train_target].predict(te_X)\n",
    "\n",
    "    \n",
    "    if train_target == 0: # x,y 학습\n",
    "        sub.iloc[:,1] = pred_data_test[:,0]\n",
    "        sub.iloc[:,2] = pred_data_test[:,1]\n",
    "        te_ys.iloc[:,0] = val_pred[:,0]\n",
    "        te_ys.iloc[:,1] = val_pred[:,1]\n",
    "\n",
    "    elif train_target == 1: # m 학습\n",
    "        sub.iloc[:,3] = pred_data_test[:,2]\n",
    "        te_ys.iloc[:,2] = val_pred[:,2]\n",
    "\n",
    "    elif train_target == 2: # v 학습\n",
    "        sub.iloc[:,4] = pred_data_test[:,3]\n",
    "        te_ys.iloc[:,3] = val_pred[:,3]\n",
    "        \n",
    "val_score = kaeri_metric(te_y, te_ys)\n",
    "print(val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'./model/{round(val_score, 5)}'\n",
    "os.mkdir(model_path)\n",
    "for i in range(3):\n",
    "    models[i].save(model_path+f'/model_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
