{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from time import time\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "\n",
    "from pystacknet.pystacknet import StackNetRegressor\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./data/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr = pd.read_csv('./data/train.csv', index_col='id')\n",
    "te = pd.read_csv('./data/test.csv', index_col='id')\n",
    "\n",
    "target_cols = ['hhb', 'hbo2', 'ca', 'na']\n",
    "target = tr[target_cols].copy()\n",
    "tr = tr.drop(target_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = tr.columns[tr.columns.str.contains('src')]\n",
    "dst = tr.columns[tr.columns.str.contains('dst')]\n",
    "n_mels = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:30<00:00, 325.53it/s]\n"
     ]
    }
   ],
   "source": [
    "stack = np.array([0 for _ in range(35)])\n",
    "for i in tqdm(tr.index):\n",
    "    temp = tr[dst].loc[i]#.replace(0, np.nan)\n",
    "    temp.index = range(35)\n",
    "    stack = np.vstack([stack, temp.interpolate(method='akima', order=5, limit_direction='both').values.flatten()])\n",
    "    \n",
    "stack = stack[1:]\n",
    "tr[dst] = pd.DataFrame(stack, columns=dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:31<00:00, 315.52it/s]\n"
     ]
    }
   ],
   "source": [
    "stack = np.array([0 for _ in range(35)])\n",
    "for i in tqdm(te.index):\n",
    "    temp = te[dst].loc[i]#.replace(0, np.nan)\n",
    "    temp.index = range(35)\n",
    "    stack = np.vstack([stack, temp.interpolate(method='akima', order=5, limit_direction='both').values.flatten()])\n",
    "    \n",
    "stack = stack[1:]\n",
    "te[dst] = pd.DataFrame(stack, columns=dst, index=te.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sum(te[dst].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = tr[dst].loc[0].replace(0, np.nan)\n",
    "# temp.index = range(35)\n",
    "# q = temp.interpolate(method='spline', order=3)\n",
    "\n",
    "# plt.plot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = tr[dst].loc[0].replace(0, np.nan)\n",
    "# temp.index = range(35)\n",
    "# q = temp.interpolate(method='akima')\n",
    "\n",
    "# plt.plot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr[dst] = tr[dst].interpolate(axis=1)\n",
    "\n",
    "te[dst] = te[dst].interpolate(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_dst=tr[dst]\n",
    "te_dst=te[dst]\n",
    "\n",
    "tr_dst.loc[tr_dst['700_dst'].isnull(),'700_dst']=tr_dst.loc[tr_dst['700_dst'].isnull(),'710_dst']\n",
    "tr_dst.loc[tr_dst['690_dst'].isnull(),'690_dst']=tr_dst.loc[tr_dst['690_dst'].isnull(),'700_dst']\n",
    "tr_dst.loc[tr_dst['680_dst'].isnull(),'680_dst']=tr_dst.loc[tr_dst['680_dst'].isnull(),'690_dst']\n",
    "tr_dst.loc[tr_dst['670_dst'].isnull(),'670_dst']=tr_dst.loc[tr_dst['670_dst'].isnull(),'680_dst']\n",
    "tr_dst.loc[tr_dst['660_dst'].isnull(),'660_dst']=tr_dst.loc[tr_dst['660_dst'].isnull(),'670_dst']\n",
    "tr_dst.loc[tr_dst['650_dst'].isnull(),'650_dst']=tr_dst.loc[tr_dst['650_dst'].isnull(),'660_dst']\n",
    "\n",
    "te_dst.loc[te_dst['700_dst'].isnull(),'700_dst']=te_dst.loc[te_dst['700_dst'].isnull(),'710_dst']\n",
    "te_dst.loc[te_dst['690_dst'].isnull(),'690_dst']=te_dst.loc[te_dst['690_dst'].isnull(),'700_dst']\n",
    "te_dst.loc[te_dst['680_dst'].isnull(),'680_dst']=te_dst.loc[te_dst['680_dst'].isnull(),'690_dst']\n",
    "te_dst.loc[te_dst['670_dst'].isnull(),'670_dst']=te_dst.loc[te_dst['670_dst'].isnull(),'680_dst']\n",
    "te_dst.loc[te_dst['660_dst'].isnull(),'660_dst']=te_dst.loc[te_dst['660_dst'].isnull(),'670_dst']\n",
    "te_dst.loc[te_dst['650_dst'].isnull(),'650_dst']=te_dst.loc[te_dst['650_dst'].isnull(),'660_dst']\n",
    "\n",
    "tr[dst] = tr_dst\n",
    "te[dst] = te_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, col in zip(src, dst):\n",
    "    tr[col+'_sq1'] = tr[col] * (tr['rho'] ** 2)\n",
    "    te[col+'_sq1'] = te[col] * (te['rho']** 2)\n",
    "    \n",
    "    tr[col+'_sq2'] = tr[col] * (np.exp(tr['rho']))\n",
    "    te[col+'_sq2'] = te[col] * (np.exp(te['rho']))\n",
    "    \n",
    "    tr[col+'_subt_sq1'] =  tr[s] - tr[col+'_sq1']\n",
    "    tr[col+'_subt_sq2'] =  tr[s] - tr[col+'_sq2']\n",
    "\n",
    "    te[col+'_subt_sq1'] =  te[s] - te[col+'_sq1']\n",
    "    te[col+'_subt_sq2'] =  te[s] - te[col+'_sq2']\n",
    "\n",
    "    \n",
    "sq1_dst = [c for c in tr.columns if 'sq1' in c and 'subt' not in c]\n",
    "sq2_dst = [c for c in tr.columns if 'sq2' in c and 'subt' not in c]\n",
    "subt1_dst = [c for c in tr.columns if 'subt' in c and 'sq' in c]\n",
    "subt2_dst = [c for c in tr.columns if 'sq2' in c and 'subt' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['sq1_dst_mean'] = tr[sq1_dst].mean(axis=1)\n",
    "tr['sq1_dst_std'] = tr[sq1_dst].std(axis=1)\n",
    "\n",
    "tr['sq2_dst_mean'] = tr[sq2_dst].mean(axis=1)\n",
    "tr['sq2_dst_std'] = tr[sq2_dst].std(axis=1)\n",
    "\n",
    "tr['subt1_dst_mean'] = tr[subt1_dst].mean(axis=1)\n",
    "tr['subt1_dst_std'] = tr[subt1_dst].std(axis=1)\n",
    "\n",
    "tr['subt2_dst_mean'] = tr[subt2_dst].mean(axis=1)\n",
    "tr['subt2_dst_std'] = tr[subt2_dst].std(axis=1)\n",
    "\n",
    "# test\n",
    "te['sq1_dst_mean'] = te[sq1_dst].mean(axis=1)\n",
    "te['sq1_dst_std'] = te[sq1_dst].std(axis=1)\n",
    "\n",
    "te['sq2_dst_mean'] = te[sq2_dst].mean(axis=1)\n",
    "te['sq2_dst_std'] = te[sq2_dst].std(axis=1)\n",
    "\n",
    "te['subt1_dst_mean'] = te[subt1_dst].mean(axis=1)\n",
    "te['subt1_dst_std'] = te[subt1_dst].std(axis=1)\n",
    "\n",
    "te['subt2_dst_mean'] = te[subt2_dst].mean(axis=1)\n",
    "te['subt2_dst_std'] = te[subt2_dst].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in src:\n",
    "    tr[col+'_sq1'] = tr[col] * (tr['rho'] ** 2)\n",
    "    te[col+'_sq1'] = te[col] * (te['rho']** 2)\n",
    "    \n",
    "    tr[col+'_sq2'] = tr[col] * (np.exp(tr['rho']))\n",
    "    te[col+'_sq2'] = te[col] * (np.exp(te['rho']))\n",
    "    \n",
    "sq1_src = [c for c in tr.columns if 'sq1' in c and 'src' in c]\n",
    "sq2_src = [c for c in tr.columns if 'sq2' in c and 'src' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in tqdm([5, 10, 15, 20]):\n",
    "#     tr_roll_mean = tr[dst].rolling(5, axis=1).mean().dropna(axis=1)\n",
    "#     newc = ['dst_rolling_mean_'+str(k)+'_'+str(i+1) for i in range(len(tr_roll_mean.columns))]\n",
    "#     tr_roll_mean.columns = newc\n",
    "#     tr_roll_std = tr[dst].rolling(5, axis=1).std().dropna(axis=1)\n",
    "#     newc =['dst_rolling_std_'+str(k)+'_'+str(i+1) for i in range(len(tr_roll_std.columns))]\n",
    "#     tr_roll_std.columns = newc\n",
    "        \n",
    "#     tr = pd.concat([tr, tr_roll_mean, tr_roll_std], axis=1)\n",
    "    \n",
    "# rolling_5_mean = [c for c in tr.columns if 'dst_rolling_mean_5' in c]\n",
    "# rolling_10_mean = [c for c in tr.columns if 'dst_rolling_mean_10' in c]\n",
    "# rolling_15_mean = [c for c in tr.columns if 'dst_rolling_mean_15' in c]\n",
    "# rolling_20_mean = [c for c in tr.columns if 'dst_rolling_mean_20' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te= te.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_x = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_chroma=tr[dst]\n",
    "te_chroma=te[dst]\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_chroma))):   \n",
    "    tr_temp.append(librosa.feature.chroma_stft(tr_chroma.values[i,:]).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_chroma))):\n",
    "    te_temp.append(librosa.feature.chroma_stft(te_chroma.values[i,:]).flatten())\n",
    "    \n",
    "chroma_cols = []\n",
    "\n",
    "for i in range(12):\n",
    "    chroma_cols.append('dst' + '_chroma_' + str(i+1))\n",
    "    \n",
    "tr_chroma = pd.DataFrame(tr_temp, columns=chroma_cols, index=tr_chroma.index)\n",
    "te_chroma = pd.DataFrame(te_temp, columns=chroma_cols, index=te_chroma.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_chroma], axis=1)\n",
    "te = pd.concat([te, te_chroma], axis=1)\n",
    "\n",
    "tr['chroma'+'_mean'] = tr[chroma_cols].mean(axis=1)\n",
    "tr['chroma'+'_std'] = tr[chroma_cols].std(axis=1)\n",
    "\n",
    "te['chroma'+'_mean'] = te[chroma_cols].mean(axis=1)\n",
    "te['chroma'+'_std'] = te[chroma_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 24\n",
    "\n",
    "tr_mel=tr[dst]\n",
    "te_mel=te[dst]\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_mel))):   \n",
    "    tr_temp.append(librosa.feature.melspectrogram(tr_mel.values[i,:], n_mels=n_mels).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_mel))):\n",
    "    te_temp.append(librosa.feature.melspectrogram(te_mel.values[i,:], n_mels=n_mels).flatten())\n",
    "    \n",
    "mel_cols = []\n",
    "for i in range(n_mels):\n",
    "    mel_cols.append('dst_melspec_'+str(i+1))\n",
    "    \n",
    "tr_mel = pd.DataFrame(tr_temp, columns=mel_cols, index=tr_mel.index)\n",
    "te_mel = pd.DataFrame(te_temp, columns=mel_cols, index=te_mel.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_mel], axis=1)\n",
    "te = pd.concat([te, te_mel], axis=1)\n",
    "\n",
    "tr['mel'+'_mean'] = tr[mel_cols].mean(axis=1)\n",
    "tr['mel'+'_std'] = tr[mel_cols].std(axis=1)\n",
    "\n",
    "te['mel'+'_mean'] = te[mel_cols].mean(axis=1)\n",
    "te['mel'+'_std'] = te[mel_cols].std(axis=1)\n",
    "\n",
    "# rnn_x.append(np.array(list(map(lambda x: x.reshape(-1, 1), tr_mel.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_mfcc=tr_chroma\n",
    "te_mfcc=te_chroma\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_mfcc))):   \n",
    "    tr_temp.append(librosa.feature.mfcc(tr_mfcc.values[i,:]).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_mfcc))):\n",
    "    te_temp.append(librosa.feature.mfcc(te_mfcc.values[i,:]).flatten())\n",
    "    \n",
    "mfcc_cols = []\n",
    "for i in range(20):\n",
    "    mfcc_cols.append('dst_chroma_mfcc_'+str(i+1))\n",
    "    \n",
    "tr_mfcc = pd.DataFrame(tr_temp, columns=mfcc_cols, index=tr_mfcc.index)\n",
    "te_mfcc = pd.DataFrame(te_temp, columns=mfcc_cols, index=te_mfcc.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_mfcc], axis=1)\n",
    "te = pd.concat([te, te_mfcc], axis=1)\n",
    "\n",
    "tr['mfcc'+'_mean'] = tr[mfcc_cols].mean(axis=1)\n",
    "tr['mfcc'+'_std'] = tr[mfcc_cols].std(axis=1)\n",
    "\n",
    "te['mfcc'+'_mean'] = te[mfcc_cols].mean(axis=1)\n",
    "te['mfcc'+'_std'] = te[mfcc_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_mel=tr_chroma\n",
    "te_mel=te_chroma\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_mel))):   \n",
    "    tr_temp.append(librosa.feature.melspectrogram(tr_mel.values[i,:], n_mels=n_mels).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_mel))):\n",
    "    te_temp.append(librosa.feature.melspectrogram(te_mel.values[i,:], n_mels=n_mels).flatten())\n",
    "    \n",
    "mel_cols = []\n",
    "for i in range(n_mels):\n",
    "    mel_cols.append('dst_chroma_melspec_'+str(i+1))\n",
    "    \n",
    "tr_mel = pd.DataFrame(tr_temp, columns=mel_cols, index=tr_mel.index)\n",
    "te_mel = pd.DataFrame(te_temp, columns=mel_cols, index=te_mel.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_mel], axis=1)\n",
    "te = pd.concat([te, te_mel], axis=1)\n",
    "\n",
    "tr['mel_chroma'+'_mean'] = tr[mel_cols].mean(axis=1)\n",
    "tr['mel_chroma'+'_std'] = tr[mel_cols].std(axis=1)\n",
    "\n",
    "te['mel_chroma'+'_mean'] = te[mel_cols].mean(axis=1)\n",
    "te['mel_chroma'+'_std'] = te[mel_cols].std(axis=1)\n",
    "\n",
    "# rnn_x.append(np.array(list(map(lambda x: x.reshape(-1, 1), tr_mel.values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dst sq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:23<00:00, 427.96it/s]\n",
      "100%|██████████| 10000/10000 [00:22<00:00, 453.64it/s]\n"
     ]
    }
   ],
   "source": [
    "tr_chroma=tr[sq2_dst]\n",
    "te_chroma=te[sq2_dst]\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_chroma))):   \n",
    "    tr_temp.append(librosa.feature.chroma_stft(tr_chroma.values[i,:]).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_chroma))):\n",
    "    te_temp.append(librosa.feature.chroma_stft(te_chroma.values[i,:]).flatten())\n",
    "    \n",
    "chroma_cols = []\n",
    "\n",
    "for i in range(12):\n",
    "    chroma_cols.append('dst' + '_chroma2_' + str(i+1))\n",
    "    \n",
    "tr_chroma = pd.DataFrame(tr_temp, columns=chroma_cols, index=tr_chroma.index)\n",
    "te_chroma = pd.DataFrame(te_temp, columns=chroma_cols, index=te_chroma.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_chroma], axis=1)\n",
    "te = pd.concat([te, te_chroma], axis=1)\n",
    "\n",
    "tr['chroma2'+'_mean'] = tr[chroma_cols].mean(axis=1)\n",
    "tr['chroma2'+'_std'] = tr[chroma_cols].std(axis=1)\n",
    "\n",
    "te['chroma2'+'_mean'] = te[chroma_cols].mean(axis=1)\n",
    "te['chroma2'+'_std'] = te[chroma_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:18<00:00, 535.94it/s]\n",
      "100%|██████████| 10000/10000 [00:18<00:00, 551.07it/s]\n"
     ]
    }
   ],
   "source": [
    "tr_mel=tr[sq2_dst]\n",
    "te_mel=te[sq2_dst]\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_mel))):   \n",
    "    tr_temp.append(librosa.feature.melspectrogram(tr_mel.values[i,:], n_mels=n_mels).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_mel))):\n",
    "    te_temp.append(librosa.feature.melspectrogram(te_mel.values[i,:], n_mels=n_mels).flatten())\n",
    "    \n",
    "mel_cols = []\n",
    "for i in range(n_mels):\n",
    "    mel_cols.append('dst_melspec2_'+str(i+1))\n",
    "    \n",
    "tr_mel = pd.DataFrame(tr_temp, columns=mel_cols, index=tr_mel.index)\n",
    "te_mel = pd.DataFrame(te_temp, columns=mel_cols, index=te_mel.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_mel], axis=1)\n",
    "te = pd.concat([te, te_mel], axis=1)\n",
    "\n",
    "tr['mel2'+'_mean'] = tr[mel_cols].mean(axis=1)\n",
    "tr['mel2'+'_std'] = tr[mel_cols].std(axis=1)\n",
    "\n",
    "te['mel2'+'_mean'] = te[mel_cols].mean(axis=1)\n",
    "te['mel2'+'_std'] = te[mel_cols].std(axis=1)\n",
    "\n",
    "# rnn_x.append(np.array(list(map(lambda x: x.reshape(-1, 1), tr_mel.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:58<00:00, 170.89it/s]\n",
      "100%|██████████| 10000/10000 [00:56<00:00, 178.32it/s]\n"
     ]
    }
   ],
   "source": [
    "tr_mfcc=tr_chroma\n",
    "te_mfcc=te_chroma\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_mfcc))):   \n",
    "    tr_temp.append(librosa.feature.mfcc(tr_mfcc.values[i,:]).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_mfcc))):\n",
    "    te_temp.append(librosa.feature.mfcc(te_mfcc.values[i,:]).flatten())\n",
    "    \n",
    "mfcc_cols = []\n",
    "for i in range(20):\n",
    "    mfcc_cols.append('dst_chroma_mfcc2_'+str(i+1))\n",
    "    \n",
    "tr_mfcc = pd.DataFrame(tr_temp, columns=mfcc_cols, index=tr_mfcc.index)\n",
    "te_mfcc = pd.DataFrame(te_temp, columns=mfcc_cols, index=te_mfcc.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_mfcc], axis=1)\n",
    "te = pd.concat([te, te_mfcc], axis=1)\n",
    "\n",
    "tr['mfcc2'+'_mean'] = tr[mfcc_cols].mean(axis=1)\n",
    "tr['mfcc2'+'_std'] = tr[mfcc_cols].std(axis=1)\n",
    "\n",
    "te['mfcc2'+'_mean'] = te[mfcc_cols].mean(axis=1)\n",
    "te['mfcc2'+'_std'] = te[mfcc_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:18<00:00, 534.98it/s]\n",
      "100%|██████████| 10000/10000 [00:17<00:00, 561.29it/s]\n"
     ]
    }
   ],
   "source": [
    "tr_mel=tr_chroma\n",
    "te_mel=te_chroma\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_mel))):   \n",
    "    tr_temp.append(librosa.feature.melspectrogram(tr_mel.values[i,:], n_mels=n_mels).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_mel))):\n",
    "    te_temp.append(librosa.feature.melspectrogram(te_mel.values[i,:], n_mels=n_mels).flatten())\n",
    "    \n",
    "mel_cols = []\n",
    "for i in range(n_mels):\n",
    "    mel_cols.append('dst_chroma2_melspec_'+str(i+1))\n",
    "    \n",
    "tr_mel = pd.DataFrame(tr_temp, columns=mel_cols, index=tr_mel.index)\n",
    "te_mel = pd.DataFrame(te_temp, columns=mel_cols, index=te_mel.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_mel], axis=1)\n",
    "te = pd.concat([te, te_mel], axis=1)\n",
    "\n",
    "tr['mel_chroma2'+'_mean'] = tr[mel_cols].mean(axis=1)\n",
    "tr['mel_chroma2'+'_std'] = tr[mel_cols].std(axis=1)\n",
    "\n",
    "te['mel_chroma2'+'_mean'] = te[mel_cols].mean(axis=1)\n",
    "te['mel_chroma2'+'_std'] = te[mel_cols].std(axis=1)\n",
    "\n",
    "# rnn_x.append(np.array(list(map(lambda x: x.reshape(-1, 1), tr_mel.values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dst subt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_chroma=tr[subt1_dst]\n",
    "te_chroma=te[subt1_dst]\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_chroma))):   \n",
    "    tr_temp.append(librosa.feature.chroma_stft(tr_chroma.values[i,:]).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_chroma))):\n",
    "    te_temp.append(librosa.feature.chroma_stft(te_chroma.values[i,:]).flatten())\n",
    "    \n",
    "chroma_cols = []\n",
    "\n",
    "for i in range(12):\n",
    "    chroma_cols.append('dst_sub_chroma1_' + str(i+1))\n",
    "    \n",
    "tr_chroma = pd.DataFrame(tr_temp, columns=chroma_cols, index=tr_chroma.index)\n",
    "te_chroma = pd.DataFrame(te_temp, columns=chroma_cols, index=te_chroma.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_chroma], axis=1)\n",
    "te = pd.concat([te, te_chroma], axis=1)\n",
    "\n",
    "tr['chroma1_sub'+'_mean'] = tr[chroma_cols].mean(axis=1)\n",
    "tr['chroma1_sub'+'_std'] = tr[chroma_cols].std(axis=1)\n",
    "\n",
    "te['chroma1_sub'+'_mean'] = te[chroma_cols].mean(axis=1)\n",
    "te['chroma1_sub'+'_std'] = te[chroma_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_mfcc=tr_chroma\n",
    "te_mfcc=te_chroma\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_mfcc))):   \n",
    "    tr_temp.append(librosa.feature.mfcc(tr_mfcc.values[i,:], htk=True).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_mfcc))):\n",
    "    te_temp.append(librosa.feature.mfcc(te_mfcc.values[i,:], htk=True).flatten())\n",
    "    \n",
    "mfcc_cols = []\n",
    "for i in range(20):\n",
    "    mfcc_cols.append('dst_sub_chroma1_mfcc2_'+str(i+1))\n",
    "    \n",
    "tr_mfcc = pd.DataFrame(tr_temp, columns=mfcc_cols, index=tr_mfcc.index)\n",
    "te_mfcc = pd.DataFrame(te_temp, columns=mfcc_cols, index=te_mfcc.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_mfcc], axis=1)\n",
    "te = pd.concat([te, te_mfcc], axis=1)\n",
    "\n",
    "tr['mfcc1_sub'+'_mean'] = tr[mfcc_cols].mean(axis=1)\n",
    "tr['mfcc1_sub'+'_std'] = tr[mfcc_cols].std(axis=1)\n",
    "\n",
    "te['mfcc1_sub'+'_mean'] = te[mfcc_cols].mean(axis=1)\n",
    "te['mfcc1_sub'+'_std'] = te[mfcc_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_chroma=tr[subt2_dst]\n",
    "te_chroma=te[subt2_dst]\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_chroma))):   \n",
    "    tr_temp.append(librosa.feature.chroma_stft(tr_chroma.values[i,:]).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_chroma))):\n",
    "    te_temp.append(librosa.feature.chroma_stft(te_chroma.values[i,:]).flatten())\n",
    "    \n",
    "chroma_cols = []\n",
    "\n",
    "for i in range(12):\n",
    "    chroma_cols.append('dst_sub_chroma2_' + str(i+1))\n",
    "    \n",
    "tr_chroma = pd.DataFrame(tr_temp, columns=chroma_cols, index=tr_chroma.index)\n",
    "te_chroma = pd.DataFrame(te_temp, columns=chroma_cols, index=te_chroma.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_chroma], axis=1)\n",
    "te = pd.concat([te, te_chroma], axis=1)\n",
    "\n",
    "tr['chroma2_sub'+'_mean'] = tr[chroma_cols].mean(axis=1)\n",
    "tr['chroma2_sub'+'_std'] = tr[chroma_cols].std(axis=1)\n",
    "\n",
    "te['chroma2_sub'+'_mean'] = te[chroma_cols].mean(axis=1)\n",
    "te['chroma2_sub'+'_std'] = te[chroma_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_mfcc=tr_chroma\n",
    "te_mfcc=te_chroma\n",
    "\n",
    "tr_temp = []\n",
    "te_temp = []\n",
    "\n",
    "for i in tqdm(range(len(tr_mfcc))):   \n",
    "    tr_temp.append(librosa.feature.mfcc(tr_mfcc.values[i,:]).flatten()) # - np.mean(tr[dst].values[0,:])\n",
    "\n",
    "for i in tqdm(range(len(te_mfcc))):\n",
    "    te_temp.append(librosa.feature.mfcc(te_mfcc.values[i,:]).flatten())\n",
    "    \n",
    "mfcc_cols = []\n",
    "for i in range(20):\n",
    "    mfcc_cols.append('dst_sub_chroma1_mfcc2_'+str(i+1))\n",
    "    \n",
    "tr_mfcc = pd.DataFrame(tr_temp, columns=mfcc_cols, index=tr_mfcc.index)\n",
    "te_mfcc = pd.DataFrame(te_temp, columns=mfcc_cols, index=te_mfcc.index)\n",
    "\n",
    "tr = pd.concat([tr, tr_mfcc], axis=1)\n",
    "te = pd.concat([te, te_mfcc], axis=1)\n",
    "\n",
    "tr['mfcc1_sub'+'_mean'] = tr[mfcc_cols].mean(axis=1)\n",
    "tr['mfcc1_sub'+'_std'] = tr[mfcc_cols].std(axis=1)\n",
    "\n",
    "te['mfcc1_sub'+'_mean'] = te[mfcc_cols].mean(axis=1)\n",
    "te['mfcc1_sub'+'_std'] = te[mfcc_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librosa.feature.melspectrogram(tr[dst].values[i,:], n_mels=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_real=tr[dst]\n",
    "alpha_imag=tr[dst]\n",
    "\n",
    "beta_real=te[dst]\n",
    "beta_imag=te[dst]\n",
    "\n",
    "for i in tqdm(alpha_real.index):\n",
    "    alpha_real.loc[i]=alpha_real.loc[i] - alpha_real.loc[i].mean()\n",
    "    alpha_imag.loc[i]=alpha_imag.loc[i] - alpha_real.loc[i].mean()\n",
    "    \n",
    "    alpha_real.loc[i] = np.fft.fft(alpha_real.loc[i], norm='ortho').real\n",
    "    alpha_imag.loc[i] = np.fft.fft(alpha_imag.loc[i], norm='ortho').imag\n",
    "\n",
    "    \n",
    "for i in tqdm(beta_real.index):\n",
    "    beta_real.loc[i]=beta_real.loc[i] - beta_real.loc[i].mean()\n",
    "    beta_imag.loc[i]=beta_imag.loc[i] - beta_imag.loc[i].mean()\n",
    "    \n",
    "    beta_real.loc[i] = np.fft.fft(beta_real.loc[i], norm='ortho').real\n",
    "    beta_imag.loc[i] = np.fft.fft(beta_imag.loc[i], norm='ortho').imag\n",
    "    \n",
    "real_part=[]\n",
    "imag_part=[]\n",
    "\n",
    "for col in dst:\n",
    "    real_part.append(col + '_fft_real')\n",
    "    imag_part.append(col + '_fft_imag')\n",
    "    \n",
    "alpha_real.columns=real_part\n",
    "alpha_imag.columns=imag_part\n",
    "alpha = pd.concat((alpha_real, alpha_imag), axis=1)\n",
    "\n",
    "beta_real.columns=real_part\n",
    "beta_imag.columns=imag_part\n",
    "beta=pd.concat((beta_real, beta_imag), axis=1)\n",
    "\n",
    "tr=pd.concat((tr, alpha), axis=1)\n",
    "te=pd.concat((te, beta), axis=1)\n",
    "\n",
    "tr['dst_fft'+'_mean'] = tr[alpha.columns].mean(axis=1)\n",
    "tr['dst_fft'+'_std'] = tr[alpha.columns].std(axis=1)\n",
    "\n",
    "te['dst_fft'+'_mean'] = te[beta.columns].mean(axis=1)\n",
    "te['dst_fft'+'_std'] = te[beta.columns].std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FT sq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:20<00:00, 495.38it/s]\n",
      "100%|██████████| 10000/10000 [00:20<00:00, 492.63it/s]\n"
     ]
    }
   ],
   "source": [
    "alpha_real=tr[sq2_dst]\n",
    "alpha_imag=tr[sq2_dst]\n",
    "\n",
    "beta_real=te[sq2_dst]\n",
    "beta_imag=te[sq2_dst]\n",
    "\n",
    "for i in tqdm(alpha_real.index):\n",
    "    alpha_real.loc[i]=alpha_real.loc[i] - alpha_real.loc[i].mean()\n",
    "    alpha_imag.loc[i]=alpha_imag.loc[i] - alpha_real.loc[i].mean()\n",
    "    \n",
    "    alpha_real.loc[i] = np.fft.fft(alpha_real.loc[i], norm='ortho').real\n",
    "    alpha_imag.loc[i] = np.fft.fft(alpha_imag.loc[i], norm='ortho').imag\n",
    "\n",
    "    \n",
    "for i in tqdm(beta_real.index):\n",
    "    beta_real.loc[i]=beta_real.loc[i] - beta_real.loc[i].mean()\n",
    "    beta_imag.loc[i]=beta_imag.loc[i] - beta_imag.loc[i].mean()\n",
    "    \n",
    "    beta_real.loc[i] = np.fft.fft(beta_real.loc[i], norm='ortho').real\n",
    "    beta_imag.loc[i] = np.fft.fft(beta_imag.loc[i], norm='ortho').imag\n",
    "    \n",
    "real_part=[]\n",
    "imag_part=[]\n",
    "\n",
    "for col in sq2_dst:\n",
    "    real_part.append(col + '_fft_real')\n",
    "    imag_part.append(col + '_fft_imag')\n",
    "    \n",
    "alpha_real.columns=real_part\n",
    "alpha_imag.columns=imag_part\n",
    "alpha = pd.concat((alpha_real, alpha_imag), axis=1)\n",
    "\n",
    "beta_real.columns=real_part\n",
    "beta_imag.columns=imag_part\n",
    "beta=pd.concat((beta_real, beta_imag), axis=1)\n",
    "\n",
    "tr=pd.concat((tr, alpha), axis=1)\n",
    "te=pd.concat((te, beta), axis=1)\n",
    "\n",
    "tr['dst_sq2_fft'+'_mean'] = tr[alpha.columns].mean(axis=1)\n",
    "tr['dst_sq2_fft'+'_std'] = tr[alpha.columns].std(axis=1)\n",
    "\n",
    "te['dst_sq2_fft'+'_mean'] = te[beta.columns].mean(axis=1)\n",
    "te['dst_sq2_fft'+'_std'] = te[beta.columns].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.sum(tr.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_real=tr[src]\n",
    "alpha_imag=tr[src]\n",
    "\n",
    "beta_real=te[src]\n",
    "beta_imag=te[src]\n",
    "\n",
    "for i in tqdm(alpha_real.index):\n",
    "    alpha_real.loc[i]=alpha_real.loc[i] - alpha_real.loc[i].mean()\n",
    "    alpha_imag.loc[i]=alpha_imag.loc[i] - alpha_real.loc[i].mean()\n",
    "    \n",
    "    alpha_real.loc[i] = np.fft.fft(alpha_real.loc[i], norm='ortho').real\n",
    "    alpha_imag.loc[i] = np.fft.fft(alpha_imag.loc[i], norm='ortho').imag\n",
    "\n",
    "    \n",
    "for i in tqdm(beta_real.index):\n",
    "    beta_real.loc[i]=beta_real.loc[i] - beta_real.loc[i].mean()\n",
    "    beta_imag.loc[i]=beta_imag.loc[i] - beta_imag.loc[i].mean()\n",
    "    \n",
    "    beta_real.loc[i] = np.fft.fft(beta_real.loc[i], norm='ortho').real\n",
    "    beta_imag.loc[i] = np.fft.fft(beta_imag.loc[i], norm='ortho').imag\n",
    "    \n",
    "real_part=[]\n",
    "imag_part=[]\n",
    "\n",
    "for col in src:\n",
    "    real_part.append(col + '_fft_real')\n",
    "    imag_part.append(col + '_fft_imag')\n",
    "    \n",
    "alpha_real.columns=real_part\n",
    "alpha_imag.columns=imag_part\n",
    "alpha = pd.concat((alpha_real, alpha_imag), axis=1)\n",
    "\n",
    "beta_real.columns=real_part\n",
    "beta_imag.columns=imag_part\n",
    "beta=pd.concat((beta_real, beta_imag), axis=1)\n",
    "\n",
    "tr=pd.concat((tr, alpha), axis=1)\n",
    "te=pd.concat((te, beta), axis=1)\n",
    "\n",
    "tr['src_fft'+'_mean'] = tr[alpha.columns].mean(axis=1)\n",
    "tr['src_fft'+'_std'] = tr[alpha.columns].std(axis=1)\n",
    "\n",
    "te['src_fft'+'_mean'] = te[beta.columns].mean(axis=1)\n",
    "te['src_fft'+'_std'] = te[beta.columns].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_real=tr[sq2_src]\n",
    "alpha_imag=tr[sq2_src]\n",
    "\n",
    "beta_real=te[sq2_src]\n",
    "beta_imag=te[sq2_src]\n",
    "\n",
    "for i in tqdm(alpha_real.index):\n",
    "    alpha_real.loc[i]=alpha_real.loc[i] - alpha_real.loc[i].mean()\n",
    "    alpha_imag.loc[i]=alpha_imag.loc[i] - alpha_real.loc[i].mean()\n",
    "    \n",
    "    alpha_real.loc[i] = np.fft.fft(alpha_real.loc[i], norm='ortho').real\n",
    "    alpha_imag.loc[i] = np.fft.fft(alpha_imag.loc[i], norm='ortho').imag\n",
    "\n",
    "    \n",
    "for i in tqdm(beta_real.index):\n",
    "    beta_real.loc[i]=beta_real.loc[i] - beta_real.loc[i].mean()\n",
    "    beta_imag.loc[i]=beta_imag.loc[i] - beta_imag.loc[i].mean()\n",
    "    \n",
    "    beta_real.loc[i] = np.fft.fft(beta_real.loc[i], norm='ortho').real\n",
    "    beta_imag.loc[i] = np.fft.fft(beta_imag.loc[i], norm='ortho').imag\n",
    "    \n",
    "real_part=[]\n",
    "imag_part=[]\n",
    "\n",
    "for col in src:\n",
    "    real_part.append(col + '_fft_real')\n",
    "    imag_part.append(col + '_fft_imag')\n",
    "    \n",
    "alpha_real.columns=real_part\n",
    "alpha_imag.columns=imag_part\n",
    "alpha = pd.concat((alpha_real, alpha_imag), axis=1)\n",
    "\n",
    "beta_real.columns=real_part\n",
    "beta_imag.columns=imag_part\n",
    "beta=pd.concat((beta_real, beta_imag), axis=1)\n",
    "\n",
    "tr=pd.concat((tr, alpha), axis=1)\n",
    "te=pd.concat((te, beta), axis=1)\n",
    "\n",
    "tr['sq2_src_fft'+'_mean'] = tr[alpha.columns].mean(axis=1)\n",
    "tr['sq2_src_fft'+'_std'] = tr[alpha.columns].std(axis=1)\n",
    "\n",
    "te['sq2_src_fft'+'_mean'] = te[beta.columns].mean(axis=1)\n",
    "te['sq2_src_fft'+'_std'] = te[beta.columns].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.sum(te.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_LGB = {\n",
    "    'num_leaves': (100, 800), \n",
    "    'min_data_in_leaf': (0, 150),\n",
    "    'bagging_fraction' : (0.3, 0.9),\n",
    "    'feature_fraction' : (0.3, 0.9),\n",
    "#     'learning_rate': (0.01, 1),\n",
    "    'min_child_weight': (0.01, 3),   \n",
    "    'reg_alpha': (0.01, 3), \n",
    "    'reg_lambda': (0.01, 1),\n",
    "    'max_depth':(6, 29),\n",
    "    'n_estimators': (64, 512)\n",
    "}\n",
    "\n",
    "def build_lgb(x, y, init_points=10, n_iter=10, cv=2, param=True, verbose=2, is_test=False):\n",
    "    train_X, test_X, train_y, test_y = train_test_split(x.values, y.values, test_size=0.3, random_state=SEED, shuffle=True)\n",
    "    def LGB_bayesian(\n",
    "        #learning_rate,\n",
    "        num_leaves, \n",
    "        bagging_fraction,\n",
    "        feature_fraction,\n",
    "        min_child_weight, \n",
    "        min_data_in_leaf,\n",
    "        max_depth,\n",
    "        reg_alpha,\n",
    "        reg_lambda,\n",
    "        n_estimators\n",
    "         ):\n",
    "        # LightGBM expects next three parameters need to be integer. \n",
    "        num_leaves = int(num_leaves)\n",
    "        min_data_in_leaf = int(min_data_in_leaf)\n",
    "        max_depth = int(max_depth)\n",
    "\n",
    "        assert type(num_leaves) == int\n",
    "        assert type(min_data_in_leaf) == int\n",
    "        assert type(max_depth) == int\n",
    "\n",
    "\n",
    "        params = {\n",
    "                  'num_leaves': num_leaves, \n",
    "                  'min_data_in_leaf': min_data_in_leaf,\n",
    "                  'min_child_weight': min_child_weight,\n",
    "                  'bagging_fraction' : bagging_fraction,\n",
    "                  'feature_fraction' : feature_fraction,\n",
    "                  'learning_rate' : 0.05,\n",
    "                  'max_depth': max_depth,\n",
    "                  'reg_alpha': reg_alpha,\n",
    "                  'reg_lambda': reg_lambda,\n",
    "                  'objective': 'regression',\n",
    "                  'save_binary': True,\n",
    "                  'seed': SEED,\n",
    "                  'feature_fraction_seed': SEED,\n",
    "                  'bagging_seed': SEED,\n",
    "                  'drop_seed': SEED,\n",
    "                  'data_random_seed': SEED,\n",
    "                  'boosting': 'gbdt', ## some get better result using 'dart'\n",
    "                  'verbose': 1,\n",
    "                  'boost_from_average': True,\n",
    "                  'metric':'mae',\n",
    "                  'n_estimators': int(n_estimators),\n",
    "                  'n_jobs': -1,\n",
    "                  'tree_learner ': 'voting'\n",
    "        }    \n",
    "\n",
    "        ## set reg options\n",
    "        reg = lgb.LGBMRegressor(**params)\n",
    "        m_reg = MultiOutputRegressor(reg)\n",
    "#         m_reg.fit(train_X, train_y)\n",
    "#         score = mean_absolute_error(test_y, m_reg.predict(test_X))\n",
    "        score = cross_val_score(m_reg, x, y, cv=cv, scoring='neg_mean_absolute_error').mean()\n",
    "\n",
    "        return score\n",
    "    \n",
    "    optimizer = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=SEED, verbose=verbose)\n",
    "    init_points = init_points\n",
    "    n_iter = n_iter\n",
    "\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "    \n",
    "    param_lgb = {\n",
    "        'min_data_in_leaf': int(optimizer.max['params']['min_data_in_leaf']), \n",
    "        'num_leaves': int(optimizer.max['params']['num_leaves']), \n",
    "        'learning_rate': 0.05,\n",
    "        'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "        'bagging_fraction': optimizer.max['params']['bagging_fraction'], \n",
    "        'feature_fraction': optimizer.max['params']['feature_fraction'],\n",
    "        'reg_lambda': optimizer.max['params']['reg_lambda'],\n",
    "        'reg_alpha': optimizer.max['params']['reg_alpha'],\n",
    "        'max_depth': int(optimizer.max['params']['max_depth']), \n",
    "        'objective': 'regression',\n",
    "        'save_binary': True,\n",
    "        'seed': SEED,\n",
    "        'feature_fraction_seed': SEED,\n",
    "        'bagging_seed': SEED,\n",
    "        'drop_seed': SEED,\n",
    "        'data_random_seed': SEED,\n",
    "        'boosting_type': 'gbdt',  # also consider 'dart'\n",
    "        'verbose': 1,\n",
    "        'boost_from_average': True,\n",
    "        'metric':'mae',\n",
    "        'n_estimators': int(optimizer.max['params']['n_estimators']),\n",
    "        'n_jobs': -1,\n",
    "        'tree_learner ': 'voting'\n",
    "    }\n",
    "\n",
    "    params = param_lgb.copy()\n",
    "\n",
    "    reg = lgb.LGBMRegressor(**params)\n",
    "    lgb_reg = MultiOutputRegressor(reg)\n",
    "    lgb_reg.fit(x.values, y.values)\n",
    "\n",
    "    if param:\n",
    "        return lgb_reg, params\n",
    "    else:\n",
    "        return lgb_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr = tr.drop(ctd, axis=1)\n",
    "# te = te.drop(ctd, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ttr = tr.drop(list(src)+list(dst), axis=1)\n",
    "# te = te.drop(src, axis=1)\n",
    "ttr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base: -1.135\n",
    "# base2: -1.132\n",
    "\n",
    "# rolling: -1.125\n",
    "# rolling2: -1.126\n",
    "\n",
    "# mfcc: -1.114\n",
    "# mfcc2: -1.114\n",
    "\n",
    "# fft: -1.091\n",
    "# fft2: -1.089\n",
    "\n",
    "# mfcc_sq1: -1.092\n",
    "# mfcc_sq1_2: \n",
    "\n",
    "# fft_sq2: -1.079\n",
    "# fft_sq2_2: -1.078\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr = tr.drop(list(imps['col'][imps['imp']==0].values), axis=1)\n",
    "tte = te.drop(list(imps['col'][imps['imp']==0].values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | max_depth | min_ch... | min_da... | n_esti... | num_le... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-1.094   \u001b[0m | \u001b[0m 0.5247  \u001b[0m | \u001b[0m 0.8704  \u001b[0m | \u001b[0m 22.84   \u001b[0m | \u001b[0m 1.8     \u001b[0m | \u001b[0m 23.4    \u001b[0m | \u001b[0m 133.9   \u001b[0m | \u001b[0m 140.7   \u001b[0m | \u001b[0m 2.6     \u001b[0m | \u001b[0m 0.6051  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-1.076   \u001b[0m | \u001b[95m 0.7248  \u001b[0m | \u001b[95m 0.3124  \u001b[0m | \u001b[95m 28.31   \u001b[0m | \u001b[95m 2.499   \u001b[0m | \u001b[95m 31.85   \u001b[0m | \u001b[95m 145.5   \u001b[0m | \u001b[95m 228.4   \u001b[0m | \u001b[95m 0.9197  \u001b[0m | \u001b[95m 0.5295  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-1.061   \u001b[0m | \u001b[95m 0.5592  \u001b[0m | \u001b[95m 0.4747  \u001b[0m | \u001b[95m 20.07   \u001b[0m | \u001b[95m 0.4271  \u001b[0m | \u001b[95m 43.82   \u001b[0m | \u001b[95m 228.1   \u001b[0m | \u001b[95m 419.2   \u001b[0m | \u001b[95m 2.358   \u001b[0m | \u001b[95m 0.2077  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-1.121   \u001b[0m | \u001b[0m 0.6085  \u001b[0m | \u001b[0m 0.6554  \u001b[0m | \u001b[0m 7.068   \u001b[0m | \u001b[0m 1.827   \u001b[0m | \u001b[0m 25.58   \u001b[0m | \u001b[0m 93.14   \u001b[0m | \u001b[0m 764.2   \u001b[0m | \u001b[0m 2.897   \u001b[0m | \u001b[0m 0.8103  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-1.07    \u001b[0m | \u001b[0m 0.4828  \u001b[0m | \u001b[0m 0.3586  \u001b[0m | \u001b[0m 21.74   \u001b[0m | \u001b[0m 1.326   \u001b[0m | \u001b[0m 18.31   \u001b[0m | \u001b[0m 285.8   \u001b[0m | \u001b[0m 124.1   \u001b[0m | \u001b[0m 2.729   \u001b[0m | \u001b[0m 0.2662  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-1.065   \u001b[0m | \u001b[0m 0.6975  \u001b[0m | \u001b[0m 0.487   \u001b[0m | \u001b[0m 17.96   \u001b[0m | \u001b[0m 1.645   \u001b[0m | \u001b[0m 27.73   \u001b[0m | \u001b[0m 498.4   \u001b[0m | \u001b[0m 642.6   \u001b[0m | \u001b[0m 2.819   \u001b[0m | \u001b[0m 0.8959  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-1.076   \u001b[0m | \u001b[0m 0.3558  \u001b[0m | \u001b[0m 0.8965  \u001b[0m | \u001b[0m 13.56   \u001b[0m | \u001b[0m 2.135   \u001b[0m | \u001b[0m 19.31   \u001b[0m | \u001b[0m 282.6   \u001b[0m | \u001b[0m 128.1   \u001b[0m | \u001b[0m 2.636   \u001b[0m | \u001b[0m 0.452   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-1.061   \u001b[0m | \u001b[95m 0.4512  \u001b[0m | \u001b[95m 0.4536  \u001b[0m | \u001b[95m 14.66   \u001b[0m | \u001b[95m 1.592   \u001b[0m | \u001b[95m 47.37   \u001b[0m | \u001b[95m 225.9   \u001b[0m | \u001b[95m 414.1   \u001b[0m | \u001b[95m 0.723   \u001b[0m | \u001b[95m 0.8059  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-1.133   \u001b[0m | \u001b[0m 0.3352  \u001b[0m | \u001b[0m 0.64    \u001b[0m | \u001b[0m 20.42   \u001b[0m | \u001b[0m 1.856   \u001b[0m | \u001b[0m 6.756   \u001b[0m | \u001b[0m 162.8   \u001b[0m | \u001b[0m 401.5   \u001b[0m | \u001b[0m 0.6407  \u001b[0m | \u001b[0m 0.2846  \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m-1.057   \u001b[0m | \u001b[95m 0.6276  \u001b[0m | \u001b[95m 0.4006  \u001b[0m | \u001b[95m 12.24   \u001b[0m | \u001b[95m 2.549   \u001b[0m | \u001b[95m 78.41   \u001b[0m | \u001b[95m 268.6   \u001b[0m | \u001b[95m 417.5   \u001b[0m | \u001b[95m 1.083   \u001b[0m | \u001b[95m 0.06755 \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-1.071   \u001b[0m | \u001b[0m 0.5263  \u001b[0m | \u001b[0m 0.8733  \u001b[0m | \u001b[0m 13.54   \u001b[0m | \u001b[0m 2.929   \u001b[0m | \u001b[0m 27.97   \u001b[0m | \u001b[0m 493.6   \u001b[0m | \u001b[0m 641.4   \u001b[0m | \u001b[0m 1.374   \u001b[0m | \u001b[0m 0.9303  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-1.065   \u001b[0m | \u001b[0m 0.8794  \u001b[0m | \u001b[0m 0.8267  \u001b[0m | \u001b[0m 14.97   \u001b[0m | \u001b[0m 2.707   \u001b[0m | \u001b[0m 34.08   \u001b[0m | \u001b[0m 497.2   \u001b[0m | \u001b[0m 637.9   \u001b[0m | \u001b[0m 2.201   \u001b[0m | \u001b[0m 0.6491  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-1.087   \u001b[0m | \u001b[0m 0.7288  \u001b[0m | \u001b[0m 0.7682  \u001b[0m | \u001b[0m 6.249   \u001b[0m | \u001b[0m 0.3364  \u001b[0m | \u001b[0m 96.05   \u001b[0m | \u001b[0m 222.3   \u001b[0m | \u001b[0m 450.8   \u001b[0m | \u001b[0m 1.876   \u001b[0m | \u001b[0m 0.17    \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-1.059   \u001b[0m | \u001b[0m 0.7706  \u001b[0m | \u001b[0m 0.3867  \u001b[0m | \u001b[0m 18.66   \u001b[0m | \u001b[0m 2.836   \u001b[0m | \u001b[0m 44.13   \u001b[0m | \u001b[0m 274.5   \u001b[0m | \u001b[0m 384.4   \u001b[0m | \u001b[0m 0.6263  \u001b[0m | \u001b[0m 0.0404  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-1.072   \u001b[0m | \u001b[0m 0.4465  \u001b[0m | \u001b[0m 0.6122  \u001b[0m | \u001b[0m 8.779   \u001b[0m | \u001b[0m 1.694   \u001b[0m | \u001b[0m 28.63   \u001b[0m | \u001b[0m 300.5   \u001b[0m | \u001b[0m 436.6   \u001b[0m | \u001b[0m 0.3069  \u001b[0m | \u001b[0m 0.4213  \u001b[0m |\n",
      "| \u001b[95m 16      \u001b[0m | \u001b[95m-1.056   \u001b[0m | \u001b[95m 0.6406  \u001b[0m | \u001b[95m 0.3463  \u001b[0m | \u001b[95m 12.27   \u001b[0m | \u001b[95m 0.8126  \u001b[0m | \u001b[95m 79.99   \u001b[0m | \u001b[95m 267.5   \u001b[0m | \u001b[95m 420.6   \u001b[0m | \u001b[95m 1.341   \u001b[0m | \u001b[95m 0.4704  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "lgb_reg = build_lgb(tr, target, 6, 10, param=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_reg2 = build_lgb(ttr, target, 3, 5, param=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf = RandomForestRegressor(max_depth=13, n_jobs=-1, random_state=42)\n",
    "\n",
    "# np.mean(cross_val_score(rf, tr, target, scoring='neg_mean_absolute_error', cv=3))\n",
    "rf.fit(ttr, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imps = pd.DataFrame()\n",
    "\n",
    "imps['col'] = ttr.columns\n",
    "imps['imp'] = 0\n",
    "for i in range(4):\n",
    "    imps['imp'] += lgb_reg.estimators_[i].booster_.feature_importance(importance_type='gain')\n",
    "# imps['imp'] = rf.feature_importances_\n",
    "\n",
    "imps.sort_values('imp', ascending=1).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(imps['col'][imps['imp']==0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_reg.predict(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise('eo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = PowerTransformer().fit(tr)\n",
    "tr2 = pd.DataFrame(sc.transform(tr), columns=tr.columns)\n",
    "\n",
    "sc = PowerTransformer().fit(ttr)\n",
    "ttr2 = pd.DataFrame(sc.transform(ttr), columns=ttr.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_reg3 = build_lgb(tr2, target, 3, 0, param=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_reg4 = build_lgb(ttr2, target, 3, 0, param=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imps = pd.DataFrame()\n",
    "\n",
    "imps['col'] = ttr.columns\n",
    "\n",
    "imps['imp'] = lgb_reg.estimators_[3].booster_.feature_importance(importance_type='gain')\n",
    "\n",
    "imps.sort_values('imp', ascending=1).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regs = {}\n",
    "for tc in target_cols:\n",
    "    temp = {}\n",
    "    for u in tr['rho'].unique():\n",
    "        ttr = tr[tr['rho']==u].drop('rho', axis=1)\n",
    "        tr_y = target[tr['rho']==u][tc]\n",
    "    \n",
    "        temp[u] = build_lgb(ttr, tr_y, 6, 5, param=False)\n",
    "        \n",
    "    regs[tc] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tc in target_cols:\n",
    "    temp = sub[tc].astype('float32')\n",
    "    for u in tr['rho'].unique():\n",
    "        tte = te[te['rho']==u].drop('rho', axis=1)\n",
    "\n",
    "        pred = regs[tc][u].predict(tte)\n",
    "        temp[te['rho']==u] = pred\n",
    "    sub[tc] = temp\n",
    "    \n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(target[target_cols[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lgb_reg.predict(te)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[target_cols] = pred\n",
    "sub.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imps = pd.DataFrame()\n",
    "\n",
    "imps['col'] = tr.columns\n",
    "\n",
    "imps['imp'] = lgb_reg.estimators_[3].booster_.feature_importance(importance_type='gain')\n",
    "\n",
    "imps.sort_values('imp', ascending=1).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.groupby('rho').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat\n",
    "reg = cat.CatBoostRegressor(eval_metric='MAE',\n",
    "                            task_type='CPU',\n",
    "                            early_stopping_rounds = 100,\n",
    "                            iterations = 10000,\n",
    "                            metric_period = 10000,\n",
    "                            grow_policy = 'Lossguide',\n",
    "                            l2_leaf_reg = 0.01,\n",
    "                            random_seed=SEED)\n",
    "\n",
    "cat_reg = MultiOutputRegressor(reg)\n",
    "# cat_reg.fit(tr, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=150,\n",
    "                                max_depth=9, \n",
    "                                max_features='sqrt', \n",
    "                                random_state=SEED)\n",
    "\n",
    "pca = PCA(10, random_state=SEED, whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    [lgb_reg, pca, cat_reg],\n",
    "    [rf]\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = StackNetRegressor(models, \n",
    "                           metric=\"mae\", \n",
    "                           folds=2,\n",
    "                           restacking=False,\n",
    "                           random_state=SEED,\n",
    "                           n_jobs=-1, \n",
    "                           verbose=1)\n",
    "\n",
    "model.fit(tr, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lgb_reg.predict(tte)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[target_cols] = pred\n",
    "sub.to_csv('0618.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.sum(tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lasso = Lasso()\n",
    "ridge = Ridge()\n",
    "svr = SVR()\n",
    "rf = RandomForestRegressor(max_depth=13, n_jobs=-1, random_state=42)\n",
    "mlp = MLPRegressor([256, 128], learning_rate='adaptive', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['rho'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr = tr[tr['rho']==20].drop('rho', axis=1)\n",
    "tr_y = target[tr['rho']==20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(mlp, tr, target[target_cols[3]], scoring='neg_mean_absolute_error', cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = PowerTransformer().fit(tr)\n",
    "\n",
    "ttr = sc.transform(tr)\n",
    "# tte = sc.transform(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(lr, tr, target, scoring='neg_mean_absolute_error', cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(rf, tr, target, scoring='neg_mean_absolute_error', cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(rf, ttr, target, scoring='neg_mean_absolute_error', cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr = AdaBoostRegressor(lasso)\n",
    "adr = MultiOutputRegressor(adr)\n",
    "\n",
    "np.mean(cross_val_score(adr, tr, target, scoring='neg_mean_absolute_error', cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr = BaggingRegressor(lasso)\n",
    "bgr = MultiOutputRegressor(bgr)\n",
    "\n",
    "np.mean(cross_val_score(bgr, ttr, target, scoring='neg_mean_absolute_error', cv=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import Dense, Concatenate, Activation, BatchNormalization\n",
    "from keras.models import Input, Model, Sequential\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def mish(x):\n",
    "    return x*K.tanh(K.softplus(x))\n",
    "\n",
    "inputs = Input(shape = (tr.shape[-1], ))\n",
    "\n",
    "x = Dense(1024, kernel_initializer='he_normal')(inputs)\n",
    "x = BatchNormalization(momentum=0.8)(x)\n",
    "x = Activation(mish)(x)\n",
    "x = Dense(512, kernel_initializer='he_normal')(x)\n",
    "x = Activation(mish)(x)\n",
    "x = Dense(256, kernel_initializer='he_normal')(x)\n",
    "x = Activation(mish)(x)\n",
    "\n",
    "x = Dense(4, kernel_initializer='he_normal')(x)\n",
    "\n",
    "model = Model(inputs, x)\n",
    "\n",
    "model.compile(loss='mae', optimizer=optimizers.adam(lr=1e-4))\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttr = sc.transform(tr)\n",
    "model.fit(tr, target,\n",
    "         epochs=100,\n",
    "         validation_split=0.3,\n",
    "         callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
