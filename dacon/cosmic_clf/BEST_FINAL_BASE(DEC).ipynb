{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenient\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import Lambda, Dense, Input, Concatenate, BatchNormalization, InputSpec\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom layer for clustering module\n",
    "class ClusteringLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0,**kwargs):\n",
    "        \n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "#         self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        \n",
    "#         self.built = True\n",
    "        \n",
    "        super(ClusteringLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEC:\n",
    "    def __init__(self, d_dim, latent_dim, n_classes):\n",
    "        self.pretrained = False\n",
    "        # data structure\n",
    "        self.d_dim = d_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes # cluster num,\n",
    "        \n",
    "        # build model\n",
    "        self.enc = self.build_encoder()\n",
    "        self.dec = self.build_decoder()\n",
    "        self.clustering = ClusteringLayer(self.n_classes, name='clustering')\n",
    "        \n",
    "        # AE\n",
    "#         z = Input(shape = (self.d_dim[1], ))\n",
    "        self.ae = Model(self.enc.input, self.dec(self.enc.output), name='AE')\n",
    "        self.ae.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        # cluster\n",
    "        self.cl = Model(self.enc.input, self.clustering(self.enc.output), name='Cluster') # clustering moduel based on t-dist distance\n",
    "        self.cl.compile(optimizer='sgd', loss='kld') # use kld loss !!\n",
    "        \n",
    "        \n",
    "    def build_encoder(self):\n",
    "        inputs = Input(shape= (self.d_dim[1], ), name='input')\n",
    "        \n",
    "        x = Denseblock(1024)(inputs)\n",
    "        x = Denseblock(512)(x)\n",
    "        x = Denseblock(256)(x)\n",
    "        \n",
    "        outputs = Dense(self.latent_dim, name='output')(x)\n",
    "        \n",
    "        enc = Model(inputs, outputs, name='encoder')\n",
    "        \n",
    "        return enc\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        inputs = Input(shape = (self.latent_dim, ))\n",
    "        \n",
    "        x = Denseblock(256)(inputs)\n",
    "        x = Denseblock(512)(x)\n",
    "        x = Denseblock(1024)(x)\n",
    "        \n",
    "        outputs = Dense(self.d_dim[1])(x)\n",
    "        \n",
    "        dec = Model(inputs, outputs, name='decoder')\n",
    "        \n",
    "        return dec\n",
    "    \n",
    "    def target_distribution(self, q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "    \n",
    "    def pretrain(self, x, epochs=500, batch_size=1024*16):\n",
    "        print('pretraining ...')\n",
    "        es = EarlyStopping(patience=20, restore_best_weights=True, monitor='loss')\n",
    "        self.ae.fit(x, x, epochs=epochs, batch_size=batch_size, callbacks=[es], verbose=0)\n",
    "        self.pretrained = True\n",
    "        \n",
    "    def train(self, x, y=None, epochs=1e5, batch_size=1024*16, interval=256,tol=1e-3):\n",
    "        if self.pretrained == False:\n",
    "            self.pretrain(x)\n",
    "            print('finish pretrain')\n",
    "        # set inital point\n",
    "        kmeans = KMeans(n_clusters=self.n_classes, n_init=20)\n",
    "        y_pred = kmeans.fit_predict(self.enc.predict(x))\n",
    "        y_pred_last = np.copy(y_pred) # to compare performance\n",
    "        self.cl.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "        \n",
    "        # set aux dist\n",
    "        q = self.cl.predict(x, verbose=0)\n",
    "        p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "        for e in range(int(epochs)+1):\n",
    "            # clusetring\n",
    "            idx = np.random.randint(0, len(x), batch_size)\n",
    "            tr = x[idx]\n",
    "            loss = self.cl.train_on_batch(tr, p[idx])\n",
    "#             print('cl_loss:', loss)\n",
    "            \n",
    "            # update distribution\n",
    "            if not e%interval:\n",
    "                q = self.cl.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)\n",
    "                y_pred = self.predict(tr)\n",
    "                if y is not None:\n",
    "                    acc, nmi, ari = self.evaluate(y[idx], y_pred)\n",
    "                    print('cl_acc:', acc, 'nmi:', nmi, 'ari:', ari)\n",
    "            \n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if delta_label < tol and e != 0:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('finish train')\n",
    "                    break\n",
    "                    \n",
    "    def evaluate(self, y, y_pred):\n",
    "        acc = np.round(cl_acc(y, y_pred), 5)\n",
    "        nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n",
    "        ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n",
    "        \n",
    "        return acc, nmi, ari\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.array(list(map(lambda p: np.argmax(p), self.cl.predict(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Denseblock(n):\n",
    "    def f(x):\n",
    "        x = Dense(n)(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.3)(x)\n",
    "        \n",
    "        return x\n",
    "    return f\n",
    "\n",
    "def cl_acc(ypred, y):\n",
    "    s = np.unique(ypred)\n",
    "    t = np.unique(y)\n",
    "    \n",
    "    N = len(np.unique(y))\n",
    "    C = np.zeros((N, N), dtype = np.int32)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            idx = np.logical_and(ypred == s[i], y == t[j])\n",
    "            C[i][j] = np.count_nonzero(idx)\n",
    "    \n",
    "    # convert the C matrix to the 'true' cost\n",
    "    Cmax = np.amax(C)\n",
    "    C = Cmax - C\n",
    "    # \n",
    "    indices = np.array(list(map(lambda x: list(x), list(zip(*linear_sum_assignment(C))))))\n",
    "    row = indices[:][:, 0]\n",
    "    col = indices[:][:, 1]\n",
    "    # calculating the accuracy according to the optimal assignment\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        idx = np.logical_and(ypred == s[row[i]], y == t[col[i]] )\n",
    "        count += np.count_nonzero(idx)\n",
    "    \n",
    "    return 1.0*count/len(y)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['psfMag', 'fiberMag', 'petroMag', 'modelMag', '_u', '_g', '_r', '_i', '_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def exp(a):\n",
    "    if a < 0:\n",
    "        return -np.log(-a)\n",
    "    else:\n",
    "        return np.log(a)\n",
    "    \n",
    "def exp(a):\n",
    "    if a < 0:\n",
    "        return -(a ** 2)\n",
    "    else:\n",
    "        return (a ** 2)\n",
    "    \n",
    "tr = pd.read_csv('./data/train.csv')\n",
    "te = pd.read_csv('./data/test.csv')\n",
    "\n",
    "sub = pd.read_csv('./data/sample_submission.csv', index_col=0)\n",
    "\n",
    "column_number = {}\n",
    "for i, column in enumerate(sub.columns):\n",
    "    column_number[column] = i\n",
    "    \n",
    "def to_number(x, dic):\n",
    "    return dic[x]\n",
    "\n",
    "tr['type_num'] = tr['type'].apply(lambda x: to_number(x, column_number))\n",
    "\n",
    "target = tr['type_num']\n",
    "t = target.copy()\n",
    "\n",
    "train_X = tr.drop(['id', 'type', 'type_num'], axis=1)\n",
    "test_X = te.drop(['id',], axis=1)\n",
    "\n",
    "train_X['t'] = np.ones(len(train_X))\n",
    "test_X['t'] = np.zeros(len(test_X))\n",
    "m = pd.concat([train_X, test_X])\n",
    "\n",
    "ctd = []\n",
    "for c in m.columns[1:-1]:\n",
    "    mini = np.min(te[c])\n",
    "    maxi = np.max(te[c])\n",
    "    ctd += (list(m[c][m[c].map(lambda x: x if mini < x < maxi else 'c') == 'c'].index.values))\n",
    "\n",
    "for c in category:\n",
    "    m[c] = np.zeros(len(m))\n",
    "    for cl in m.columns:\n",
    "        if c in cl:\n",
    "            m[c] += m[cl]\n",
    "            \n",
    "            \n",
    "new_col = ['fiberID', 'psfMag_u', 'psfMag_g', 'psfMag_r', 'psfMag_i', 'psfMag_z',\n",
    "       'fiberMag_u', 'fiberMag_g', 'fiberMag_r', 'fiberMag_i', 'fiberMag_z',\n",
    "       'petroMag_u', 'petroMag_g', 'petroMag_r', 'petroMag_i', 'petroMag_z',\n",
    "       'modelMag_u', 'modelMag_g', 'modelMag_r', 'modelMag_i', 'modelMag_z',\n",
    "       'psfMag', 'fiberMag', 'petroMag', 'modelMag','_u', '_g', '_r', '_i', '_z', 't']\n",
    "\n",
    "m = m[new_col]\n",
    "tr = m[m['t'] == 1].drop(['t'], axis=1)\n",
    "te = m[m['t'] == 0].drop(['t'], axis=1)\n",
    "\n",
    "ctd = np.array(list(set(ctd)))\n",
    "tr = tr.drop(ctd)\n",
    "tr = tr.reset_index().drop('index', axis=1)\n",
    "\n",
    "tr2 = tr.copy()\n",
    "te2 = te.copy()\n",
    "\n",
    "for c in tr.columns[1:]:\n",
    "    tr[c] = tr[c].map(exp)\n",
    "    te[c] = te[c].map(exp)\n",
    "    trf = RobustScaler().fit(tr[c].values.reshape(-1, 1))\n",
    "    tr[c] = trf.transform(tr[c].values.reshape(-1, 1))\n",
    "    te[c] = trf.transform(te[c].values.reshape(-1, 1))\n",
    "\n",
    "#     tr[c] = (tr[c] - np.mean(tr[c]))/np.std(tr[c])\n",
    "#     te[c] = (te[c] - np.mean(tr[c]))/np.std(tr[c])\n",
    "    tr2[c] = (tr2[c] - np.min(tr2[c]))/(np.max(tr2[c]) - np.min(tr2[c])) # for nmf\n",
    "    te2[c] = (te2[c] - np.min(tr2[c]))/(np.max(tr2[c]) - np.min(tr2[c]))\n",
    "\n",
    "m = pd.concat([tr, te])\n",
    "fiber = pd.get_dummies(m['fiberID'], prefix='fiber')\n",
    "tr_fiber = fiber.iloc[0:len(tr), :]\n",
    "te_fiber = fiber.iloc[len(tr): , :]\n",
    "\n",
    "# train_X = train_X.drop('fiberID', axis=1)\n",
    "# test_X = test_X.drop('fiberID', axis=1)\n",
    "\n",
    "t = t.drop(ctd)\n",
    "t = t.reset_index().drop('index', axis=1)\n",
    "\n",
    "target = t.copy()\n",
    "target = target.values.flatten()\n",
    "target_wide = to_categorical(target)\n",
    "\n",
    "tr_X = tr.copy()\n",
    "te_X = te.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr = tr.drop('fiberID', axis=1)\n",
    "tte = te.drop('fiberID', axis=1)\n",
    "d_dim = ttr.shape\n",
    "dec = DEC(d_dim, 19, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dec.train(ttr.values, t.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_acc(t.values.flatten(), dec.predict(ttr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = GaussianMixture(19).fit_predict(ttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tr = dec.predict(ttr.values)\n",
    "dec_te = dec.predict(tte.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ami:',round(metrics.adjusted_mutual_info_score(t.values.flatten(), km), 3))\n",
    "print('ari:', round(metrics.adjusted_rand_score(t.values.flatten(), km), 3))\n",
    "print('cluster acc:', round(cl_acc(t.values.flatten(), km), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ami:',round(metrics.adjusted_mutual_info_score(t.values.flatten(), dec_tr), 3))\n",
    "print('ari:', round(metrics.adjusted_rand_score(t.values.flatten(), dec_tr), 3))\n",
    "print('cluster acc:', round(cl_acc(t.values.flatten(), dec_tr), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr = tr.drop('fiberID', axis=1)\n",
    "tte = te.drop('fiberID', axis=1)\n",
    "\n",
    "pca = PCA(15, random_state=42).fit(ttr)\n",
    "pca_tr = pca.transform(ttr)\n",
    "pca_te = pca.transform(tte)\n",
    "print('complete pca')\n",
    "\n",
    "# nmf = NMF(15, random_state=42).fit(tr2)\n",
    "# nmf_tr = nmf.transform(tr2)\n",
    "# nmf_te = nmf.transform(te2)\n",
    "# print('complete nmf')\n",
    "\n",
    "tr = np.concatenate([tr.values, pca_tr], axis=1)\n",
    "te = np.concatenate([te.values, pca_te], axis=1)\n",
    "\n",
    "km = KMeans(19, random_state=42).fit(ttr)\n",
    "km_tr1 = km.predict(ttr)\n",
    "km_tr2 = to_categorical(km_tr1)\n",
    "km_te1 = km.predict(tte)\n",
    "km_te2 = to_categorical(km_te1)\n",
    "print('complete kmeans')\n",
    "\n",
    "gm = GaussianMixture(19, random_state=42).fit(ttr)\n",
    "gm_tr1 = gm.predict(ttr)\n",
    "gm_tr2 = to_categorical(gm_tr1)\n",
    "gm_te1 = gm.predict(tte)\n",
    "gm_te2 = to_categorical(gm_te1)\n",
    "print('complete ggm')\n",
    "\n",
    "tr_X = np.concatenate([tr, km_tr1.reshape(-1, 1), gm_tr1.reshape(-1, 1), dec_tr.reshape(-1, 1)], axis=1)\n",
    "te_X = np.concatenate([te, km_te1.reshape(-1, 1), gm_te1.reshape(-1, 1), dec_te.reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "svc = SVC(random_state=42, probability=True)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "rf = RandomForestClassifier(max_depth=9,\n",
    "                           random_state=42)\n",
    "models = [knn]\n",
    "for m in models:\n",
    "    s = time()\n",
    "    print(np.mean(cross_val_score(m, tr_X, t,  scoring='neg_log_loss', cv = 4 )))\n",
    "    print(time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(tr_X, t, test_size=0.3, random_state=12, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGB_bayesian(\n",
    "    #learning_rate,\n",
    "    num_leaves, \n",
    "    bagging_fraction,\n",
    "    feature_fraction,\n",
    "    min_child_weight, \n",
    "    min_data_in_leaf,\n",
    "    max_depth,\n",
    "    reg_alpha,\n",
    "    reg_lambda\n",
    "     ):\n",
    "    # LightGBM expects next three parameters need to be integer. \n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "    \n",
    "\n",
    "    params = {\n",
    "              'num_leaves': num_leaves, \n",
    "              'min_data_in_leaf': min_data_in_leaf,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "#               'learning_rate' : 0.03,\n",
    "              'max_depth': max_depth,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'objective': 'softmax',\n",
    "              'save_binary': True,\n",
    "              'seed': 12,\n",
    "              'feature_fraction_seed': 12,\n",
    "              'bagging_seed': 12,\n",
    "              'drop_seed': 12,\n",
    "              'data_random_seed': 12,\n",
    "              'boosting': 'gbdt', ## some get better result using 'dart'\n",
    "              'verbose': 1,\n",
    "              'is_unbalance': True,\n",
    "              'boost_from_average': True,\n",
    "              'metric':'multi_logloss'}    \n",
    "    \n",
    "    ## set clf options\n",
    "    clf = lgb.LGBMClassifier(**params).fit(train_X, train_y, early_stopping_rounds=50,eval_set=[(test_X, test_y)], eval_metric='multi_logloss', verbose=0)\n",
    "    \n",
    "    score = -log_loss(test_y, clf.predict_proba(test_X))\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_LGB = {\n",
    "    'num_leaves': (300, 1000), \n",
    "    'min_data_in_leaf': (0, 150),\n",
    "    'bagging_fraction' : (0.3, 0.9),\n",
    "    'feature_fraction' : (0.3, 0.9),\n",
    "#     'learning_rate': (0.01, 0.3),\n",
    "    'min_child_weight': (0.01, 3),   \n",
    "    'reg_alpha': (0.1, 3), \n",
    "    'reg_lambda': (0.1, 3),\n",
    "    'max_depth':(6, 25),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_points = 10\n",
    "n_iter = 20\n",
    "\n",
    "optimizer.maximize(init_points=init_points, n_iter=n_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lgb = {\n",
    "        'min_data_in_leaf': int(optimizer.max['params']['min_data_in_leaf']), \n",
    "        'num_leaves': int(optimizer.max['params']['num_leaves']), \n",
    "        #'learning_rate': LGB_BO.max['params']['learning_rate'],\n",
    "        'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "        'bagging_fraction': optimizer.max['params']['bagging_fraction'], \n",
    "        'feature_fraction': optimizer.max['params']['feature_fraction'],\n",
    "        'reg_lambda': optimizer.max['params']['reg_lambda'],\n",
    "        'reg_alpha': optimizer.max['params']['reg_alpha'],\n",
    "        'max_depth': int(optimizer.max['params']['max_depth']), \n",
    "        'objective': 'softmax',\n",
    "        'save_binary': True,\n",
    "        'seed': 12,\n",
    "        'feature_fraction_seed': 12,\n",
    "        'bagging_seed': 12,\n",
    "        'drop_seed': 12,\n",
    "        'data_random_seed': 12,\n",
    "        'boosting_type': 'gbdt',  # also consider 'dart'\n",
    "        'verbose': 1,\n",
    "        'is_unbalance': False,\n",
    "        'boost_from_average': True,\n",
    "        'metric':'multi_logloss'\n",
    "    }\n",
    "\n",
    "params = param_lgb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('best_params_robust.bin', 'wb')\n",
    "pickle.dump(params, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('best_params.bin', 'rb')\n",
    "params = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb_clf = lgb.LGBMClassifier(**params, early_stoppong_rounds = 50)\n",
    "lgb_clf.fit(tr_X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('score is',np.mean(cross_val_score(lgb_clf, tr_X, t,  scoring='neg_log_loss', cv = 4 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = lgb_clf.predict_proba(te_X)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/robust_new_lgb4.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'############################'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parmas for xgboost\n",
    "params_fx = {'min_data_in_leaf': params['min_data_in_leaf'],\n",
    "             'num_leaves': params['num_leaves'],\n",
    "             'min_child_weight': params['min_child_weight'],\n",
    "             'bagging_fraction': params['bagging_fraction'],\n",
    "             'feature_fraction': params['feature_fraction'],\n",
    "             'reg_lambda': params['reg_lambda'],\n",
    "             'reg_alpha': params['reg_alpha'],\n",
    "             'max_depth': params['max_depth'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "            **params_fx,\n",
    "#             n_estimators=500,\n",
    "            tree_method = 'hist',\n",
    "            booster = 'gbtree',\n",
    "            eval_metric = 'mlogloss',\n",
    "            objective = 'multi:softprob',\n",
    "            num_class = 19,\n",
    "            early_stoppong_rounds = 50\n",
    "            \n",
    "    ).fit(tr_X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_clf.predict_proba(te_X)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/xgb7.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cat_clf = cat.CatBoostClassifier(early_stopping_rounds=50, random_state=42, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cat_clf.predict_proba(te_X)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/cat2.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_clf = lgb.LGBMClassifier(**params, early_stoppong_rounds = 50)\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "            **params_fx,\n",
    "#             n_estimators=500,\n",
    "            tree_method = 'hist',\n",
    "            booster = 'gbtree',\n",
    "            eval_metric = 'mlogloss',\n",
    "            objective = 'multi:softprob',\n",
    "            num_class = 19,\n",
    "            early_stoppong_rounds = 50\n",
    "            \n",
    "    )\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=200,\n",
    "                                max_depth=13, \n",
    "                                max_features='sqrt', \n",
    "                                random_state=42)\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators=150,\n",
    "                                max_depth=9, \n",
    "                                max_features='sqrt', \n",
    "                                random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=9, \n",
    "                             max_features='sqrt', \n",
    "                             random_state=42)\n",
    "\n",
    "pca = PCA(15)\n",
    "\n",
    "estimators = [('lgb', lgb_clf), ('xgb', xgb_clf), ('rf', rf)]\n",
    "vclf = VotingClassifier(estimators,\n",
    "                       voting='soft',\n",
    "                       weights = [0.6, 0.3, 0.1],\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vclf.fit(tr_X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = vclf.predict_proba(te_X)\n",
    "# print(y_pred)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/vclf4.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(t, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[lgb_clf, xgb_clf], \n",
    "          [rf2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackNetClassifier(models, \n",
    "                           metric=\"logloss\", \n",
    "                           folds=4,\n",
    "                           restacking=False,\n",
    "                           use_retraining=True,\n",
    "                           use_proba=True, # To use predict_proba after training\n",
    "                           random_state=42,\n",
    "                           n_jobs=-1, \n",
    "                           verbose=1)\n",
    "\n",
    "model.fit(tr_X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(te_X)\n",
    "# print(y_pred)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "print(log_loss(t, y_pred))\n",
    "submission.to_csv('./sub/pre/stk15.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = [lgb_clf, xgb_clf, cat_clf, model]\n",
    "for m in md:\n",
    "    s = time()\n",
    "    print(np.mean(cross_val_score(m, tr_X, t,  scoring='neg_log_loss', cv = 4 )))\n",
    "    print(time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking using boosting and then NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = model.predict_up_to(tr_X)\n",
    "k2 = model.predict_up_to(te_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr_X = k1[0]\n",
    "new_te_X = k2[0]\n",
    "print(new_tr_X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('new_te_X.bin', 'wb')\n",
    "pickle.dump(new_te_X, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape = (new_tr_X.shape[1],))\n",
    "\n",
    "x = Dense(128)(inputs1)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Dense(64)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Dense(64)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "\n",
    "outputs1 = Dense(new_tr_X.shape[1])(x)\n",
    "outputs2 = Dense(19, activation='softmax')(x)\n",
    "\n",
    "q = Model(inputs1, [outputs1, outputs2])\n",
    "\n",
    "q.compile(optimizer = 'adam', loss=['mse', 'categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q.fit(new_tr_X, [new_tr_X,target_wide], batch_size= 1024*16, epochs=200, shuffle=True, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.predict(new_te_X)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.log_loss(target_wide, q.predict(new_tr_X)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = q.predict(new_te_X)[1]\n",
    "# print(y_pred)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/stk_nn.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossentropy_AE\n",
    "ypred = KMeans(random_state=42, n_clusters=19).fit_predict(q.predict(new_tr_X)[1])\n",
    "print(  metrics.normalized_mutual_info_score(target, ypred),\n",
    "        metrics.adjusted_mutual_info_score(target, ypred),\n",
    "        metrics.adjusted_rand_score(target, ypred),\n",
    "        acc(target, ypred)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEC\n",
    "ypred = q.predict(new_te_X)[1]\n",
    "tsne = TSNE(random_state=42, perplexity=100).fit_transform(ypred)\n",
    "labels = KMeans(random_state=42, n_clusters=19).fit_predict(ypred)\n",
    "xs = tsne[:,0]\n",
    "ys = tsne[:,1]\n",
    "plt.scatter(xs,ys,c=labels)\n",
    "plt.show()\n",
    "sns.scatterplot(xs,ys,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_AE():\n",
    "    K.clear_session()\n",
    "    inputs1 = layers.Input(shape=(tr_X.shape[1], ))\n",
    "    inputs2 = layers.Input(shape=(target_wide.shape[1], ))\n",
    "#     x1 = layers.Dense(64)(inputs1)\n",
    "    x2 = layers.Dense(64)(inputs2)\n",
    "    \n",
    "    x = layers.Concatenate()([inputs1, x2])\n",
    "    \n",
    "    x = layers.Dense(32)(x)\n",
    "    x = layers.advanced_activations.LeakyReLU(0.3)(x)\n",
    "\n",
    "    cl = layers.Dense(19)(x)\n",
    "\n",
    "    x = layers.Dense(32)(cl)\n",
    "    x = layers.advanced_activations.LeakyReLU(0.3)(x)\n",
    "\n",
    "    x = layers.Dense(64)(x)\n",
    "    x = layers.advanced_activations.LeakyReLU(0.3)(x)\n",
    "\n",
    "    outputs1 = layers.Dense(tr_X.shape[1])(x)\n",
    "    outputs2 = layers.Dense(target_wide.shape[1], activation='softmax')(x)\n",
    "\n",
    "    m = models.Model([inputs1, inputs2], [outputs1, outputs2])\n",
    "    cl = models.Model([inputs1, inputs2], cl)\n",
    "    return m, cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, cl = make_AE()\n",
    "\n",
    "m.compile(loss=['mse', 'categorical_crossentropy'], optimizer=optimizers.Adam(2e-4,0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = m.fit([tr_X.values, target_wide], [tr_X.values, target_wide],\n",
    "     epochs=100,\n",
    "     batch_size=1024*128\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossentropy_AE\n",
    "km = KMeans(random_state=42, n_clusters=19).fit(cl.predict([tr_X.values, target_wide]))\n",
    "ypred = km.predict(cl.predict([tr_X.values, target_wide]))\n",
    "print(  metrics.normalized_mutual_info_score(target, ypred),\n",
    "        metrics.adjusted_mutual_info_score(target, ypred),\n",
    "        metrics.adjusted_rand_score(target, ypred),\n",
    "        acc(target, ypred)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred_wide = to_categorical(ypred)\n",
    "\n",
    "te_pred = km.predict()\n",
    "tr_pred_wide = to_categorical(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_shape = [tr_X.shape, tr_pred_wide.shape, tr_fiber.shape]\n",
    "\n",
    "inputs1 = layers.Input(shape = (inputs_shape[0][1], ))\n",
    "inputs2 = layers.Input(shape = (inputs_shape[1][1], ))\n",
    "inputs3 = layers.Input(shape = (inputs_shape[2][1], ))\n",
    "\n",
    "######### 여기부터는 test 해봐야 함\n",
    "x2 = layers.Dense(64)(inputs2)\n",
    "x3 = layers.Dense(64)(inputs3)\n",
    "\n",
    "x = layers.Concatenate()([inputs1, x2, x3])\n",
    "\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.Dense(32)(x)\n",
    "\n",
    "outputs = layers.Dense(19, activation='softmax')(x)\n",
    "\n",
    "nn = models.Model([inputs1, inputs2], outputs)\n",
    "\n",
    "nn.compile(optimizer = 'adam',\n",
    "          loss = 'sparse_categorical_crossentropy',\n",
    "          metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "inputs_shape = [train_X.shape, tr_fiber.shape]\n",
    "\n",
    "inputs1 = layers.Input(shape = (inputs_shape[0][1], ))\n",
    "inputs2 = layers.Input(shape = (inputs_shape[1][1], ))\n",
    "\n",
    "x2 = layers.Dense(64)(inputs2)\n",
    "\n",
    "x = layers.Concatenate()([inputs1, x2])\n",
    "\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.Dense(32)(x)\n",
    "\n",
    "outputs = layers.Dense(19, activation='softmax')(x)\n",
    "\n",
    "nn = models.Model([inputs1, inputs2], outputs)\n",
    "\n",
    "nn.compile(optimizer = 'adam',\n",
    "          loss = 'sparse_categorical_crossentropy',\n",
    "          metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.fit([train_X.values, tr_fiber.values], t.values,\n",
    "      epochs=500,\n",
    "      validation_split=0.15,\n",
    "      callbacks=[es],\n",
    "      batch_size=1024*16\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict([test_X, te_fiber])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict([test_X, te_fiber])\n",
    "# print(y_pred)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/adv_nn1.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_imp = lgb_clf.feature_importances_\n",
    "\n",
    "lgb_imp_idx = []\n",
    "# cols = tr_X.columns\n",
    "for i, imp in enumerate(lgb_imp):\n",
    "    if imp > 0:\n",
    "        lgb_imp_idx.append(i)\n",
    "    else:\n",
    "        print(i)\n",
    "        \n",
    "# new_cols_imp = cols[lgb_imp_idx]\n",
    "# new_cols_imp = list(new_cols_imp)\n",
    "# print(lgb_imp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tr_X.columns), len(new_cols_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./new_cols.bin', 'wb')\n",
    "pickle.dump(new_cols_imp, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
