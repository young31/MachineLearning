{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# for convenient\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import Lambda, Dense, Input, Concatenate, BatchNormalization, InputSpec\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom layer for clustering module\n",
    "class ClusteringLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0,**kwargs):\n",
    "        \n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "#         self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        \n",
    "#         self.built = True\n",
    "        \n",
    "        super(ClusteringLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEC:\n",
    "    def __init__(self, d_dim, latent_dim, n_classes):\n",
    "        self.pretrained = False\n",
    "        # data structure\n",
    "        self.d_dim = d_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_classes = n_classes # cluster num,\n",
    "        \n",
    "        # build model\n",
    "        self.enc = self.build_encoder()\n",
    "        self.dec = self.build_decoder()\n",
    "        self.clustering = ClusteringLayer(self.n_classes, name='clustering')\n",
    "        \n",
    "        # AE\n",
    "#         z = Input(shape = (self.d_dim[1], ))\n",
    "        self.ae = Model(self.enc.input, self.dec(self.enc.output), name='AE')\n",
    "        self.ae.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        # cluster\n",
    "        self.cl = Model(self.enc.input, self.clustering(self.enc.output), name='Cluster') # clustering moduel based on t-dist distance\n",
    "        self.cl.compile(optimizer='sgd', loss='kld') # use kld loss !!\n",
    "        \n",
    "        \n",
    "    def build_encoder(self):\n",
    "        inputs = Input(shape= (self.d_dim[1], ), name='input')\n",
    "        \n",
    "        x = Denseblock(1024)(inputs)\n",
    "        x = Denseblock(512)(x)\n",
    "        x = Denseblock(256)(x)\n",
    "        \n",
    "        outputs = Dense(self.latent_dim, name='output')(x)\n",
    "        \n",
    "        enc = Model(inputs, outputs, name='encoder')\n",
    "        \n",
    "        return enc\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        inputs = Input(shape = (self.latent_dim, ))\n",
    "        \n",
    "        x = Denseblock(256)(inputs)\n",
    "        x = Denseblock(512)(x)\n",
    "        x = Denseblock(1024)(x)\n",
    "        \n",
    "        outputs = Dense(self.d_dim[1])(x)\n",
    "        \n",
    "        dec = Model(inputs, outputs, name='decoder')\n",
    "        \n",
    "        return dec\n",
    "    \n",
    "    def target_distribution(self, q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "    \n",
    "    def pretrain(self, x, epochs=500, batch_size=1024*16):\n",
    "        print('pretraining ...')\n",
    "        es = EarlyStopping(patience=20, restore_best_weights=True, monitor='loss')\n",
    "        self.ae.fit(x, x, epochs=epochs, batch_size=batch_size, callbacks=[es], verbose=0)\n",
    "        self.pretrained = True\n",
    "        \n",
    "    def train(self, x, y=None, epochs=1e5, batch_size=1024*16, interval=256,tol=1e-3):\n",
    "        if self.pretrained == False:\n",
    "            self.pretrain(x)\n",
    "            print('finish pretrain')\n",
    "        # set inital point\n",
    "        kmeans = KMeans(n_clusters=self.n_classes, n_init=20)\n",
    "        y_pred = kmeans.fit_predict(self.enc.predict(x))\n",
    "        y_pred_last = np.copy(y_pred) # to compare performance\n",
    "        self.cl.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "        \n",
    "        # set aux dist\n",
    "        q = self.cl.predict(x, verbose=0)\n",
    "        p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "        for e in range(int(epochs)+1):\n",
    "            # clusetring\n",
    "            idx = np.random.randint(0, len(x), batch_size)\n",
    "            tr = x[idx]\n",
    "            loss = self.cl.train_on_batch(tr, p[idx])\n",
    "#             print('cl_loss:', loss)\n",
    "            \n",
    "            # update distribution\n",
    "            if not e%interval:\n",
    "                q = self.cl.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)\n",
    "                y_pred = self.predict(tr)\n",
    "                if y is not None:\n",
    "                    acc, nmi, ari = self.evaluate(y[idx], y_pred)\n",
    "                    print('cl_acc:', acc, 'nmi:', nmi, 'ari:', ari)\n",
    "            \n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if delta_label < tol and e != 0:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('finish train')\n",
    "                    break\n",
    "                    \n",
    "    def evaluate(self, y, y_pred):\n",
    "        acc = np.round(cl_acc(y, y_pred), 5)\n",
    "        nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)\n",
    "        ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)\n",
    "        \n",
    "        return acc, nmi, ari\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.array(list(map(lambda p: np.argmax(p), self.cl.predict(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Denseblock(n):\n",
    "    def f(x):\n",
    "        x = Dense(n)(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.3)(x)\n",
    "        \n",
    "        return x\n",
    "    return f\n",
    "\n",
    "def cl_acc(ypred, y):\n",
    "    s = np.unique(ypred)\n",
    "    t = np.unique(y)\n",
    "    \n",
    "    N = len(np.unique(y))\n",
    "    C = np.zeros((N, N), dtype = np.int32)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            idx = np.logical_and(ypred == s[i], y == t[j])\n",
    "            C[i][j] = np.count_nonzero(idx)\n",
    "    \n",
    "    # convert the C matrix to the 'true' cost\n",
    "    Cmax = np.amax(C)\n",
    "    C = Cmax - C\n",
    "    # \n",
    "    indices = np.array(list(map(lambda x: list(x), list(zip(*linear_sum_assignment(C))))))\n",
    "    row = indices[:][:, 0]\n",
    "    col = indices[:][:, 1]\n",
    "    # calculating the accuracy according to the optimal assignment\n",
    "    count = 0\n",
    "    for i in range(N):\n",
    "        idx = np.logical_and(ypred == s[row[i]], y == t[col[i]] )\n",
    "        count += np.count_nonzero(idx)\n",
    "    \n",
    "    return 1.0*count/len(y)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['psfMag', 'fiberMag', 'petroMag', 'modelMag', '_u', '_g', '_r', '_i', '_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def exp(a):\n",
    "    if a < 0:\n",
    "        return -np.log(-a)\n",
    "    else:\n",
    "        return np.log(a)\n",
    "    \n",
    "def exp(a):\n",
    "    if a < 0:\n",
    "        return -(a ** 2)\n",
    "    else:\n",
    "        return (a ** 2)\n",
    "    \n",
    "tr = pd.read_csv('./data/train.csv')\n",
    "te = pd.read_csv('./data/test.csv')\n",
    "\n",
    "sub = pd.read_csv('./data/sample_submission.csv', index_col=0)\n",
    "\n",
    "column_number = {}\n",
    "for i, column in enumerate(sub.columns):\n",
    "    column_number[column] = i\n",
    "    \n",
    "def to_number(x, dic):\n",
    "    return dic[x]\n",
    "\n",
    "tr['type_num'] = tr['type'].apply(lambda x: to_number(x, column_number))\n",
    "\n",
    "target = tr['type_num']\n",
    "t = target.copy()\n",
    "\n",
    "train_X = tr.drop(['id', 'type', 'type_num'], axis=1)\n",
    "test_X = te.drop(['id',], axis=1)\n",
    "\n",
    "train_X['t'] = np.ones(len(train_X))\n",
    "test_X['t'] = np.zeros(len(test_X))\n",
    "m = pd.concat([train_X, test_X])\n",
    "\n",
    "ctd = []\n",
    "for c in m.columns[1:-1]:\n",
    "    mini = np.min(te[c])\n",
    "    maxi = np.max(te[c])\n",
    "    ctd += (list(m[c][m[c].map(lambda x: x if mini < x < maxi else 'c') == 'c'].index.values))\n",
    "\n",
    "for c in category:\n",
    "    m[c] = np.zeros(len(m))\n",
    "    for cl in m.columns:\n",
    "        if c in cl:\n",
    "            m[c] += m[cl]\n",
    "            \n",
    "            \n",
    "new_col = ['fiberID', 'psfMag_u', 'psfMag_g', 'psfMag_r', 'psfMag_i', 'psfMag_z',\n",
    "       'fiberMag_u', 'fiberMag_g', 'fiberMag_r', 'fiberMag_i', 'fiberMag_z',\n",
    "       'petroMag_u', 'petroMag_g', 'petroMag_r', 'petroMag_i', 'petroMag_z',\n",
    "       'modelMag_u', 'modelMag_g', 'modelMag_r', 'modelMag_i', 'modelMag_z',\n",
    "       'psfMag', 'fiberMag', 'petroMag', 'modelMag','_u', '_g', '_r', '_i', '_z', 't']\n",
    "\n",
    "m = m[new_col]\n",
    "tr = m[m['t'] == 1].drop(['t'], axis=1)\n",
    "te = m[m['t'] == 0].drop(['t'], axis=1)\n",
    "\n",
    "ctd = np.array(list(set(ctd)))\n",
    "tr = tr.drop(ctd)\n",
    "tr = tr.reset_index().drop('index', axis=1)\n",
    "\n",
    "tr2 = tr.copy()\n",
    "te2 = te.copy()\n",
    "\n",
    "for c in tr.columns[1:]:\n",
    "    tr[c] = tr[c].map(exp)\n",
    "    te[c] = te[c].map(exp)\n",
    "    trf = RobustScaler().fit(tr[c].values.reshape(-1, 1))\n",
    "    tr[c] = trf.transform(tr[c].values.reshape(-1, 1))\n",
    "    te[c] = trf.transform(te[c].values.reshape(-1, 1))\n",
    "\n",
    "#     tr[c] = (tr[c] - np.mean(tr[c]))/np.std(tr[c])\n",
    "#     te[c] = (te[c] - np.mean(tr[c]))/np.std(tr[c])\n",
    "    tr2[c] = (tr2[c] - np.min(tr2[c]))/(np.max(tr2[c]) - np.min(tr2[c])) # for nmf\n",
    "    te2[c] = (te2[c] - np.min(tr2[c]))/(np.max(tr2[c]) - np.min(tr2[c]))\n",
    "\n",
    "m = pd.concat([tr, te])\n",
    "fiber = pd.get_dummies(m['fiberID'], prefix='fiber')\n",
    "tr_fiber = fiber.iloc[0:len(tr), :]\n",
    "te_fiber = fiber.iloc[len(tr): , :]\n",
    "\n",
    "# train_X = train_X.drop('fiberID', axis=1)\n",
    "# test_X = test_X.drop('fiberID', axis=1)\n",
    "\n",
    "t = t.drop(ctd)\n",
    "t = t.reset_index().drop('index', axis=1)\n",
    "\n",
    "target = t.copy()\n",
    "target = target.values.flatten()\n",
    "target_wide = to_categorical(target)\n",
    "\n",
    "tr_X = tr.copy()\n",
    "te_X = te.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr = tr.drop('fiberID', axis=1)\n",
    "tte = te.drop('fiberID', axis=1)\n",
    "d_dim = ttr.shape\n",
    "dec = DEC(d_dim, 19, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretraining ...\n",
      "finish pretrain\n",
      "cl_acc: 0.39752 nmi: 0.53312 ari: 0.33028\n",
      "cl_acc: 0.39532 nmi: 0.52779 ari: 0.32438\n",
      "cl_acc: 0.4046 nmi: 0.5286 ari: 0.33013\n",
      "cl_acc: 0.41058 nmi: 0.5266 ari: 0.33106\n",
      "cl_acc: 0.39923 nmi: 0.51562 ari: 0.32046\n",
      "cl_acc: 0.40771 nmi: 0.52054 ari: 0.32861\n",
      "cl_acc: 0.41217 nmi: 0.52847 ari: 0.3424\n",
      "cl_acc: 0.41956 nmi: 0.52126 ari: 0.33221\n",
      "cl_acc: 0.42102 nmi: 0.52255 ari: 0.33371\n",
      "cl_acc: 0.41724 nmi: 0.52667 ari: 0.3375\n",
      "cl_acc: 0.41815 nmi: 0.52388 ari: 0.33257\n",
      "cl_acc: 0.42023 nmi: 0.52268 ari: 0.33449\n",
      "cl_acc: 0.4162 nmi: 0.51928 ari: 0.3315\n",
      "cl_acc: 0.42151 nmi: 0.53073 ari: 0.34721\n",
      "cl_acc: 0.42456 nmi: 0.53067 ari: 0.34684\n",
      "cl_acc: 0.42944 nmi: 0.52792 ari: 0.34718\n",
      "cl_acc: 0.42657 nmi: 0.52678 ari: 0.3477\n",
      "cl_acc: 0.42639 nmi: 0.52704 ari: 0.34334\n",
      "cl_acc: 0.42151 nmi: 0.53156 ari: 0.34766\n",
      "cl_acc: 0.43341 nmi: 0.5332 ari: 0.35248\n",
      "cl_acc: 0.43542 nmi: 0.53084 ari: 0.35048\n",
      "cl_acc: 0.42743 nmi: 0.53497 ari: 0.35226\n",
      "cl_acc: 0.4317 nmi: 0.53399 ari: 0.35656\n",
      "cl_acc: 0.43701 nmi: 0.53904 ari: 0.35939\n",
      "cl_acc: 0.4436 nmi: 0.53698 ari: 0.36117\n",
      "cl_acc: 0.43903 nmi: 0.53778 ari: 0.35785\n",
      "cl_acc: 0.44067 nmi: 0.53894 ari: 0.36207\n",
      "cl_acc: 0.37561 nmi: 0.53594 ari: 0.35877\n",
      "cl_acc: 0.37775 nmi: 0.54416 ari: 0.36748\n",
      "cl_acc: 0.44891 nmi: 0.54743 ari: 0.37094\n",
      "cl_acc: 0.4408 nmi: 0.5447 ari: 0.36507\n",
      "cl_acc: 0.45459 nmi: 0.54188 ari: 0.36684\n",
      "cl_acc: 0.45721 nmi: 0.54689 ari: 0.37346\n",
      "cl_acc: 0.44904 nmi: 0.54514 ari: 0.36837\n",
      "cl_acc: 0.45538 nmi: 0.54626 ari: 0.37487\n",
      "cl_acc: 0.38855 nmi: 0.55266 ari: 0.38084\n",
      "cl_acc: 0.45667 nmi: 0.54779 ari: 0.37436\n",
      "cl_acc: 0.46222 nmi: 0.54852 ari: 0.37482\n",
      "cl_acc: 0.453 nmi: 0.54918 ari: 0.36985\n",
      "cl_acc: 0.4649 nmi: 0.55764 ari: 0.38358\n",
      "cl_acc: 0.46777 nmi: 0.55 ari: 0.37526\n",
      "cl_acc: 0.39386 nmi: 0.56047 ari: 0.38699\n",
      "cl_acc: 0.46667 nmi: 0.55506 ari: 0.38012\n",
      "cl_acc: 0.39606 nmi: 0.56008 ari: 0.39067\n",
      "cl_acc: 0.4021 nmi: 0.56345 ari: 0.39171\n",
      "cl_acc: 0.47107 nmi: 0.56264 ari: 0.39133\n",
      "cl_acc: 0.47339 nmi: 0.56243 ari: 0.39158\n",
      "cl_acc: 0.39697 nmi: 0.56048 ari: 0.38597\n",
      "cl_acc: 0.47058 nmi: 0.55939 ari: 0.38757\n",
      "cl_acc: 0.47162 nmi: 0.56231 ari: 0.39045\n",
      "cl_acc: 0.46796 nmi: 0.55788 ari: 0.38233\n",
      "cl_acc: 0.47614 nmi: 0.56471 ari: 0.38829\n",
      "cl_acc: 0.48029 nmi: 0.56026 ari: 0.38704\n",
      "cl_acc: 0.47369 nmi: 0.56534 ari: 0.38859\n",
      "cl_acc: 0.47546 nmi: 0.5668 ari: 0.3919\n",
      "cl_acc: 0.48163 nmi: 0.56763 ari: 0.39896\n",
      "cl_acc: 0.47504 nmi: 0.56393 ari: 0.38821\n",
      "cl_acc: 0.47833 nmi: 0.56639 ari: 0.39311\n",
      "cl_acc: 0.39984 nmi: 0.56393 ari: 0.39039\n",
      "cl_acc: 0.48248 nmi: 0.56646 ari: 0.3947\n",
      "cl_acc: 0.40631 nmi: 0.56503 ari: 0.39509\n",
      "cl_acc: 0.40527 nmi: 0.56549 ari: 0.39284\n",
      "cl_acc: 0.47589 nmi: 0.57234 ari: 0.39395\n",
      "cl_acc: 0.4118 nmi: 0.57278 ari: 0.39935\n",
      "cl_acc: 0.47443 nmi: 0.5641 ari: 0.38577\n",
      "cl_acc: 0.48853 nmi: 0.57281 ari: 0.40229\n",
      "cl_acc: 0.48511 nmi: 0.57459 ari: 0.40296\n",
      "cl_acc: 0.48596 nmi: 0.57086 ari: 0.39756\n",
      "cl_acc: 0.41608 nmi: 0.57376 ari: 0.39925\n",
      "cl_acc: 0.40912 nmi: 0.57362 ari: 0.40057\n",
      "cl_acc: 0.40869 nmi: 0.57324 ari: 0.40064\n",
      "cl_acc: 0.48743 nmi: 0.57111 ari: 0.39944\n",
      "cl_acc: 0.48309 nmi: 0.57242 ari: 0.39698\n",
      "cl_acc: 0.41602 nmi: 0.56896 ari: 0.40358\n",
      "cl_acc: 0.40479 nmi: 0.56791 ari: 0.39221\n",
      "cl_acc: 0.41858 nmi: 0.56985 ari: 0.40079\n",
      "cl_acc: 0.48383 nmi: 0.57355 ari: 0.39784\n",
      "cl_acc: 0.48962 nmi: 0.5726 ari: 0.39931\n",
      "cl_acc: 0.41602 nmi: 0.57272 ari: 0.39824\n",
      "cl_acc: 0.42456 nmi: 0.57588 ari: 0.41113\n",
      "cl_acc: 0.40765 nmi: 0.57218 ari: 0.39961\n",
      "cl_acc: 0.49176 nmi: 0.56873 ari: 0.39704\n",
      "cl_acc: 0.48932 nmi: 0.57644 ari: 0.40754\n",
      "cl_acc: 0.48737 nmi: 0.57329 ari: 0.39629\n",
      "cl_acc: 0.48639 nmi: 0.57206 ari: 0.39969\n",
      "cl_acc: 0.48798 nmi: 0.57366 ari: 0.39369\n",
      "cl_acc: 0.49017 nmi: 0.57284 ari: 0.39705\n",
      "cl_acc: 0.41382 nmi: 0.56971 ari: 0.39831\n",
      "cl_acc: 0.40533 nmi: 0.57129 ari: 0.39197\n",
      "cl_acc: 0.48682 nmi: 0.57352 ari: 0.39625\n",
      "cl_acc: 0.48578 nmi: 0.57667 ari: 0.40297\n",
      "cl_acc: 0.40344 nmi: 0.57133 ari: 0.39661\n",
      "cl_acc: 0.40723 nmi: 0.57058 ari: 0.39448\n",
      "cl_acc: 0.40399 nmi: 0.56931 ari: 0.39463\n",
      "cl_acc: 0.49036 nmi: 0.58381 ari: 0.411\n",
      "cl_acc: 0.48407 nmi: 0.57544 ari: 0.40436\n",
      "cl_acc: 0.48785 nmi: 0.57533 ari: 0.39773\n",
      "cl_acc: 0.49078 nmi: 0.57754 ari: 0.40882\n",
      "cl_acc: 0.48871 nmi: 0.57752 ari: 0.40679\n",
      "cl_acc: 0.48163 nmi: 0.5734 ari: 0.39885\n",
      "cl_acc: 0.48267 nmi: 0.57402 ari: 0.40007\n",
      "cl_acc: 0.49066 nmi: 0.57764 ari: 0.40575\n",
      "cl_acc: 0.48517 nmi: 0.5759 ari: 0.40354\n",
      "cl_acc: 0.48657 nmi: 0.57644 ari: 0.39941\n",
      "cl_acc: 0.48798 nmi: 0.5794 ari: 0.40389\n",
      "cl_acc: 0.48444 nmi: 0.57204 ari: 0.39637\n",
      "cl_acc: 0.40594 nmi: 0.57569 ari: 0.4042\n",
      "cl_acc: 0.47498 nmi: 0.57202 ari: 0.40466\n",
      "cl_acc: 0.41052 nmi: 0.57694 ari: 0.39816\n",
      "cl_acc: 0.48663 nmi: 0.57551 ari: 0.40221\n",
      "cl_acc: 0.4212 nmi: 0.58436 ari: 0.41363\n",
      "cl_acc: 0.48694 nmi: 0.57671 ari: 0.40006\n",
      "cl_acc: 0.48901 nmi: 0.57914 ari: 0.41108\n",
      "cl_acc: 0.49469 nmi: 0.58647 ari: 0.41562\n",
      "cl_acc: 0.41791 nmi: 0.57965 ari: 0.40189\n",
      "cl_acc: 0.48865 nmi: 0.58094 ari: 0.40097\n",
      "cl_acc: 0.42255 nmi: 0.58259 ari: 0.40753\n",
      "cl_acc: 0.42175 nmi: 0.58565 ari: 0.40771\n",
      "cl_acc: 0.41797 nmi: 0.58273 ari: 0.40071\n",
      "cl_acc: 0.41418 nmi: 0.58209 ari: 0.39861\n",
      "cl_acc: 0.42078 nmi: 0.58036 ari: 0.39607\n",
      "cl_acc: 0.42126 nmi: 0.57967 ari: 0.39541\n",
      "cl_acc: 0.47009 nmi: 0.57218 ari: 0.38192\n",
      "cl_acc: 0.47253 nmi: 0.57339 ari: 0.37999\n",
      "cl_acc: 0.47778 nmi: 0.57528 ari: 0.38087\n",
      "cl_acc: 0.42181 nmi: 0.57383 ari: 0.38406\n",
      "cl_acc: 0.42444 nmi: 0.57567 ari: 0.37816\n",
      "cl_acc: 0.46997 nmi: 0.57214 ari: 0.37443\n",
      "cl_acc: 0.44116 nmi: 0.57982 ari: 0.38613\n",
      "cl_acc: 0.47815 nmi: 0.57199 ari: 0.37528\n",
      "cl_acc: 0.46375 nmi: 0.57433 ari: 0.36953\n",
      "cl_acc: 0.47449 nmi: 0.58075 ari: 0.38428\n",
      "cl_acc: 0.42981 nmi: 0.57568 ari: 0.37841\n",
      "cl_acc: 0.43488 nmi: 0.58151 ari: 0.38148\n",
      "cl_acc: 0.48077 nmi: 0.57301 ari: 0.37027\n",
      "cl_acc: 0.43713 nmi: 0.57853 ari: 0.37843\n",
      "cl_acc: 0.48535 nmi: 0.57726 ari: 0.37404\n",
      "cl_acc: 0.43854 nmi: 0.57935 ari: 0.37472\n",
      "cl_acc: 0.48163 nmi: 0.57715 ari: 0.37393\n",
      "cl_acc: 0.48907 nmi: 0.58223 ari: 0.38163\n",
      "cl_acc: 0.48694 nmi: 0.57383 ari: 0.37545\n",
      "cl_acc: 0.43262 nmi: 0.58469 ari: 0.38082\n",
      "cl_acc: 0.43579 nmi: 0.58611 ari: 0.38524\n",
      "cl_acc: 0.495 nmi: 0.58392 ari: 0.38124\n",
      "cl_acc: 0.42694 nmi: 0.58197 ari: 0.37514\n",
      "cl_acc: 0.50055 nmi: 0.58933 ari: 0.38389\n",
      "cl_acc: 0.42468 nmi: 0.58079 ari: 0.37234\n",
      "cl_acc: 0.49048 nmi: 0.5855 ari: 0.37614\n",
      "cl_acc: 0.48401 nmi: 0.57857 ari: 0.36463\n",
      "cl_acc: 0.49023 nmi: 0.58506 ari: 0.37432\n",
      "cl_acc: 0.48108 nmi: 0.58472 ari: 0.37149\n",
      "cl_acc: 0.48328 nmi: 0.58702 ari: 0.37402\n",
      "cl_acc: 0.48407 nmi: 0.58448 ari: 0.37051\n",
      "cl_acc: 0.48328 nmi: 0.58111 ari: 0.36739\n",
      "cl_acc: 0.48364 nmi: 0.58571 ari: 0.37265\n",
      "cl_acc: 0.48468 nmi: 0.58407 ari: 0.36988\n",
      "cl_acc: 0.48376 nmi: 0.58867 ari: 0.36957\n",
      "cl_acc: 0.41748 nmi: 0.5837 ari: 0.36783\n",
      "cl_acc: 0.42108 nmi: 0.58474 ari: 0.37205\n",
      "cl_acc: 0.48114 nmi: 0.58451 ari: 0.36699\n",
      "cl_acc: 0.4787 nmi: 0.5836 ari: 0.36785\n",
      "cl_acc: 0.40955 nmi: 0.57781 ari: 0.35637\n",
      "cl_acc: 0.47662 nmi: 0.57278 ari: 0.35711\n",
      "cl_acc: 0.47845 nmi: 0.57795 ari: 0.358\n",
      "cl_acc: 0.47003 nmi: 0.57007 ari: 0.34639\n",
      "cl_acc: 0.4707 nmi: 0.57478 ari: 0.34944\n",
      "cl_acc: 0.47009 nmi: 0.57002 ari: 0.34031\n",
      "cl_acc: 0.46442 nmi: 0.5658 ari: 0.33424\n",
      "cl_acc: 0.50342 nmi: 0.56964 ari: 0.33683\n",
      "cl_acc: 0.4184 nmi: 0.56486 ari: 0.33347\n",
      "cl_acc: 0.41418 nmi: 0.55902 ari: 0.32343\n",
      "cl_acc: 0.45398 nmi: 0.5652 ari: 0.33856\n",
      "cl_acc: 0.453 nmi: 0.56352 ari: 0.32763\n",
      "cl_acc: 0.41852 nmi: 0.55943 ari: 0.32837\n",
      "cl_acc: 0.41632 nmi: 0.56273 ari: 0.32424\n",
      "cl_acc: 0.43518 nmi: 0.56522 ari: 0.32379\n",
      "cl_acc: 0.4173 nmi: 0.5613 ari: 0.32519\n",
      "cl_acc: 0.43555 nmi: 0.55518 ari: 0.32176\n",
      "cl_acc: 0.42798 nmi: 0.55898 ari: 0.32323\n",
      "cl_acc: 0.43121 nmi: 0.56199 ari: 0.32332\n",
      "cl_acc: 0.42181 nmi: 0.55528 ari: 0.31711\n",
      "cl_acc: 0.41852 nmi: 0.55458 ari: 0.32486\n",
      "cl_acc: 0.42523 nmi: 0.55619 ari: 0.31743\n",
      "cl_acc: 0.42572 nmi: 0.55378 ari: 0.32118\n",
      "cl_acc: 0.41595 nmi: 0.55441 ari: 0.31638\n",
      "cl_acc: 0.41541 nmi: 0.5534 ari: 0.31582\n",
      "cl_acc: 0.40485 nmi: 0.55714 ari: 0.31664\n",
      "cl_acc: 0.41229 nmi: 0.55803 ari: 0.32196\n",
      "cl_acc: 0.40881 nmi: 0.55427 ari: 0.31543\n",
      "cl_acc: 0.39459 nmi: 0.54312 ari: 0.30019\n",
      "cl_acc: 0.40912 nmi: 0.55436 ari: 0.31331\n",
      "cl_acc: 0.39948 nmi: 0.55244 ari: 0.31098\n",
      "cl_acc: 0.40417 nmi: 0.55134 ari: 0.30672\n",
      "cl_acc: 0.4021 nmi: 0.55234 ari: 0.30437\n",
      "cl_acc: 0.39819 nmi: 0.55003 ari: 0.30908\n",
      "cl_acc: 0.40247 nmi: 0.54757 ari: 0.31461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl_acc: 0.3855 nmi: 0.54773 ari: 0.30295\n",
      "cl_acc: 0.40356 nmi: 0.54421 ari: 0.30627\n",
      "cl_acc: 0.39404 nmi: 0.54849 ari: 0.306\n",
      "cl_acc: 0.38818 nmi: 0.54527 ari: 0.29975\n",
      "cl_acc: 0.4046 nmi: 0.54653 ari: 0.30744\n",
      "cl_acc: 0.39404 nmi: 0.5404 ari: 0.29465\n",
      "cl_acc: 0.40283 nmi: 0.54696 ari: 0.30417\n",
      "cl_acc: 0.40186 nmi: 0.54508 ari: 0.30237\n",
      "cl_acc: 0.39935 nmi: 0.54146 ari: 0.2994\n",
      "cl_acc: 0.40015 nmi: 0.54028 ari: 0.29792\n",
      "cl_acc: 0.40436 nmi: 0.54689 ari: 0.30136\n",
      "cl_acc: 0.39783 nmi: 0.54391 ari: 0.30443\n",
      "cl_acc: 0.40131 nmi: 0.54793 ari: 0.30819\n",
      "cl_acc: 0.40118 nmi: 0.54124 ari: 0.30208\n",
      "cl_acc: 0.40228 nmi: 0.53972 ari: 0.29766\n",
      "cl_acc: 0.3952 nmi: 0.54071 ari: 0.29674\n",
      "cl_acc: 0.39417 nmi: 0.53658 ari: 0.28898\n",
      "cl_acc: 0.3988 nmi: 0.54251 ari: 0.29703\n",
      "cl_acc: 0.39709 nmi: 0.53972 ari: 0.29708\n",
      "cl_acc: 0.39526 nmi: 0.53601 ari: 0.28978\n",
      "cl_acc: 0.39264 nmi: 0.54001 ari: 0.29708\n",
      "cl_acc: 0.39856 nmi: 0.54075 ari: 0.29606\n",
      "cl_acc: 0.39435 nmi: 0.54332 ari: 0.3035\n",
      "cl_acc: 0.40088 nmi: 0.54265 ari: 0.29606\n",
      "cl_acc: 0.40485 nmi: 0.54466 ari: 0.30542\n",
      "cl_acc: 0.3941 nmi: 0.54298 ari: 0.30238\n",
      "cl_acc: 0.40741 nmi: 0.54313 ari: 0.29979\n",
      "cl_acc: 0.39606 nmi: 0.5419 ari: 0.29883\n",
      "cl_acc: 0.3941 nmi: 0.53727 ari: 0.29072\n",
      "cl_acc: 0.38593 nmi: 0.54042 ari: 0.29868\n",
      "cl_acc: 0.38141 nmi: 0.54085 ari: 0.2909\n",
      "cl_acc: 0.40027 nmi: 0.54194 ari: 0.29686\n",
      "cl_acc: 0.39655 nmi: 0.53986 ari: 0.29536\n",
      "cl_acc: 0.39569 nmi: 0.53726 ari: 0.29019\n",
      "cl_acc: 0.40314 nmi: 0.537 ari: 0.28857\n",
      "cl_acc: 0.38715 nmi: 0.54302 ari: 0.3002\n",
      "cl_acc: 0.40491 nmi: 0.54255 ari: 0.28996\n",
      "cl_acc: 0.38062 nmi: 0.54907 ari: 0.30094\n",
      "cl_acc: 0.3786 nmi: 0.54714 ari: 0.30441\n",
      "cl_acc: 0.42474 nmi: 0.55143 ari: 0.31667\n",
      "cl_acc: 0.42834 nmi: 0.55279 ari: 0.32269\n",
      "cl_acc: 0.41425 nmi: 0.5447 ari: 0.30317\n",
      "cl_acc: 0.38336 nmi: 0.54954 ari: 0.3088\n",
      "cl_acc: 0.38757 nmi: 0.55338 ari: 0.31369\n",
      "cl_acc: 0.41406 nmi: 0.5461 ari: 0.3\n",
      "cl_acc: 0.42657 nmi: 0.54569 ari: 0.30381\n",
      "cl_acc: 0.4184 nmi: 0.54506 ari: 0.30323\n",
      "cl_acc: 0.37964 nmi: 0.54459 ari: 0.29558\n",
      "cl_acc: 0.37811 nmi: 0.54055 ari: 0.29545\n",
      "cl_acc: 0.41425 nmi: 0.54308 ari: 0.29775\n",
      "cl_acc: 0.41138 nmi: 0.54778 ari: 0.30129\n",
      "cl_acc: 0.40784 nmi: 0.54537 ari: 0.29942\n",
      "cl_acc: 0.37903 nmi: 0.54318 ari: 0.29492\n",
      "cl_acc: 0.4082 nmi: 0.54517 ari: 0.30054\n",
      "cl_acc: 0.40546 nmi: 0.54088 ari: 0.29405\n",
      "cl_acc: 0.40186 nmi: 0.5386 ari: 0.29301\n",
      "cl_acc: 0.37677 nmi: 0.53712 ari: 0.28885\n",
      "cl_acc: 0.39648 nmi: 0.53661 ari: 0.28894\n",
      "cl_acc: 0.3775 nmi: 0.54242 ari: 0.29229\n",
      "cl_acc: 0.38123 nmi: 0.54003 ari: 0.28854\n",
      "cl_acc: 0.40485 nmi: 0.54077 ari: 0.29265\n",
      "cl_acc: 0.3783 nmi: 0.54008 ari: 0.29037\n",
      "cl_acc: 0.39655 nmi: 0.54375 ari: 0.29558\n",
      "cl_acc: 0.40167 nmi: 0.54106 ari: 0.29364\n",
      "cl_acc: 0.37421 nmi: 0.53839 ari: 0.28735\n",
      "cl_acc: 0.40283 nmi: 0.5357 ari: 0.28849\n",
      "cl_acc: 0.39966 nmi: 0.5398 ari: 0.29025\n",
      "cl_acc: 0.37439 nmi: 0.5386 ari: 0.2957\n",
      "cl_acc: 0.38092 nmi: 0.53838 ari: 0.29928\n",
      "cl_acc: 0.3772 nmi: 0.53892 ari: 0.29201\n",
      "cl_acc: 0.37488 nmi: 0.54253 ari: 0.29099\n",
      "cl_acc: 0.39539 nmi: 0.53706 ari: 0.28976\n",
      "cl_acc: 0.37628 nmi: 0.54 ari: 0.2949\n",
      "cl_acc: 0.37268 nmi: 0.54198 ari: 0.29436\n",
      "cl_acc: 0.37335 nmi: 0.5412 ari: 0.2911\n",
      "cl_acc: 0.37347 nmi: 0.53565 ari: 0.28831\n",
      "cl_acc: 0.38184 nmi: 0.53991 ari: 0.29417\n",
      "cl_acc: 0.37567 nmi: 0.53319 ari: 0.29171\n",
      "cl_acc: 0.39404 nmi: 0.5396 ari: 0.29467\n",
      "cl_acc: 0.37421 nmi: 0.53415 ari: 0.28502\n",
      "cl_acc: 0.38 nmi: 0.536 ari: 0.29125\n",
      "cl_acc: 0.36053 nmi: 0.53601 ari: 0.28316\n",
      "cl_acc: 0.38898 nmi: 0.53697 ari: 0.29037\n",
      "cl_acc: 0.38812 nmi: 0.53427 ari: 0.28918\n",
      "cl_acc: 0.37781 nmi: 0.53596 ari: 0.29059\n",
      "cl_acc: 0.39209 nmi: 0.53514 ari: 0.28941\n",
      "cl_acc: 0.37561 nmi: 0.53847 ari: 0.29051\n",
      "cl_acc: 0.38745 nmi: 0.53542 ari: 0.29191\n",
      "cl_acc: 0.38995 nmi: 0.54056 ari: 0.29219\n",
      "cl_acc: 0.3858 nmi: 0.53419 ari: 0.28442\n",
      "cl_acc: 0.38715 nmi: 0.53647 ari: 0.28992\n",
      "cl_acc: 0.38849 nmi: 0.53336 ari: 0.28375\n",
      "cl_acc: 0.37183 nmi: 0.53377 ari: 0.2866\n",
      "cl_acc: 0.38892 nmi: 0.53261 ari: 0.2875\n",
      "cl_acc: 0.3811 nmi: 0.54054 ari: 0.2944\n",
      "cl_acc: 0.37128 nmi: 0.53185 ari: 0.28728\n",
      "cl_acc: 0.37695 nmi: 0.538 ari: 0.29174\n",
      "cl_acc: 0.39313 nmi: 0.54043 ari: 0.29358\n",
      "cl_acc: 0.39795 nmi: 0.53464 ari: 0.28824\n",
      "cl_acc: 0.38037 nmi: 0.5397 ari: 0.29613\n",
      "cl_acc: 0.37372 nmi: 0.53292 ari: 0.28457\n",
      "cl_acc: 0.41034 nmi: 0.53468 ari: 0.29377\n",
      "cl_acc: 0.41736 nmi: 0.54315 ari: 0.31163\n",
      "cl_acc: 0.41571 nmi: 0.54627 ari: 0.31352\n",
      "cl_acc: 0.38739 nmi: 0.55172 ari: 0.32371\n",
      "cl_acc: 0.39392 nmi: 0.55804 ari: 0.34031\n",
      "cl_acc: 0.40259 nmi: 0.55717 ari: 0.34804\n",
      "cl_acc: 0.40259 nmi: 0.54789 ari: 0.34287\n",
      "cl_acc: 0.4624 nmi: 0.5503 ari: 0.35632\n",
      "cl_acc: 0.41064 nmi: 0.55697 ari: 0.37112\n",
      "cl_acc: 0.47095 nmi: 0.5494 ari: 0.36687\n",
      "cl_acc: 0.41376 nmi: 0.55292 ari: 0.37228\n",
      "cl_acc: 0.48834 nmi: 0.55456 ari: 0.3829\n",
      "cl_acc: 0.48187 nmi: 0.55585 ari: 0.37922\n",
      "cl_acc: 0.41388 nmi: 0.55284 ari: 0.38373\n",
      "cl_acc: 0.41943 nmi: 0.54979 ari: 0.38618\n",
      "cl_acc: 0.49194 nmi: 0.54573 ari: 0.38802\n",
      "cl_acc: 0.48895 nmi: 0.54887 ari: 0.39361\n",
      "cl_acc: 0.49005 nmi: 0.55019 ari: 0.39763\n",
      "cl_acc: 0.49023 nmi: 0.54865 ari: 0.3948\n",
      "cl_acc: 0.49365 nmi: 0.54856 ari: 0.3985\n",
      "cl_acc: 0.48444 nmi: 0.54535 ari: 0.39017\n",
      "cl_acc: 0.48682 nmi: 0.53985 ari: 0.39242\n",
      "cl_acc: 0.48285 nmi: 0.54583 ari: 0.39507\n",
      "cl_acc: 0.41315 nmi: 0.54267 ari: 0.3955\n",
      "cl_acc: 0.48706 nmi: 0.543 ari: 0.39946\n",
      "cl_acc: 0.40991 nmi: 0.53979 ari: 0.39068\n",
      "cl_acc: 0.48993 nmi: 0.54069 ari: 0.39999\n",
      "cl_acc: 0.4903 nmi: 0.54362 ari: 0.40354\n",
      "cl_acc: 0.48584 nmi: 0.54223 ari: 0.39599\n",
      "cl_acc: 0.49628 nmi: 0.54794 ari: 0.41009\n",
      "cl_acc: 0.49371 nmi: 0.5383 ari: 0.40122\n",
      "cl_acc: 0.42145 nmi: 0.543 ari: 0.40885\n",
      "cl_acc: 0.4278 nmi: 0.54376 ari: 0.40856\n",
      "cl_acc: 0.41113 nmi: 0.54648 ari: 0.39525\n",
      "cl_acc: 0.47919 nmi: 0.54155 ari: 0.39449\n",
      "cl_acc: 0.39758 nmi: 0.54287 ari: 0.38203\n",
      "cl_acc: 0.3988 nmi: 0.54755 ari: 0.38982\n",
      "cl_acc: 0.40814 nmi: 0.54181 ari: 0.38263\n",
      "cl_acc: 0.47742 nmi: 0.53821 ari: 0.38217\n",
      "cl_acc: 0.47797 nmi: 0.53884 ari: 0.39272\n",
      "cl_acc: 0.40503 nmi: 0.54251 ari: 0.39252\n",
      "cl_acc: 0.40472 nmi: 0.54197 ari: 0.38883\n",
      "cl_acc: 0.47998 nmi: 0.54351 ari: 0.39185\n",
      "cl_acc: 0.48096 nmi: 0.54089 ari: 0.39035\n",
      "cl_acc: 0.48212 nmi: 0.54344 ari: 0.39222\n",
      "cl_acc: 0.48828 nmi: 0.54439 ari: 0.40026\n",
      "cl_acc: 0.48218 nmi: 0.54428 ari: 0.39436\n",
      "cl_acc: 0.4801 nmi: 0.54475 ari: 0.39543\n",
      "cl_acc: 0.41077 nmi: 0.54501 ari: 0.3944\n",
      "cl_acc: 0.48669 nmi: 0.54378 ari: 0.40414\n",
      "cl_acc: 0.48596 nmi: 0.53618 ari: 0.39414\n",
      "cl_acc: 0.48145 nmi: 0.53971 ari: 0.39268\n",
      "cl_acc: 0.41296 nmi: 0.53857 ari: 0.39479\n",
      "cl_acc: 0.49335 nmi: 0.54472 ari: 0.40375\n",
      "cl_acc: 0.4245 nmi: 0.54068 ari: 0.40068\n",
      "cl_acc: 0.48834 nmi: 0.53895 ari: 0.40041\n",
      "cl_acc: 0.48578 nmi: 0.54186 ari: 0.39464\n",
      "cl_acc: 0.41876 nmi: 0.54315 ari: 0.39541\n",
      "cl_acc: 0.47992 nmi: 0.53961 ari: 0.3944\n",
      "cl_acc: 0.47845 nmi: 0.54245 ari: 0.39568\n",
      "cl_acc: 0.47754 nmi: 0.55067 ari: 0.39938\n",
      "cl_acc: 0.4679 nmi: 0.5457 ari: 0.3851\n",
      "cl_acc: 0.4646 nmi: 0.54908 ari: 0.38711\n",
      "cl_acc: 0.45813 nmi: 0.5464 ari: 0.38189\n",
      "cl_acc: 0.45612 nmi: 0.54489 ari: 0.38436\n",
      "cl_acc: 0.45605 nmi: 0.53939 ari: 0.37786\n",
      "cl_acc: 0.46527 nmi: 0.54643 ari: 0.38921\n",
      "cl_acc: 0.38403 nmi: 0.54672 ari: 0.38119\n",
      "cl_acc: 0.46887 nmi: 0.5485 ari: 0.39234\n",
      "cl_acc: 0.40033 nmi: 0.54738 ari: 0.39572\n",
      "cl_acc: 0.39911 nmi: 0.54784 ari: 0.38689\n",
      "cl_acc: 0.47229 nmi: 0.54677 ari: 0.3931\n",
      "cl_acc: 0.39691 nmi: 0.54255 ari: 0.38508\n",
      "cl_acc: 0.40167 nmi: 0.5402 ari: 0.38391\n",
      "cl_acc: 0.4787 nmi: 0.5447 ari: 0.39634\n",
      "cl_acc: 0.40442 nmi: 0.54291 ari: 0.38988\n",
      "cl_acc: 0.47687 nmi: 0.54676 ari: 0.3958\n",
      "cl_acc: 0.47632 nmi: 0.54645 ari: 0.39782\n",
      "cl_acc: 0.41071 nmi: 0.5465 ari: 0.39829\n",
      "cl_acc: 0.40381 nmi: 0.54162 ari: 0.39135\n",
      "cl_acc: 0.48022 nmi: 0.54733 ari: 0.39491\n",
      "cl_acc: 0.479 nmi: 0.54765 ari: 0.39655\n",
      "cl_acc: 0.47186 nmi: 0.53997 ari: 0.38798\n",
      "cl_acc: 0.414 nmi: 0.54451 ari: 0.39464\n",
      "cl_acc: 0.47913 nmi: 0.54028 ari: 0.39436\n",
      "cl_acc: 0.40753 nmi: 0.54504 ari: 0.39713\n",
      "cl_acc: 0.40131 nmi: 0.54542 ari: 0.39316\n",
      "cl_acc: 0.45978 nmi: 0.54312 ari: 0.38371\n",
      "cl_acc: 0.39978 nmi: 0.5498 ari: 0.38977\n",
      "cl_acc: 0.46771 nmi: 0.54937 ari: 0.3922\n",
      "cl_acc: 0.47223 nmi: 0.54979 ari: 0.39115\n",
      "cl_acc: 0.39441 nmi: 0.54344 ari: 0.38821\n",
      "cl_acc: 0.46844 nmi: 0.55269 ari: 0.39323\n",
      "cl_acc: 0.38928 nmi: 0.5444 ari: 0.3852\n"
     ]
    }
   ],
   "source": [
    "dec.train(ttr.values, t.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4941391666040242"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_acc(t.values.flatten(), dec.predict(ttr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = GaussianMixture(19).fit_predict(ttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tr = dec.predict(ttr.values)\n",
    "dec_te = dec.predict(tte.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ami: 0.526\n",
      "ari: 0.345\n",
      "cluster acc: 0.489\n"
     ]
    }
   ],
   "source": [
    "print('ami:',round(metrics.adjusted_mutual_info_score(t.values.flatten(), km), 3))\n",
    "print('ari:', round(metrics.adjusted_rand_score(t.values.flatten(), km), 3))\n",
    "print('cluster acc:', round(cl_acc(t.values.flatten(), km), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ami:',round(metrics.adjusted_mutual_info_score(t.values.flatten(), dec_tr), 3))\n",
    "print('ari:', round(metrics.adjusted_rand_score(t.values.flatten(), dec_tr), 3))\n",
    "print('cluster acc:', round(cl_acc(t.values.flatten(), dec_tr), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete pca\n",
      "complete kmeans\n",
      "complete ggm\n"
     ]
    }
   ],
   "source": [
    "ttr = tr.drop('fiberID', axis=1)\n",
    "tte = te.drop('fiberID', axis=1)\n",
    "\n",
    "pca = PCA(15, random_state=42).fit(ttr)\n",
    "pca_tr = pca.transform(ttr)\n",
    "pca_te = pca.transform(tte)\n",
    "print('complete pca')\n",
    "\n",
    "# nmf = NMF(15, random_state=42).fit(tr2)\n",
    "# nmf_tr = nmf.transform(tr2)\n",
    "# nmf_te = nmf.transform(te2)\n",
    "# print('complete nmf')\n",
    "\n",
    "tr = np.concatenate([tr.values, pca_tr], axis=1)\n",
    "te = np.concatenate([te.values, pca_te], axis=1)\n",
    "\n",
    "km = KMeans(19, random_state=42).fit(ttr)\n",
    "km_tr1 = km.predict(ttr)\n",
    "km_tr2 = to_categorical(km_tr1)\n",
    "km_te1 = km.predict(tte)\n",
    "km_te2 = to_categorical(km_te1)\n",
    "print('complete kmeans')\n",
    "\n",
    "gm = GaussianMixture(19, random_state=42).fit(ttr)\n",
    "gm_tr1 = gm.predict(ttr)\n",
    "gm_tr2 = to_categorical(gm_tr1)\n",
    "gm_te1 = gm.predict(tte)\n",
    "gm_te2 = to_categorical(gm_te1)\n",
    "print('complete ggm')\n",
    "\n",
    "tr_X = np.concatenate([tr, km_tr1.reshape(-1, 1), gm_tr1.reshape(-1, 1), dec_tr.reshape(-1, 1)], axis=1)\n",
    "te_X = np.concatenate([te, km_te1.reshape(-1, 1), gm_te1.reshape(-1, 1), dec_te.reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "svc = SVC(random_state=42, probability=True)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "rf = RandomForestClassifier(max_depth=9,\n",
    "                           random_state=42)\n",
    "models = [knn]\n",
    "for m in models:\n",
    "    s = time()\n",
    "    print(np.mean(cross_val_score(m, tr_X, t,  scoring='neg_log_loss', cv = 4 )))\n",
    "    print(time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(tr_X, t, test_size=0.3, random_state=12, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGB_bayesian(\n",
    "    #learning_rate,\n",
    "    num_leaves, \n",
    "    bagging_fraction,\n",
    "    feature_fraction,\n",
    "    min_child_weight, \n",
    "    min_data_in_leaf,\n",
    "    max_depth,\n",
    "    reg_alpha,\n",
    "    reg_lambda\n",
    "     ):\n",
    "    # LightGBM expects next three parameters need to be integer. \n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "    \n",
    "\n",
    "    params = {\n",
    "              'num_leaves': num_leaves, \n",
    "              'min_data_in_leaf': min_data_in_leaf,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "#               'learning_rate' : 0.03,\n",
    "              'max_depth': max_depth,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'objective': 'softmax',\n",
    "              'save_binary': True,\n",
    "              'seed': 12,\n",
    "              'feature_fraction_seed': 12,\n",
    "              'bagging_seed': 12,\n",
    "              'drop_seed': 12,\n",
    "              'data_random_seed': 12,\n",
    "              'boosting': 'gbdt', ## some get better result using 'dart'\n",
    "              'verbose': 1,\n",
    "              'is_unbalance': True,\n",
    "              'boost_from_average': True,\n",
    "              'metric':'multi_logloss'}    \n",
    "    \n",
    "    ## set clf options\n",
    "    clf = lgb.LGBMClassifier(**params).fit(train_X, train_y, early_stopping_rounds=50,eval_set=[(test_X, test_y)], eval_metric='multi_logloss', verbose=0)\n",
    "    \n",
    "    score = -log_loss(test_y, clf.predict_proba(test_X))\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_LGB = {\n",
    "    'num_leaves': (300, 1000), \n",
    "    'min_data_in_leaf': (0, 150),\n",
    "    'bagging_fraction' : (0.3, 0.9),\n",
    "    'feature_fraction' : (0.3, 0.9),\n",
    "#     'learning_rate': (0.01, 0.3),\n",
    "    'min_child_weight': (0.01, 3),   \n",
    "    'reg_alpha': (0.1, 3), \n",
    "    'reg_lambda': (0.1, 3),\n",
    "    'max_depth':(6, 25),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | max_depth | min_ch... | min_da... | num_le... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.3603  \u001b[0m | \u001b[0m 0.5247  \u001b[0m | \u001b[0m 0.8704  \u001b[0m | \u001b[0m 19.91   \u001b[0m | \u001b[0m 1.8     \u001b[0m | \u001b[0m 23.4    \u001b[0m | \u001b[0m 409.2   \u001b[0m | \u001b[0m 0.2684  \u001b[0m | \u001b[0m 2.612   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.3665  \u001b[0m | \u001b[0m 0.6607  \u001b[0m | \u001b[0m 0.7248  \u001b[0m | \u001b[0m 6.391   \u001b[0m | \u001b[0m 2.91    \u001b[0m | \u001b[0m 124.9   \u001b[0m | \u001b[0m 448.6   \u001b[0m | \u001b[0m 0.6273  \u001b[0m | \u001b[0m 0.6319  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.354   \u001b[0m | \u001b[95m 0.4825  \u001b[0m | \u001b[95m 0.6149  \u001b[0m | \u001b[95m 14.21   \u001b[0m | \u001b[95m 0.8808  \u001b[0m | \u001b[95m 91.78   \u001b[0m | \u001b[95m 397.6   \u001b[0m | \u001b[95m 0.9472  \u001b[0m | \u001b[95m 1.162   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.3562  \u001b[0m | \u001b[0m 0.5736  \u001b[0m | \u001b[0m 0.7711  \u001b[0m | \u001b[0m 9.794   \u001b[0m | \u001b[0m 1.548   \u001b[0m | \u001b[0m 88.86   \u001b[0m | \u001b[0m 332.5   \u001b[0m | \u001b[0m 1.862   \u001b[0m | \u001b[0m 0.5945  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.3585  \u001b[0m | \u001b[0m 0.339   \u001b[0m | \u001b[0m 0.8693  \u001b[0m | \u001b[0m 24.35   \u001b[0m | \u001b[0m 2.427   \u001b[0m | \u001b[0m 45.69   \u001b[0m | \u001b[0m 368.4   \u001b[0m | \u001b[0m 2.084   \u001b[0m | \u001b[0m 1.376   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.365   \u001b[0m | \u001b[0m 0.3732  \u001b[0m | \u001b[0m 0.5971  \u001b[0m | \u001b[0m 6.653   \u001b[0m | \u001b[0m 2.729   \u001b[0m | \u001b[0m 38.82   \u001b[0m | \u001b[0m 763.8   \u001b[0m | \u001b[0m 1.004   \u001b[0m | \u001b[0m 1.608   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-0.3538  \u001b[0m | \u001b[95m 0.628   \u001b[0m | \u001b[95m 0.4109  \u001b[0m | \u001b[95m 24.42   \u001b[0m | \u001b[95m 2.328   \u001b[0m | \u001b[95m 140.9   \u001b[0m | \u001b[95m 926.4   \u001b[0m | \u001b[95m 1.834   \u001b[0m | \u001b[95m 2.773   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.3677  \u001b[0m | \u001b[0m 0.3531  \u001b[0m | \u001b[0m 0.4176  \u001b[0m | \u001b[0m 6.859   \u001b[0m | \u001b[0m 0.9827  \u001b[0m | \u001b[0m 58.3    \u001b[0m | \u001b[0m 489.9   \u001b[0m | \u001b[0m 2.503   \u001b[0m | \u001b[0m 1.135   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.3571  \u001b[0m | \u001b[0m 0.4686  \u001b[0m | \u001b[0m 0.6256  \u001b[0m | \u001b[0m 8.678   \u001b[0m | \u001b[0m 2.409   \u001b[0m | \u001b[0m 11.18   \u001b[0m | \u001b[0m 990.8   \u001b[0m | \u001b[0m 2.34    \u001b[0m | \u001b[0m 0.6763  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.3562  \u001b[0m | \u001b[0m 0.3033  \u001b[0m | \u001b[0m 0.7893  \u001b[0m | \u001b[0m 19.43   \u001b[0m | \u001b[0m 2.19    \u001b[0m | \u001b[0m 115.7   \u001b[0m | \u001b[0m 351.8   \u001b[0m | \u001b[0m 1.14    \u001b[0m | \u001b[0m 0.436   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.3644  \u001b[0m | \u001b[0m 0.4256  \u001b[0m | \u001b[0m 0.4279  \u001b[0m | \u001b[0m 7.149   \u001b[0m | \u001b[0m 0.922   \u001b[0m | \u001b[0m 143.6   \u001b[0m | \u001b[0m 999.2   \u001b[0m | \u001b[0m 2.622   \u001b[0m | \u001b[0m 2.477   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.3657  \u001b[0m | \u001b[0m 0.6482  \u001b[0m | \u001b[0m 0.4182  \u001b[0m | \u001b[0m 24.35   \u001b[0m | \u001b[0m 0.422   \u001b[0m | \u001b[0m 0.609   \u001b[0m | \u001b[0m 891.3   \u001b[0m | \u001b[0m 1.71    \u001b[0m | \u001b[0m 0.1138  \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m-0.3534  \u001b[0m | \u001b[95m 0.555   \u001b[0m | \u001b[95m 0.5681  \u001b[0m | \u001b[95m 23.02   \u001b[0m | \u001b[95m 0.1018  \u001b[0m | \u001b[95m 146.2   \u001b[0m | \u001b[95m 747.5   \u001b[0m | \u001b[95m 0.3348  \u001b[0m | \u001b[95m 1.804   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.3607  \u001b[0m | \u001b[0m 0.6665  \u001b[0m | \u001b[0m 0.5516  \u001b[0m | \u001b[0m 7.238   \u001b[0m | \u001b[0m 0.1477  \u001b[0m | \u001b[0m 149.4   \u001b[0m | \u001b[0m 863.0   \u001b[0m | \u001b[0m 0.9685  \u001b[0m | \u001b[0m 1.709   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.3568  \u001b[0m | \u001b[0m 0.8674  \u001b[0m | \u001b[0m 0.5896  \u001b[0m | \u001b[0m 24.19   \u001b[0m | \u001b[0m 0.1091  \u001b[0m | \u001b[0m 68.47   \u001b[0m | \u001b[0m 993.1   \u001b[0m | \u001b[0m 0.4378  \u001b[0m | \u001b[0m 2.079   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.3599  \u001b[0m | \u001b[0m 0.7696  \u001b[0m | \u001b[0m 0.5065  \u001b[0m | \u001b[0m 24.35   \u001b[0m | \u001b[0m 2.405   \u001b[0m | \u001b[0m 1.69    \u001b[0m | \u001b[0m 995.0   \u001b[0m | \u001b[0m 0.5526  \u001b[0m | \u001b[0m 2.946   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.3565  \u001b[0m | \u001b[0m 0.5658  \u001b[0m | \u001b[0m 0.3226  \u001b[0m | \u001b[0m 24.16   \u001b[0m | \u001b[0m 2.872   \u001b[0m | \u001b[0m 104.0   \u001b[0m | \u001b[0m 831.7   \u001b[0m | \u001b[0m 2.978   \u001b[0m | \u001b[0m 2.732   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.3544  \u001b[0m | \u001b[0m 0.4698  \u001b[0m | \u001b[0m 0.319   \u001b[0m | \u001b[0m 23.52   \u001b[0m | \u001b[0m 0.3417  \u001b[0m | \u001b[0m 142.6   \u001b[0m | \u001b[0m 304.2   \u001b[0m | \u001b[0m 0.7868  \u001b[0m | \u001b[0m 2.951   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.3603  \u001b[0m | \u001b[0m 0.8522  \u001b[0m | \u001b[0m 0.8439  \u001b[0m | \u001b[0m 7.26    \u001b[0m | \u001b[0m 0.05132 \u001b[0m | \u001b[0m 3.07    \u001b[0m | \u001b[0m 304.1   \u001b[0m | \u001b[0m 0.4654  \u001b[0m | \u001b[0m 2.696   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.3538  \u001b[0m | \u001b[0m 0.5061  \u001b[0m | \u001b[0m 0.5476  \u001b[0m | \u001b[0m 22.79   \u001b[0m | \u001b[0m 0.1504  \u001b[0m | \u001b[0m 90.78   \u001b[0m | \u001b[0m 304.7   \u001b[0m | \u001b[0m 0.1963  \u001b[0m | \u001b[0m 2.662   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.3547  \u001b[0m | \u001b[0m 0.8166  \u001b[0m | \u001b[0m 0.539   \u001b[0m | \u001b[0m 24.84   \u001b[0m | \u001b[0m 0.01244 \u001b[0m | \u001b[0m 138.5   \u001b[0m | \u001b[0m 388.2   \u001b[0m | \u001b[0m 2.471   \u001b[0m | \u001b[0m 2.871   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-0.354   \u001b[0m | \u001b[0m 0.4322  \u001b[0m | \u001b[0m 0.3188  \u001b[0m | \u001b[0m 24.9    \u001b[0m | \u001b[0m 0.363   \u001b[0m | \u001b[0m 141.1   \u001b[0m | \u001b[0m 652.8   \u001b[0m | \u001b[0m 0.4398  \u001b[0m | \u001b[0m 2.622   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.3541  \u001b[0m | \u001b[0m 0.3932  \u001b[0m | \u001b[0m 0.4138  \u001b[0m | \u001b[0m 24.94   \u001b[0m | \u001b[0m 0.3252  \u001b[0m | \u001b[0m 103.8   \u001b[0m | \u001b[0m 712.3   \u001b[0m | \u001b[0m 0.1039  \u001b[0m | \u001b[0m 2.521   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.3566  \u001b[0m | \u001b[0m 0.5727  \u001b[0m | \u001b[0m 0.3623  \u001b[0m | \u001b[0m 8.246   \u001b[0m | \u001b[0m 0.2138  \u001b[0m | \u001b[0m 76.37   \u001b[0m | \u001b[0m 935.9   \u001b[0m | \u001b[0m 0.173   \u001b[0m | \u001b[0m 2.956   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.3613  \u001b[0m | \u001b[0m 0.6699  \u001b[0m | \u001b[0m 0.8401  \u001b[0m | \u001b[0m 7.124   \u001b[0m | \u001b[0m 0.2087  \u001b[0m | \u001b[0m 81.92   \u001b[0m | \u001b[0m 339.6   \u001b[0m | \u001b[0m 0.4884  \u001b[0m | \u001b[0m 2.789   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.3563  \u001b[0m | \u001b[0m 0.7507  \u001b[0m | \u001b[0m 0.5379  \u001b[0m | \u001b[0m 24.55   \u001b[0m | \u001b[0m 0.3688  \u001b[0m | \u001b[0m 108.6   \u001b[0m | \u001b[0m 339.9   \u001b[0m | \u001b[0m 0.2209  \u001b[0m | \u001b[0m 0.1642  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.3542  \u001b[0m | \u001b[0m 0.8648  \u001b[0m | \u001b[0m 0.4223  \u001b[0m | \u001b[0m 24.72   \u001b[0m | \u001b[0m 2.74    \u001b[0m | \u001b[0m 148.8   \u001b[0m | \u001b[0m 718.2   \u001b[0m | \u001b[0m 1.984   \u001b[0m | \u001b[0m 1.793   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.3552  \u001b[0m | \u001b[0m 0.8364  \u001b[0m | \u001b[0m 0.5183  \u001b[0m | \u001b[0m 24.44   \u001b[0m | \u001b[0m 0.4305  \u001b[0m | \u001b[0m 110.2   \u001b[0m | \u001b[0m 926.4   \u001b[0m | \u001b[0m 2.806   \u001b[0m | \u001b[0m 0.1345  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-0.3537  \u001b[0m | \u001b[0m 0.498   \u001b[0m | \u001b[0m 0.4333  \u001b[0m | \u001b[0m 24.75   \u001b[0m | \u001b[0m 2.728   \u001b[0m | \u001b[0m 128.9   \u001b[0m | \u001b[0m 897.3   \u001b[0m | \u001b[0m 0.2375  \u001b[0m | \u001b[0m 0.2853  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.3557  \u001b[0m | \u001b[0m 0.3533  \u001b[0m | \u001b[0m 0.413   \u001b[0m | \u001b[0m 24.96   \u001b[0m | \u001b[0m 0.06651 \u001b[0m | \u001b[0m 147.4   \u001b[0m | \u001b[0m 776.5   \u001b[0m | \u001b[0m 2.288   \u001b[0m | \u001b[0m 0.1171  \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "init_points = 10\n",
    "n_iter = 20\n",
    "\n",
    "optimizer.maximize(init_points=init_points, n_iter=n_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lgb = {\n",
    "        'min_data_in_leaf': int(optimizer.max['params']['min_data_in_leaf']), \n",
    "        'num_leaves': int(optimizer.max['params']['num_leaves']), \n",
    "        #'learning_rate': LGB_BO.max['params']['learning_rate'],\n",
    "        'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "        'bagging_fraction': optimizer.max['params']['bagging_fraction'], \n",
    "        'feature_fraction': optimizer.max['params']['feature_fraction'],\n",
    "        'reg_lambda': optimizer.max['params']['reg_lambda'],\n",
    "        'reg_alpha': optimizer.max['params']['reg_alpha'],\n",
    "        'max_depth': int(optimizer.max['params']['max_depth']), \n",
    "        'objective': 'softmax',\n",
    "        'save_binary': True,\n",
    "        'seed': 12,\n",
    "        'feature_fraction_seed': 12,\n",
    "        'bagging_seed': 12,\n",
    "        'drop_seed': 12,\n",
    "        'data_random_seed': 12,\n",
    "        'boosting_type': 'gbdt',  # also consider 'dart'\n",
    "        'verbose': 1,\n",
    "        'is_unbalance': False,\n",
    "        'boost_from_average': True,\n",
    "        'metric':'multi_logloss'\n",
    "    }\n",
    "\n",
    "params = param_lgb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('best_params_robust.bin', 'wb')\n",
    "pickle.dump(params, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('best_params.bin', 'rb')\n",
    "params = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.5549882691339685, bagging_seed=12,\n",
       "               boost_from_average=True, boosting_type='gbdt', class_weight=None,\n",
       "               colsample_bytree=1.0, data_random_seed=12, drop_seed=12,\n",
       "               early_stoppong_rounds=50, feature_fraction=0.5681186119708894,\n",
       "               feature_fraction_seed=12, importance_type='split',\n",
       "               is_unbalance=False, learning_rate=0.1, max_depth=23,\n",
       "               metric='multi_logloss', min_child_samples=20,\n",
       "               min_child_weight=0.10181687904170462, min_data_in_leaf=146,\n",
       "               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=747,\n",
       "               objective='softmax', random_state=None,\n",
       "               reg_alpha=0.33481616479697107, reg_lambda=1.803712738624059,\n",
       "               save_binary=True, seed=12, silent=True, ...)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lgb_clf = lgb.LGBMClassifier(**params, early_stoppong_rounds = 50)\n",
    "lgb_clf.fit(tr_X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score is -0.3517665700897817\n"
     ]
    }
   ],
   "source": [
    "print('score is',np.mean(cross_val_score(lgb_clf, tr_X, t,  scoring='neg_log_loss', cv = 4 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = lgb_clf.predict_proba(te_X)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/robust_new_lgb4.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'############################'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parmas for xgboost\n",
    "params_fx = {'min_data_in_leaf': params['min_data_in_leaf'],\n",
    "             'num_leaves': params['num_leaves'],\n",
    "             'min_child_weight': params['min_child_weight'],\n",
    "             'bagging_fraction': params['bagging_fraction'],\n",
    "             'feature_fraction': params['feature_fraction'],\n",
    "             'reg_lambda': params['reg_lambda'],\n",
    "             'reg_alpha': params['reg_alpha'],\n",
    "             'max_depth': params['max_depth'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "            **params_fx,\n",
    "#             n_estimators=500,\n",
    "            tree_method = 'hist',\n",
    "            booster = 'gbtree',\n",
    "            eval_metric = 'mlogloss',\n",
    "            objective = 'multi:softprob',\n",
    "            num_class = 19,\n",
    "            early_stoppong_rounds = 50\n",
    "            \n",
    "    ).fit(tr_X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_clf.predict_proba(te_X)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/xgb7.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cat_clf = cat.CatBoostClassifier(early_stopping_rounds=50, random_state=42, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cat_clf.predict_proba(te_X)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/cat2.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_clf = lgb.LGBMClassifier(**params, early_stoppong_rounds = 50)\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "            **params_fx,\n",
    "#             n_estimators=500,\n",
    "            tree_method = 'hist',\n",
    "            booster = 'gbtree',\n",
    "            eval_metric = 'mlogloss',\n",
    "            objective = 'multi:softprob',\n",
    "            num_class = 19,\n",
    "            early_stoppong_rounds = 50\n",
    "            \n",
    "    )\n",
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=200,\n",
    "                                max_depth=13, \n",
    "                                max_features='sqrt', \n",
    "                                random_state=42)\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators=150,\n",
    "                                max_depth=9, \n",
    "                                max_features='sqrt', \n",
    "                                random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=9, \n",
    "                             max_features='sqrt', \n",
    "                             random_state=42)\n",
    "\n",
    "pca = PCA(15)\n",
    "\n",
    "estimators = [('lgb', lgb_clf), ('xgb', xgb_clf), ('rf', rf)]\n",
    "vclf = VotingClassifier(estimators,\n",
    "                       voting='soft',\n",
    "                       weights = [0.6, 0.3, 0.1],\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vclf.fit(tr_X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = vclf.predict_proba(te_X)\n",
    "# print(y_pred)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/vclf4.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(t, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[lgb_clf, xgb_clf], \n",
    "          [rf2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackNetClassifier(models, \n",
    "                           metric=\"logloss\", \n",
    "                           folds=4,\n",
    "                           restacking=False,\n",
    "                           use_retraining=True,\n",
    "                           use_proba=True, # To use predict_proba after training\n",
    "                           random_state=42,\n",
    "                           n_jobs=-1, \n",
    "                           verbose=1)\n",
    "\n",
    "model.fit(tr_X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(te_X)\n",
    "# print(y_pred)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "print(log_loss(t, y_pred))\n",
    "submission.to_csv('./sub/pre/stk15.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = [lgb_clf, xgb_clf, cat_clf, model]\n",
    "for m in md:\n",
    "    s = time()\n",
    "    print(np.mean(cross_val_score(m, tr_X, t,  scoring='neg_log_loss', cv = 4 )))\n",
    "    print(time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking using boosting and then NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = model.predict_up_to(tr_X)\n",
    "k2 = model.predict_up_to(te_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr_X = k1[0]\n",
    "new_te_X = k2[0]\n",
    "print(new_tr_X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('new_te_X.bin', 'wb')\n",
    "pickle.dump(new_te_X, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape = (new_tr_X.shape[1],))\n",
    "\n",
    "x = Dense(128)(inputs1)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Dense(64)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Dense(64)(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "\n",
    "outputs1 = Dense(new_tr_X.shape[1])(x)\n",
    "outputs2 = Dense(19, activation='softmax')(x)\n",
    "\n",
    "q = Model(inputs1, [outputs1, outputs2])\n",
    "\n",
    "q.compile(optimizer = 'adam', loss=['mse', 'categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q.fit(new_tr_X, [new_tr_X,target_wide], batch_size= 1024*16, epochs=200, shuffle=True, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.predict(new_te_X)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.log_loss(target_wide, q.predict(new_tr_X)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = q.predict(new_te_X)[1]\n",
    "# print(y_pred)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/stk_nn.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossentropy_AE\n",
    "ypred = KMeans(random_state=42, n_clusters=19).fit_predict(q.predict(new_tr_X)[1])\n",
    "print(  metrics.normalized_mutual_info_score(target, ypred),\n",
    "        metrics.adjusted_mutual_info_score(target, ypred),\n",
    "        metrics.adjusted_rand_score(target, ypred),\n",
    "        acc(target, ypred)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEC\n",
    "ypred = q.predict(new_te_X)[1]\n",
    "tsne = TSNE(random_state=42, perplexity=100).fit_transform(ypred)\n",
    "labels = KMeans(random_state=42, n_clusters=19).fit_predict(ypred)\n",
    "xs = tsne[:,0]\n",
    "ys = tsne[:,1]\n",
    "plt.scatter(xs,ys,c=labels)\n",
    "plt.show()\n",
    "sns.scatterplot(xs,ys,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_AE():\n",
    "    K.clear_session()\n",
    "    inputs1 = layers.Input(shape=(tr_X.shape[1], ))\n",
    "    inputs2 = layers.Input(shape=(target_wide.shape[1], ))\n",
    "#     x1 = layers.Dense(64)(inputs1)\n",
    "    x2 = layers.Dense(64)(inputs2)\n",
    "    \n",
    "    x = layers.Concatenate()([inputs1, x2])\n",
    "    \n",
    "    x = layers.Dense(32)(x)\n",
    "    x = layers.advanced_activations.LeakyReLU(0.3)(x)\n",
    "\n",
    "    cl = layers.Dense(19)(x)\n",
    "\n",
    "    x = layers.Dense(32)(cl)\n",
    "    x = layers.advanced_activations.LeakyReLU(0.3)(x)\n",
    "\n",
    "    x = layers.Dense(64)(x)\n",
    "    x = layers.advanced_activations.LeakyReLU(0.3)(x)\n",
    "\n",
    "    outputs1 = layers.Dense(tr_X.shape[1])(x)\n",
    "    outputs2 = layers.Dense(target_wide.shape[1], activation='softmax')(x)\n",
    "\n",
    "    m = models.Model([inputs1, inputs2], [outputs1, outputs2])\n",
    "    cl = models.Model([inputs1, inputs2], cl)\n",
    "    return m, cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, cl = make_AE()\n",
    "\n",
    "m.compile(loss=['mse', 'categorical_crossentropy'], optimizer=optimizers.Adam(2e-4,0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = m.fit([tr_X.values, target_wide], [tr_X.values, target_wide],\n",
    "     epochs=100,\n",
    "     batch_size=1024*128\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossentropy_AE\n",
    "km = KMeans(random_state=42, n_clusters=19).fit(cl.predict([tr_X.values, target_wide]))\n",
    "ypred = km.predict(cl.predict([tr_X.values, target_wide]))\n",
    "print(  metrics.normalized_mutual_info_score(target, ypred),\n",
    "        metrics.adjusted_mutual_info_score(target, ypred),\n",
    "        metrics.adjusted_rand_score(target, ypred),\n",
    "        acc(target, ypred)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred_wide = to_categorical(ypred)\n",
    "\n",
    "te_pred = km.predict()\n",
    "tr_pred_wide = to_categorical(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_shape = [tr_X.shape, tr_pred_wide.shape, tr_fiber.shape]\n",
    "\n",
    "inputs1 = layers.Input(shape = (inputs_shape[0][1], ))\n",
    "inputs2 = layers.Input(shape = (inputs_shape[1][1], ))\n",
    "inputs3 = layers.Input(shape = (inputs_shape[2][1], ))\n",
    "\n",
    "######### 여기부터는 test 해봐야 함\n",
    "x2 = layers.Dense(64)(inputs2)\n",
    "x3 = layers.Dense(64)(inputs3)\n",
    "\n",
    "x = layers.Concatenate()([inputs1, x2, x3])\n",
    "\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.Dense(32)(x)\n",
    "\n",
    "outputs = layers.Dense(19, activation='softmax')(x)\n",
    "\n",
    "nn = models.Model([inputs1, inputs2], outputs)\n",
    "\n",
    "nn.compile(optimizer = 'adam',\n",
    "          loss = 'sparse_categorical_crossentropy',\n",
    "          metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "inputs_shape = [train_X.shape, tr_fiber.shape]\n",
    "\n",
    "inputs1 = layers.Input(shape = (inputs_shape[0][1], ))\n",
    "inputs2 = layers.Input(shape = (inputs_shape[1][1], ))\n",
    "\n",
    "x2 = layers.Dense(64)(inputs2)\n",
    "\n",
    "x = layers.Concatenate()([inputs1, x2])\n",
    "\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.Dense(32)(x)\n",
    "\n",
    "outputs = layers.Dense(19, activation='softmax')(x)\n",
    "\n",
    "nn = models.Model([inputs1, inputs2], outputs)\n",
    "\n",
    "nn.compile(optimizer = 'adam',\n",
    "          loss = 'sparse_categorical_crossentropy',\n",
    "          metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.fit([train_X.values, tr_fiber.values], t.values,\n",
    "      epochs=500,\n",
    "      validation_split=0.15,\n",
    "      callbacks=[es],\n",
    "      batch_size=1024*16\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.predict([test_X, te_fiber])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict([test_X, te_fiber])\n",
    "# print(y_pred)\n",
    "submission = pd.DataFrame(data=y_pred, columns=sub.columns, index=sub.index)\n",
    "submission.to_csv('./sub/pre/adv_nn1.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_imp = lgb_clf.feature_importances_\n",
    "\n",
    "lgb_imp_idx = []\n",
    "# cols = tr_X.columns\n",
    "for i, imp in enumerate(lgb_imp):\n",
    "    if imp > 0:\n",
    "        lgb_imp_idx.append(i)\n",
    "    else:\n",
    "        print(i)\n",
    "        \n",
    "# new_cols_imp = cols[lgb_imp_idx]\n",
    "# new_cols_imp = list(new_cols_imp)\n",
    "# print(lgb_imp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tr_X.columns), len(new_cols_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./new_cols.bin', 'wb')\n",
    "pickle.dump(new_cols_imp, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
