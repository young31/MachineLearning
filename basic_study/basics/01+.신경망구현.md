신경망의 원리가 아닌 구현하는 법에 대해 알아봅니다.

딥러닝, 즉 신경망을 구현을 도와주는 몇 가지 프레임워크로는 pytorch, tensorflow, keras 등이 있으며 모두 파이썬에서 지원합니다.

본인은 keras를 사용하며 구현합니다.

tensorflow에서도 kears를 공식 api로 사용합니다.

keras도 많이 사용하여 부족한 부분은 없지만, 최근 pytorch가 뜨고 있는 것으로 보입니다. 

참고하셔서 맞는 언어를 선택하시길 바랍니다.

CNN, RNN 내용 참고(특히 CNN의 블로그는 유명한 블로거)

[CNN1](https://bcho.tistory.com/1149)

[CNN2](https://zzsza.github.io/data/2018/02/23/introduction-convolution/)

[RNN1](http://jaejunyoo.blogspot.com/2017/06/anyone-can-learn-to-code-LSTM-RNN-Python.html)

[RNN2](https://tykimos.github.io/2017/09/09/Time-series_Numerical_Input_Numerical_Prediction_Model_Recipe/)

# 모형 구성

- 모형을 구성하는 방법을 두 가지 정도로 나눠볼 수 있습니다.

## 1.1 개념정리

- 모형을 구성할 때는 기본적으로 층, 활성화함수, 손실함수, optimizer를 사용합니다,
- 층(layer): 기본적인 퍼센트론 층부터 CNN, RNN 등의 층을 사용할 수 있습니다.
- 활성화함수(activation): 각 노드가 어떤 출력변환을 할지 결정합니다.
  - 대표적으로는 linear(그대로), relu, tanh 를 사용합니다.
- 손실함수(loss): 사용하는 목적에 맞게 구성합니다.
  - 회귀는 mse, 분류는 crossentropy 등을 사용합니다.
- optimizer: 다양한 방법이 있습니다만, 잘 모르겠으면 adam을 사용합시다.
  - batch-size를 다루면서 이야기했던 SGD 등도 사용 가능합니다.



## 1.2 순차 구성(sequential)

- 순차구성은 선형적으로 한 층씩 쌓아 가는 방법입니다.
- 모델의 시작을 선언하고 그 뒤에 더해가는 방식으로 구현합니다.
- input의 차원에 대해 신경쓰지 않아도 되는 특징이 있습니다.
- 처음 접하기에 가장 쉽게 사용할 수 있습니다.

```python
model = models.Sequential() # 모델 시작

model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='linear')) # output의 차원과 같은 차원으로 출력노드 조정

model.compile(optimizer = 'adam',
             loss = 'mse') # 회귀이므로 loss를 mse로, 분류시 조정해줌
```

```python
# RNN 을 활용한 시계열 예제
model = models.Sequential()

model.add(layers.SimpleRNN(16, activation='tanh')) 
model.add(layers.Dense(1, activation='linear'))

model.compile(optimizer = 'adam',
             loss = 'mse')
```



## 1.3 함수형 구성

- 함수형 구성은 각 층마다 연결할 대상을 지정해 줍니다.
- 확장성이 높다는 의미에서 추천하는 방법입니다.
  - 위의 예로는 복잡한 층구성을 표현하지 못합니다.(ex) xception)

```python
inputs = models.Input(shape=(16, )) # 사용하는 변수가 16개로 가정

l1 = layers.Dense(16, activation='relu')(inputs) # 연결될 곳을 지정
outputs = layers.Dense(1, activation='linear')(l1)

model = models.Model(inputs, outputs)

model.compile(optimizer = 'SGD',
             loss = 'binary_crossentropy')
```

- 복잡한 모형예시
  - 해당 예시에서는 왜 이렇게 까지 하나 싶을 지 몰라도 inception, resnet 등의 이미지처리 모형 등을 보면 훨씬 깊게 구성하며 경사손실 등을 막기 위해서 다양한 처리를 추가함 

```python
input1 = layers.Input(shape=(16, )) # 두 종류의 다른 인풋을 함께 받는 경우
input2 = layers.Input(shape=(8, 4))

x11 = layers.Dense(32, activation='relu')(input1)
x21 = layers.LSTM(32, activation='tanh', return_sequences=False)(input2)

x12 = layers.Dense(16, activation='relu')(x11)

x2 = layers.concatenate([x12, x21])

x3 = layers.Dense(8)(x2)

output = layers.Dense(1, activation='linear')(x3)

model = models.Model([input1, input2], output)

model.compile(optimizer = optimizers.Adam(lr = 1e-4), # optimizer 세부조정도 가능함
             loss = 'mse')

```

```python
inputs = layers.Input(shape=(16, ))

x1 = layers.Dense(16, activation='relu')(inputs) # 하나의 층으로 부터 다수의 층을 만들어서 전개
x2 = layers.Dense(8, activation='relu')(inputs)
x3 = layers.Dense(16, activation='tanh')(inputs)

x = layers.concatenate([x1, x2, x3])

x = layers.Dense(16, activation = 'relu')(x)

outputs = layers.Dense(1, activation='linear')(x)

model = models.Model(inputs, outputs)

model.compile(optimizer = 'adam',
             loss = 'mae')

```



## 1.4 추가 기술 

- l1, l2 정규화항

```python
from keras.regularizers import L1L2

layers.Dense(16, kernel_regularizer=L1L2(l1=0, l2=0.1)) # 정규화 계수를 적음
```

- dropout

```python
x = layers,Dense(16)(x)
x = layers.Dropout(0.3)(x) # dropout 비율을 적음
```

- batch-normalization
  - 이 작업을 수행하는 경우 정규화 후 활성화함수를 명시적으로 적어 주어야 함

```python
x = layers.Dense(16)(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')
```

