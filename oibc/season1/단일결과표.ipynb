{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "from scipy import stats\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from scipy.stats import norm, kurtosis\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.executing_eagerly()\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers, models, optimizers, initializers, regularizers, constraints, losses\n",
    "from keras.models import Sequential, Model, load_model, Input\n",
    "from keras.layers import (Dense, Concatenate, BatchNormalization, Activation, Add,\n",
    "                          concatenate, Dropout, AlphaDropout, Reshape, Layer, Multiply, Lambda)\n",
    "from keras.layers import (Dense, Concatenate, BatchNormalization, Activation, Add,\n",
    "                          concatenate, Dropout, AlphaDropout, Reshape, Layer, Multiply)\n",
    "from keras.layers import (Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling1D, \n",
    "                          GlobalMaxPooling1D, SeparableConv1D, MaxPool1D, AveragePooling1D, \n",
    "                          SeparableConv1D, AtrousConvolution1D)\n",
    "from keras.layers import (Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, \n",
    "                          GlobalMaxPooling2D, SeparableConv2D, MaxPool2D, AveragePooling2D, \n",
    "                          SeparableConv2D, AtrousConvolution2D)\n",
    "from keras.layers import LSTM, GRU, Bidirectional\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras.losses import binary_crossentropy,  kullback_leibler_divergence, mean_squared_error\n",
    "\n",
    "def mish(x):\n",
    "    return x*K.tanh(K.softplus(x))\n",
    "\n",
    "def decay(epoch, steps=100):\n",
    "    initial_lrate = 1e-3\n",
    "    drop = 0.9\n",
    "    epochs_drop = 25\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    lrate = max(lrate, 5e-5)\n",
    "    return lrate\n",
    "\n",
    "es = EarlyStopping(patience=25, restore_best_weights=True, monitor='val_total_loss')\n",
    "lrs = LearningRateScheduler(decay, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grap_year(x):\n",
    "    y, _, _ = x.split('-')\n",
    "    return int(y)\n",
    "\n",
    "def grap_date(x):\n",
    "    y = x[5:10].replace('-','')\n",
    "    return y\n",
    "\n",
    "def grap_hour(x):\n",
    "    return x[11:13]\n",
    "\n",
    "# def custom_loss(y_true, y_pred):\n",
    "#     res = 0\n",
    "#     cnt = 0\n",
    "#     for i in range(0, len(y_true), 24):\n",
    "#         yt = y_true[i:i+24]\n",
    "#         yp = y_pred[i:i+24]\n",
    "#         a = np.abs(yt-yp)\n",
    "#         c = 113\n",
    "#         S = np.sum(yt)\n",
    "#         res += np.sum(a*yt/(S*c))\n",
    "#         cnt += 1\n",
    "#     return res/cnt\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    res = np.abs(y_true-y_pred)*y_true\n",
    "    ans = []\n",
    "    for i, s in enumerate(res):\n",
    "        ans.append(s/np.sum(y_true[i]))\n",
    "    return np.mean(ans)\n",
    "\n",
    "def custom_loss_nn(y_true, y_pred):\n",
    "    res = K.abs(y_true*113-y_pred*113)*y_true/K.sum(y_true)\n",
    "    res = K.mean(res, axis=1)\n",
    "\n",
    "    return K.mean(res)\n",
    "\n",
    "class Lookahead(keras.optimizers.Optimizer):\n",
    "    \"\"\"The lookahead mechanism for optimizers.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        optimizer: An existed optimizer.\n",
    "        sync_period: int > 0. The synchronization period.\n",
    "        slow_step: float, 0 < alpha < 1. The step size of slow weights.\n",
    "    # References\n",
    "        - [Lookahead Optimizer: k steps forward, 1 step back]\n",
    "          (https://arxiv.org/pdf/1907.08610v1.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, sync_period=5, slow_step=0.5, **kwargs):\n",
    "        super(Lookahead, self).__init__(**kwargs)\n",
    "        self.optimizer = keras.optimizers.get(optimizer)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.sync_period = K.variable(sync_period, dtype='int64', name='sync_period')\n",
    "            self.slow_step = K.variable(slow_step, name='slow_step')\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.optimizer.lr\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, lr):\n",
    "        self.optimizer.lr = lr\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self.optimizer.learning_rate\n",
    "\n",
    "    @learning_rate.setter\n",
    "    def learning_rate(self, learning_rate):\n",
    "        self.optimizer.learning_rate = learning_rate\n",
    "\n",
    "    @property\n",
    "    def iterations(self):\n",
    "        return self.optimizer.iterations\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        sync_cond = K.equal((self.iterations + 1) // self.sync_period * self.sync_period, (self.iterations + 1))\n",
    "        slow_params = {p.name: K.variable(K.get_value(p), name='sp_{}'.format(i)) for i, p in enumerate(params)}\n",
    "        update_names = ['update', 'update_add', 'update_sub']\n",
    "        original_updates = [getattr(K, name) for name in update_names]\n",
    "        setattr(K, 'update', lambda x, new_x: ('update', x, new_x))\n",
    "        setattr(K, 'update_add', lambda x, new_x: ('update_add', x, new_x))\n",
    "        setattr(K, 'update_sub', lambda x, new_x: ('update_sub', x, new_x))\n",
    "        self.updates = self.optimizer.get_updates(loss, params)\n",
    "        for name, original_update in zip(update_names, original_updates):\n",
    "            setattr(K, name, original_update)\n",
    "        slow_updates = []\n",
    "        for i, update in enumerate(self.updates):\n",
    "            if isinstance(update, tuple):\n",
    "                name, x, new_x, adjusted = update + (update[-1],)\n",
    "                update_func = getattr(K, name)\n",
    "                if name == 'update_add':\n",
    "                    adjusted = x + new_x\n",
    "                if name == 'update_sub':\n",
    "                    adjusted = x - new_x\n",
    "                if x.name not in slow_params:\n",
    "                    self.updates[i] = update_func(x, new_x)\n",
    "                else:\n",
    "                    slow_param = slow_params[x.name]\n",
    "                    slow_param_t = slow_param + self.slow_step * (adjusted - slow_param)\n",
    "                    slow_updates.append(K.update(slow_param, K.switch(\n",
    "                        sync_cond,\n",
    "                        slow_param_t,\n",
    "                        slow_param,\n",
    "                    )))\n",
    "                    self.updates[i] = K.update(x, K.switch(\n",
    "                        sync_cond,\n",
    "                        slow_param_t,\n",
    "                        adjusted,\n",
    "                    ))\n",
    "        slow_params = list(slow_params.values())\n",
    "        self.updates += slow_updates\n",
    "        self.weights = self.optimizer.weights + slow_params\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'optimizer': keras.optimizers.serialize(self.optimizer),\n",
    "            'sync_period': int(K.get_value(self.sync_period)),\n",
    "            'slow_step': float(K.get_value(self.slow_step)),\n",
    "        }\n",
    "        base_config = super(Lookahead, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        optimizer = keras.optimizers.deserialize(config.pop('optimizer'))\n",
    "        return cls(optimizer, **config)\n",
    "    \n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv('./data/SolarPV_Elec_Problem.csv', header=None)\n",
    "data = pd.read_csv('./data/features3.csv')#.iloc[0:-2,:] # 자료가 21시까지\n",
    "# sub = pd.read_csv('./data/제출양식_복원값.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2 = pd.read_csv('./data/Solar_PV_July.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target['year'] = target[0].map(grap_year)\n",
    "target['md'] = target[0].map(grap_date)\n",
    "target['hour'] = target[0].map(grap_hour)\n",
    "target = target.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2['year'] = target2[0].map(grap_year)\n",
    "target2['md'] = target2[0].map(grap_date)\n",
    "target2['hour'] = target2[0].map(grap_hour)\n",
    "target2 = target2.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.fillna(-10)\n",
    "y = target.groupby(['year', 'md', 'hour']).sum()[1].values[:8760]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = target2.groupby(['year', 'md', 'hour']).sum()[1].values\n",
    "y2 = np.hstack([y2, [0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 24\n",
    "tr_X = []\n",
    "tr_y = []\n",
    "val_X = []\n",
    "val_y = []\n",
    "\n",
    "y1 = y[:5136]\n",
    "for i in range(steps, len(y1)+1, 24):\n",
    "    tr_X.append(y1[i-steps:i])\n",
    "    tr_y.append(y1[i:i+24])\n",
    "    \n",
    "tr_X = tr_X[:-1]\n",
    "tr_y = tr_y[:-1]\n",
    "val_X.append(tr_X[-1])\n",
    "val_y.append(tr_y[-1])\n",
    "tr_X.pop(-1)\n",
    "tr_y.pop(-1)\n",
    "    \n",
    "    \n",
    "y1 = y[5160:6576]\n",
    "for i in range(steps, len(y1)+1, 24):\n",
    "    tr_X.append(y1[i-steps:i])\n",
    "    tr_y.append(y1[i:i+24])\n",
    "    \n",
    "tr_X = tr_X[:-1]\n",
    "tr_y = tr_y[:-1]\n",
    "val_X.append(tr_X[-1])\n",
    "val_y.append(tr_y[-1])\n",
    "tr_X.pop(-1)\n",
    "tr_y.pop(-1)\n",
    "    \n",
    "    \n",
    "y1 = y[6600:8040]\n",
    "for i in range(steps, len(y1)+1, 24):\n",
    "    tr_X.append(y1[i-steps:i])\n",
    "    tr_y.append(y1[i:i+24])\n",
    "    \n",
    "tr_X = tr_X[:-1]\n",
    "tr_y = tr_y[:-1]\n",
    "val_X.append(tr_X[-1])\n",
    "val_y.append(tr_y[-1])\n",
    "tr_X.pop(-1)\n",
    "tr_y.pop(-1)\n",
    "    \n",
    "    \n",
    "y1 = y[8064:8760]\n",
    "for i in range(steps, len(y1)+1, 24):\n",
    "    tr_X.append(y1[i-steps:i])\n",
    "    tr_y.append(y1[i:i+24])\n",
    "    \n",
    "tr_X = tr_X[:-1]\n",
    "tr_y = tr_y[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "       0.0000e+00, 5.0000e-03, 4.2100e-01, 4.0770e+00, 7.5910e+00,\n",
       "       6.5240e+00, 1.0965e+01, 1.4411e+01, 1.1109e+01, 1.6948e+01,\n",
       "       1.5657e+01, 1.0954e+01, 6.0110e+00, 2.5490e+00, 4.6900e-01,\n",
       "       8.0000e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = 24\n",
    "y1 = y2.copy()\n",
    "for i in range(steps, len(y1)+1, 24):\n",
    "    tr_X.append(y1[i-steps:i])\n",
    "    tr_y.append(y1[i:i+24])\n",
    "    \n",
    "te_X = tr_X[-1]\n",
    "tr_X = tr_X[:-1]\n",
    "tr_y = tr_y[:-1]\n",
    "\n",
    "val_X.append(tr_X[-1])\n",
    "val_y.append(tr_y[-1])\n",
    "tr_X.pop(-1)\n",
    "tr_y.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X = np.array(tr_X)\n",
    "tr_y = np.array(tr_y)\n",
    "val_X = np.array(val_X)[:-1]\n",
    "val_y = np.array(val_y)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((382, 24), (382, 24), (3, 24), (3, 24))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_X.shape, tr_y.shape, val_X.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_block(filters,kernel_size,n):\n",
    "    def f(x):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters=filters,\n",
    "                    kernel_size=1, \n",
    "                    padding='same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters=filters,\n",
    "                    kernel_size=kernel_size, \n",
    "                    padding='same',\n",
    "                    activation = 'tanh',\n",
    "                    dilation_rate=dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters=filters,\n",
    "                    kernel_size=kernel_size, \n",
    "                    padding='same',\n",
    "                    activation = 'sigmoid',\n",
    "                    dilation_rate=dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out,sigm_out])\n",
    "            x = Conv1D(filters = filters,\n",
    "                           kernel_size = 1,\n",
    "                           padding='same')(x)\n",
    "            # x = BatchNormalization()(x)\n",
    "    #             x = SpatialDropout1D(0.2)(x)\n",
    "            res_x = Add()([res_x,x])\n",
    "        return res_x\n",
    "    return f\n",
    "\n",
    "def conv1d_block(x, fs, ks, activation=None):\n",
    "    x = Conv1D(fs, ks, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_flat():\n",
    "    inputs = Input(shape = ttr_X.shape[1:])\n",
    "    x1 = conv1d_block(inputs, 16, 2, mish)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = conv1d_block(x1, 32, 2, mish)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = Bidirectional(LSTM(32, return_sequences=True))(x1)\n",
    "    x1 = Flatten()(x1)\n",
    "    # x1 = Attention(24)(x1)\n",
    "\n",
    "    x1_out = Dense(512, kernel_initializer='he_normal')(x1)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(256, kernel_initializer='he_normal')(x1_out)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(128, kernel_initializer='he_normal')(x1_out)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(1, kernel_initializer='he_normal')(x1_out)\n",
    "\n",
    "    x2 = conv1d_block(inputs, 16, 3, mish)\n",
    "    x2 = MaxPooling1D(2)(x2)\n",
    "    x2 = conv1d_block(x2, 32, 3, mish)\n",
    "    x2 = MaxPooling1D(2)(x2)\n",
    "    x2 = Bidirectional(LSTM(32, return_sequences=True))(x2)\n",
    "    x2 = Flatten()(x2)\n",
    "    # x2 = Attention(24)(x2)\n",
    "\n",
    "    x2_out = Dense(512, kernel_initializer='he_normal')(x2)\n",
    "    x2_out = Activation(mish)(x2_out)\n",
    "    x2_out = Dense(256, kernel_initializer='he_normal')(x2_out)\n",
    "    x2_out = Activation(mish)(x2_out)\n",
    "    x2_out = Dense(128, kernel_initializer='he_normal')(x2_out)\n",
    "    x2_out = Activation(mish)(x2_out)\n",
    "    x2_out = Dense(24)(x2_out)\n",
    "\n",
    "    x3 = conv1d_block(inputs, 16, 5, mish)\n",
    "    x3 = MaxPooling1D(2)(x3)\n",
    "    x3 = conv1d_block(x3, 32, 5, mish)\n",
    "    x3 = MaxPooling1D(2)(x3)\n",
    "    x3 = Bidirectional(LSTM(32, return_sequences=True))(x3)\n",
    "    x3 = Flatten()(x3)\n",
    "    # x3 = Attention(24)(x3)\n",
    "\n",
    "    x3_out = Dense(512, kernel_initializer='he_normal')(x3)\n",
    "    x3_out = Activation(mish)(x3_out)\n",
    "    x3_out = Dense(256, kernel_initializer='he_normal')(x3_out)\n",
    "    x3_out = Activation(mish)(x3_out)\n",
    "    x3_out = Dense(128, kernel_initializer='he_normal')(x3_out)\n",
    "    x3_out = Activation(mish)(x3_out)\n",
    "    x3_out = Dense(24)(x3_out)\n",
    "\n",
    "    x4 = conv1d_block(inputs, 16, 7, mish)\n",
    "    x4 = MaxPooling1D(2)(x4)\n",
    "    x4 = conv1d_block(x4, 32, 7, mish)\n",
    "    x4 = MaxPooling1D(2)(x4)\n",
    "    x4 = Bidirectional(LSTM(32, return_sequences=True))(x4)\n",
    "    x4 = Flatten()(x4)\n",
    "    # x4 = Attention(24)(x4)\n",
    "\n",
    "    x4_out = Dense(512, kernel_initializer='he_normal')(x4)\n",
    "    x4_out = Activation(mish)(x4_out)\n",
    "    x4_out = Dense(256, kernel_initializer='he_normal')(x4_out)\n",
    "    x4_out = Activation(mish)(x4_out)\n",
    "    x4_out = Dense(128, kernel_initializer='he_normal')(x4_out)\n",
    "    x4_out = Activation(mish)(x4_out)\n",
    "    x4_out = Dense(24)(x4_out)\n",
    "\n",
    "    x = Add()([x1, x2, x3, x4])\n",
    "    x = Dense(512, kernel_initializer='he_normal')(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = Dense(256, kernel_initializer='he_normal')(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = Dense(24)(x)\n",
    "\n",
    "    nn = Model(inputs, [x, x1_out,x2_out,x3_out, x4_out])\n",
    "    return nn\n",
    "\n",
    "\n",
    "def build_att():\n",
    "    inputs = Input(shape = ttr_X.shape[1:])\n",
    "    x1 = conv1d_block(inputs, 16, 2, mish)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = conv1d_block(x1, 32, 2, mish)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = Bidirectional(LSTM(32, return_sequences=True))(x1)\n",
    "#     x1 = Flatten()(x1)\n",
    "    x1 = Attention(6)(x1)\n",
    "\n",
    "    x1_out = Dense(512, kernel_initializer='he_normal')(x1)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(256, kernel_initializer='he_normal')(x1_out)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(128, kernel_initializer='he_normal')(x1_out)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(24, kernel_initializer='he_normal')(x1_out)\n",
    "\n",
    "    x2 = conv1d_block(inputs, 16, 3, mish)\n",
    "    x2 = MaxPooling1D(2)(x2)\n",
    "    x2 = conv1d_block(x2, 32, 3, mish)\n",
    "    x2 = MaxPooling1D(2)(x2)\n",
    "    x2 = Bidirectional(LSTM(32, return_sequences=True))(x2)\n",
    "#     x2 = Flatten()(x2)\n",
    "    x2 = Attention(6)(x2)\n",
    "\n",
    "    x2_out = Dense(64, kernel_initializer='he_normal')(x2)\n",
    "    x2_out = Activation(mish)(x2_out)\n",
    "    x2_out = Dense(32, kernel_initializer='he_normal')(x2_out)\n",
    "    x2_out = Activation(mish)(x2_out)\n",
    "#     x2_out = Dense(128, kernel_initializer='he_normal')(x2_out)\n",
    "#     x2_out = Activation(mish)(x2_out)\n",
    "    x2_out = Dense(24)(x2_out)\n",
    "\n",
    "    x3 = conv1d_block(inputs, 16, 5, mish)\n",
    "    x3 = MaxPooling1D(2)(x3)\n",
    "    x3 = conv1d_block(x3, 32, 5, mish)\n",
    "    x3 = MaxPooling1D(2)(x3)\n",
    "    x3 = Bidirectional(LSTM(32, return_sequences=True))(x3)\n",
    "#     x3 = Flatten()(x3)\n",
    "    x3 = Attention(6)(x3)\n",
    "\n",
    "    x3_out = Dense(64, kernel_initializer='he_normal')(x3)\n",
    "    x3_out = Activation(mish)(x3_out)\n",
    "    x3_out = Dense(32, kernel_initializer='he_normal')(x3_out)\n",
    "    x3_out = Activation(mish)(x3_out)\n",
    "#     x3_out = Dense(128, kernel_initializer='he_normal')(x3_out)\n",
    "#     x3_out = Activation(mish)(x3_out)\n",
    "    x3_out = Dense(24)(x3_out)\n",
    "\n",
    "    x4 = conv1d_block(inputs, 16, 7, mish)\n",
    "    x4 = MaxPooling1D(2)(x4)\n",
    "    x4 = conv1d_block(x4, 32, 7, mish)\n",
    "    x4 = MaxPooling1D(2)(x4)\n",
    "    x4 = Bidirectional(LSTM(32, return_sequences=True))(x4)\n",
    "#     x4 = Flatten()(x4)\n",
    "    x4 = Attention(6)(x4)\n",
    "\n",
    "    x4_out = Dense(64, kernel_initializer='he_normal')(x4)\n",
    "    x4_out = Activation(mish)(x4_out)\n",
    "    x4_out = Dense(32, kernel_initializer='he_normal')(x4_out)\n",
    "    x4_out = Activation(mish)(x4_out)\n",
    "#     x4_out = Dense(128, kernel_initializer='he_normal')(x4_out)\n",
    "#     x4_out = Activation(mish)(x4_out)\n",
    "    x4_out = Dense(24)(x4_out)\n",
    "\n",
    "    x = Add()([x1, x2, x3, x4])\n",
    "    x = Dense(64, kernel_initializer='he_normal')(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = Dense(32, kernel_initializer='he_normal')(x)\n",
    "    x = Activation(mish)(x)\n",
    "#     x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "#     x = Activation(mish)(x)\n",
    "    x = Dense(24)(x)\n",
    "\n",
    "    nn = Model(inputs, [x, x1_out,x2_out,x3_out, x4_out])\n",
    "    return nn\n",
    "\n",
    "def build_wav():\n",
    "    inputs = Input(shape = ttr_X.shape[1:])\n",
    "    x1 = wave_block(16, 3, 5)(inputs)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = wave_block(32, 3, 3)(x1)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = Bidirectional(LSTM(32, return_sequences=True))(x1)\n",
    "    x1 = Attention(6)(x1)\n",
    "\n",
    "    x = Dense(64, kernel_initializer='he_normal')(x1)\n",
    "    x = Activation(mish)(x)\n",
    "#     x = Dense(32, kernel_initializer='he_normal')(x)\n",
    "#     x = Activation(mish)(x)\n",
    "#     x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "#     x = Activation(mish)(x)\n",
    "    x = Dense(24)(x)\n",
    "\n",
    "    nn = Model(inputs, x)\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 24, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttr_X = np.expand_dims(tr_X, -1)\n",
    "ttr_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 24, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 24, 16)       48          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 24, 16)       0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 24, 16)       64          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 24, 16)       96          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 24, 16)       128         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 12, 16)       0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 24, 16)       0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 24, 16)       0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 24, 16)       0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 12, 32)       1056        max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 12, 16)       0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 12, 16)       0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 12, 16)       0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 12, 32)       0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 12, 32)       1568        max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 12, 32)       2592        max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 12, 32)       3616        max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 6, 32)        0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 12, 32)       0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 12, 32)       0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 12, 32)       0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 6, 64)        16640       max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 6, 32)        0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 6, 32)        0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 6, 32)        0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 64)           70          bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 6, 64)        16640       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 6, 64)        16640       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 6, 64)        16640       max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 64)           70          bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 64)           70          bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 64)           70          bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 512)          33280       attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 64)           0           attention_6[0][0]                \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 attention_8[0][0]                \n",
      "                                                                 attention_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 512)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 64)           4160        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 256)          131328      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 64)           4160        attention_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 64)           4160        attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 64)           4160        attention_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 64)           0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 256)          0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64)           0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 64)           0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 64)           0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 32)           2080        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 128)          32896       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 32)           2080        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 32)           2080        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 32)           2080        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32)           0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 128)          0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 32)           0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32)           0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32)           0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 24)           792         activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 24)           3096        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 24)           792         activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 24)           792         activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 24)           792         activation_37[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 304,736\n",
      "Trainable params: 304,736\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn = build_att()\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 305 samples, validate on 77 samples\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 5s 16ms/step - loss: 0.3004 - dense_34_loss: 0.0631 - dense_22_loss: 0.0416 - dense_25_loss: 0.0628 - dense_28_loss: 0.0691 - dense_31_loss: 0.0677 - val_loss: 0.2277 - val_dense_34_loss: 0.0518 - val_dense_22_loss: 0.0351 - val_dense_25_loss: 0.0530 - val_dense_28_loss: 0.0601 - val_dense_31_loss: 0.0559\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.1638 - dense_34_loss: 0.0335 - dense_22_loss: 0.0249 - dense_25_loss: 0.0366 - dense_28_loss: 0.0376 - dense_31_loss: 0.0331 - val_loss: 0.1533 - val_dense_34_loss: 0.0406 - val_dense_22_loss: 0.0320 - val_dense_25_loss: 0.0389 - val_dense_28_loss: 0.0359 - val_dense_31_loss: 0.0335\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.1176 - dense_34_loss: 0.0243 - dense_22_loss: 0.0226 - dense_25_loss: 0.0256 - dense_28_loss: 0.0239 - dense_31_loss: 0.0246 - val_loss: 0.1296 - val_dense_34_loss: 0.0304 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0349 - val_dense_28_loss: 0.0299 - val_dense_31_loss: 0.0318\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.1061 - dense_34_loss: 0.0216 - dense_22_loss: 0.0217 - dense_25_loss: 0.0232 - dense_28_loss: 0.0221 - dense_31_loss: 0.0226 - val_loss: 0.1215 - val_dense_34_loss: 0.0301 - val_dense_22_loss: 0.0277 - val_dense_25_loss: 0.0308 - val_dense_28_loss: 0.0308 - val_dense_31_loss: 0.0303\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.1016 - dense_34_loss: 0.0211 - dense_22_loss: 0.0208 - dense_25_loss: 0.0213 - dense_28_loss: 0.0204 - dense_31_loss: 0.0210 - val_loss: 0.1188 - val_dense_34_loss: 0.0312 - val_dense_22_loss: 0.0317 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0281 - val_dense_31_loss: 0.0299\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0976 - dense_34_loss: 0.0204 - dense_22_loss: 0.0207 - dense_25_loss: 0.0204 - dense_28_loss: 0.0195 - dense_31_loss: 0.0199 - val_loss: 0.1138 - val_dense_34_loss: 0.0292 - val_dense_22_loss: 0.0278 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0281 - val_dense_31_loss: 0.0282\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0954 - dense_34_loss: 0.0200 - dense_22_loss: 0.0199 - dense_25_loss: 0.0199 - dense_28_loss: 0.0195 - dense_31_loss: 0.0197 - val_loss: 0.1129 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0279 - val_dense_31_loss: 0.0283\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0954 - dense_34_loss: 0.0199 - dense_22_loss: 0.0200 - dense_25_loss: 0.0201 - dense_28_loss: 0.0197 - dense_31_loss: 0.0198 - val_loss: 0.1132 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0285 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0288\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0921 - dense_34_loss: 0.0186 - dense_22_loss: 0.0188 - dense_25_loss: 0.0192 - dense_28_loss: 0.0188 - dense_31_loss: 0.0189 - val_loss: 0.1118 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0277 - val_dense_25_loss: 0.0284 - val_dense_28_loss: 0.0280 - val_dense_31_loss: 0.0284\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0924 - dense_34_loss: 0.0185 - dense_22_loss: 0.0189 - dense_25_loss: 0.0191 - dense_28_loss: 0.0188 - dense_31_loss: 0.0189 - val_loss: 0.1121 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0274 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0281 - val_dense_31_loss: 0.0286\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0959 - dense_34_loss: 0.0202 - dense_22_loss: 0.0209 - dense_25_loss: 0.0201 - dense_28_loss: 0.0195 - dense_31_loss: 0.0199 - val_loss: 0.1148 - val_dense_34_loss: 0.0303 - val_dense_22_loss: 0.0295 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0290\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0918 - dense_34_loss: 0.0187 - dense_22_loss: 0.0195 - dense_25_loss: 0.0193 - dense_28_loss: 0.0193 - dense_31_loss: 0.0192 - val_loss: 0.1125 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0282 - val_dense_28_loss: 0.0278 - val_dense_31_loss: 0.0284\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0915 - dense_34_loss: 0.0186 - dense_22_loss: 0.0189 - dense_25_loss: 0.0191 - dense_28_loss: 0.0190 - dense_31_loss: 0.0189 - val_loss: 0.1127 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0279 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0284 - val_dense_31_loss: 0.0293\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0907 - dense_34_loss: 0.0189 - dense_22_loss: 0.0189 - dense_25_loss: 0.0190 - dense_28_loss: 0.0190 - dense_31_loss: 0.0190 - val_loss: 0.1129 - val_dense_34_loss: 0.0292 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0284 - val_dense_28_loss: 0.0278 - val_dense_31_loss: 0.0283\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0888 - dense_34_loss: 0.0179 - dense_22_loss: 0.0179 - dense_25_loss: 0.0182 - dense_28_loss: 0.0183 - dense_31_loss: 0.0182 - val_loss: 0.1142 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0292 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0299\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0885 - dense_34_loss: 0.0179 - dense_22_loss: 0.0177 - dense_25_loss: 0.0182 - dense_28_loss: 0.0183 - dense_31_loss: 0.0184 - val_loss: 0.1135 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0298 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0282 - val_dense_31_loss: 0.0287\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0887 - dense_34_loss: 0.0181 - dense_22_loss: 0.0179 - dense_25_loss: 0.0182 - dense_28_loss: 0.0182 - dense_31_loss: 0.0184 - val_loss: 0.1131 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0284 - val_dense_28_loss: 0.0278 - val_dense_31_loss: 0.0279\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0895 - dense_34_loss: 0.0183 - dense_22_loss: 0.0185 - dense_25_loss: 0.0183 - dense_28_loss: 0.0185 - dense_31_loss: 0.0184 - val_loss: 0.1130 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0282 - val_dense_28_loss: 0.0280 - val_dense_31_loss: 0.0285\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0904 - dense_34_loss: 0.0188 - dense_22_loss: 0.0185 - dense_25_loss: 0.0191 - dense_28_loss: 0.0192 - dense_31_loss: 0.0190 - val_loss: 0.1146 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0295 - val_dense_25_loss: 0.0284 - val_dense_28_loss: 0.0282 - val_dense_31_loss: 0.0288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0882 - dense_34_loss: 0.0180 - dense_22_loss: 0.0186 - dense_25_loss: 0.0183 - dense_28_loss: 0.0182 - dense_31_loss: 0.0181 - val_loss: 0.1139 - val_dense_34_loss: 0.0293 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0285 - val_dense_31_loss: 0.0296\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0885 - dense_34_loss: 0.0186 - dense_22_loss: 0.0186 - dense_25_loss: 0.0189 - dense_28_loss: 0.0191 - dense_31_loss: 0.0189 - val_loss: 0.1154 - val_dense_34_loss: 0.0297 - val_dense_22_loss: 0.0281 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0290 - val_dense_31_loss: 0.0302\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0861 - dense_34_loss: 0.0175 - dense_22_loss: 0.0177 - dense_25_loss: 0.0177 - dense_28_loss: 0.0181 - dense_31_loss: 0.0176 - val_loss: 0.1134 - val_dense_34_loss: 0.0284 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0285 - val_dense_28_loss: 0.0278 - val_dense_31_loss: 0.0286\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0892 - dense_34_loss: 0.0186 - dense_22_loss: 0.0190 - dense_25_loss: 0.0190 - dense_28_loss: 0.0193 - dense_31_loss: 0.0187 - val_loss: 0.1147 - val_dense_34_loss: 0.0284 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0285 - val_dense_28_loss: 0.0282 - val_dense_31_loss: 0.0290\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 0.001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0874 - dense_34_loss: 0.0181 - dense_22_loss: 0.0185 - dense_25_loss: 0.0185 - dense_28_loss: 0.0185 - dense_31_loss: 0.0181 - val_loss: 0.1150 - val_dense_34_loss: 0.0288 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0294 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0293\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0851 - dense_34_loss: 0.0170 - dense_22_loss: 0.0172 - dense_25_loss: 0.0174 - dense_28_loss: 0.0176 - dense_31_loss: 0.0171 - val_loss: 0.1132 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0284 - val_dense_31_loss: 0.0291\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0858 - dense_34_loss: 0.0174 - dense_22_loss: 0.0174 - dense_25_loss: 0.0176 - dense_28_loss: 0.0184 - dense_31_loss: 0.0173 - val_loss: 0.1208 - val_dense_34_loss: 0.0313 - val_dense_22_loss: 0.0301 - val_dense_25_loss: 0.0328 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0318\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0887 - dense_34_loss: 0.0183 - dense_22_loss: 0.0177 - dense_25_loss: 0.0184 - dense_28_loss: 0.0184 - dense_31_loss: 0.0178 - val_loss: 0.1137 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0301 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0282 - val_dense_31_loss: 0.0294\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0864 - dense_34_loss: 0.0177 - dense_22_loss: 0.0179 - dense_25_loss: 0.0177 - dense_28_loss: 0.0179 - dense_31_loss: 0.0179 - val_loss: 0.1136 - val_dense_34_loss: 0.0288 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0285 - val_dense_31_loss: 0.0289\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0873 - dense_34_loss: 0.0176 - dense_22_loss: 0.0192 - dense_25_loss: 0.0179 - dense_28_loss: 0.0180 - dense_31_loss: 0.0179 - val_loss: 0.1145 - val_dense_34_loss: 0.0295 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0288\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0895 - dense_34_loss: 0.0185 - dense_22_loss: 0.0197 - dense_25_loss: 0.0186 - dense_28_loss: 0.0183 - dense_31_loss: 0.0179 - val_loss: 0.1144 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0284 - val_dense_31_loss: 0.0286\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0861 - dense_34_loss: 0.0179 - dense_22_loss: 0.0181 - dense_25_loss: 0.0178 - dense_28_loss: 0.0175 - dense_31_loss: 0.0174 - val_loss: 0.1126 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0281 - val_dense_25_loss: 0.0284 - val_dense_28_loss: 0.0284 - val_dense_31_loss: 0.0285\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0846 - dense_34_loss: 0.0173 - dense_22_loss: 0.0179 - dense_25_loss: 0.0177 - dense_28_loss: 0.0176 - dense_31_loss: 0.0174 - val_loss: 0.1144 - val_dense_34_loss: 0.0293 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0299 - val_dense_28_loss: 0.0286 - val_dense_31_loss: 0.0293\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0857 - dense_34_loss: 0.0178 - dense_22_loss: 0.0180 - dense_25_loss: 0.0181 - dense_28_loss: 0.0184 - dense_31_loss: 0.0180 - val_loss: 0.1207 - val_dense_34_loss: 0.0308 - val_dense_22_loss: 0.0305 - val_dense_25_loss: 0.0315 - val_dense_28_loss: 0.0301 - val_dense_31_loss: 0.0314\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0904 - dense_34_loss: 0.0187 - dense_22_loss: 0.0189 - dense_25_loss: 0.0189 - dense_28_loss: 0.0189 - dense_31_loss: 0.0184 - val_loss: 0.1125 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0282 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0278\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0913 - dense_34_loss: 0.0188 - dense_22_loss: 0.0199 - dense_25_loss: 0.0185 - dense_28_loss: 0.0188 - dense_31_loss: 0.0183 - val_loss: 0.1133 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0283 - val_dense_28_loss: 0.0279 - val_dense_31_loss: 0.0281\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0866 - dense_34_loss: 0.0179 - dense_22_loss: 0.0189 - dense_25_loss: 0.0180 - dense_28_loss: 0.0183 - dense_31_loss: 0.0180 - val_loss: 0.1136 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0292 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0285 - val_dense_31_loss: 0.0288\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0842 - dense_34_loss: 0.0172 - dense_22_loss: 0.0177 - dense_25_loss: 0.0175 - dense_28_loss: 0.0176 - dense_31_loss: 0.0174 - val_loss: 0.1138 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0291 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0288\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0839 - dense_34_loss: 0.0171 - dense_22_loss: 0.0177 - dense_25_loss: 0.0175 - dense_28_loss: 0.0176 - dense_31_loss: 0.0173 - val_loss: 0.1146 - val_dense_34_loss: 0.0294 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0297 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0296\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0833 - dense_34_loss: 0.0169 - dense_22_loss: 0.0173 - dense_25_loss: 0.0172 - dense_28_loss: 0.0173 - dense_31_loss: 0.0171 - val_loss: 0.1127 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0283\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0827 - dense_34_loss: 0.0170 - dense_22_loss: 0.0172 - dense_25_loss: 0.0173 - dense_28_loss: 0.0174 - dense_31_loss: 0.0172 - val_loss: 0.1166 - val_dense_34_loss: 0.0300 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0304 - val_dense_28_loss: 0.0300 - val_dense_31_loss: 0.0300\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0848 - dense_34_loss: 0.0180 - dense_22_loss: 0.0179 - dense_25_loss: 0.0183 - dense_28_loss: 0.0182 - dense_31_loss: 0.0181 - val_loss: 0.1167 - val_dense_34_loss: 0.0299 - val_dense_22_loss: 0.0299 - val_dense_25_loss: 0.0305 - val_dense_28_loss: 0.0299 - val_dense_31_loss: 0.0300\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0851 - dense_34_loss: 0.0176 - dense_22_loss: 0.0173 - dense_25_loss: 0.0177 - dense_28_loss: 0.0178 - dense_31_loss: 0.0175 - val_loss: 0.1156 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0281\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0865 - dense_34_loss: 0.0181 - dense_22_loss: 0.0174 - dense_25_loss: 0.0183 - dense_28_loss: 0.0183 - dense_31_loss: 0.0178 - val_loss: 0.1139 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0292 - val_dense_25_loss: 0.0294 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0286\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0866 - dense_34_loss: 0.0184 - dense_22_loss: 0.0184 - dense_25_loss: 0.0185 - dense_28_loss: 0.0187 - dense_31_loss: 0.0182 - val_loss: 0.1195 - val_dense_34_loss: 0.0304 - val_dense_22_loss: 0.0313 - val_dense_25_loss: 0.0308 - val_dense_28_loss: 0.0311 - val_dense_31_loss: 0.0306\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0861 - dense_34_loss: 0.0178 - dense_22_loss: 0.0177 - dense_25_loss: 0.0180 - dense_28_loss: 0.0182 - dense_31_loss: 0.0180 - val_loss: 0.1130 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0293 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0281 - val_dense_31_loss: 0.0282\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0837 - dense_34_loss: 0.0168 - dense_22_loss: 0.0178 - dense_25_loss: 0.0171 - dense_28_loss: 0.0171 - dense_31_loss: 0.0169 - val_loss: 0.1132 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0291 - val_dense_31_loss: 0.0288\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0832 - dense_34_loss: 0.0169 - dense_22_loss: 0.0175 - dense_25_loss: 0.0173 - dense_28_loss: 0.0171 - dense_31_loss: 0.0171 - val_loss: 0.1138 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0290 - val_dense_31_loss: 0.0287\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0823 - dense_34_loss: 0.0167 - dense_22_loss: 0.0170 - dense_25_loss: 0.0170 - dense_28_loss: 0.0172 - dense_31_loss: 0.0170 - val_loss: 0.1156 - val_dense_34_loss: 0.0293 - val_dense_22_loss: 0.0291 - val_dense_25_loss: 0.0303 - val_dense_28_loss: 0.0299 - val_dense_31_loss: 0.0297\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 0.0009000000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0824 - dense_34_loss: 0.0170 - dense_22_loss: 0.0171 - dense_25_loss: 0.0173 - dense_28_loss: 0.0172 - dense_31_loss: 0.0172 - val_loss: 0.1124 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0285\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0822 - dense_34_loss: 0.0170 - dense_22_loss: 0.0172 - dense_25_loss: 0.0173 - dense_28_loss: 0.0171 - dense_31_loss: 0.0171 - val_loss: 0.1129 - val_dense_34_loss: 0.0284 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0284 - val_dense_31_loss: 0.0284\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0836 - dense_34_loss: 0.0173 - dense_22_loss: 0.0174 - dense_25_loss: 0.0176 - dense_28_loss: 0.0176 - dense_31_loss: 0.0174 - val_loss: 0.1147 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0287 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0284 - val_dense_31_loss: 0.0283\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0845 - dense_34_loss: 0.0174 - dense_22_loss: 0.0174 - dense_25_loss: 0.0174 - dense_28_loss: 0.0176 - dense_31_loss: 0.0175 - val_loss: 0.1168 - val_dense_34_loss: 0.0298 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0306 - val_dense_28_loss: 0.0303 - val_dense_31_loss: 0.0305\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0831 - dense_34_loss: 0.0172 - dense_22_loss: 0.0171 - dense_25_loss: 0.0177 - dense_28_loss: 0.0175 - dense_31_loss: 0.0176 - val_loss: 0.1121 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0282\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0824 - dense_34_loss: 0.0168 - dense_22_loss: 0.0170 - dense_25_loss: 0.0173 - dense_28_loss: 0.0173 - dense_31_loss: 0.0171 - val_loss: 0.1126 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0282\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0825 - dense_34_loss: 0.0170 - dense_22_loss: 0.0171 - dense_25_loss: 0.0172 - dense_28_loss: 0.0173 - dense_31_loss: 0.0171 - val_loss: 0.1131 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0286 - val_dense_31_loss: 0.0288\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0819 - dense_34_loss: 0.0167 - dense_22_loss: 0.0170 - dense_25_loss: 0.0170 - dense_28_loss: 0.0169 - dense_31_loss: 0.0169 - val_loss: 0.1126 - val_dense_34_loss: 0.0284 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0286 - val_dense_31_loss: 0.0286\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0813 - dense_34_loss: 0.0166 - dense_22_loss: 0.0169 - dense_25_loss: 0.0170 - dense_28_loss: 0.0168 - dense_31_loss: 0.0167 - val_loss: 0.1136 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0298 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0286 - val_dense_31_loss: 0.0286\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0815 - dense_34_loss: 0.0166 - dense_22_loss: 0.0170 - dense_25_loss: 0.0168 - dense_28_loss: 0.0169 - dense_31_loss: 0.0168 - val_loss: 0.1126 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0287 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0285 - val_dense_31_loss: 0.0285\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0873 - dense_34_loss: 0.0186 - dense_22_loss: 0.0187 - dense_25_loss: 0.0186 - dense_28_loss: 0.0185 - dense_31_loss: 0.0186 - val_loss: 0.1133 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0287 - val_dense_25_loss: 0.0285 - val_dense_28_loss: 0.0282 - val_dense_31_loss: 0.0281\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0808 - dense_34_loss: 0.0164 - dense_22_loss: 0.0165 - dense_25_loss: 0.0166 - dense_28_loss: 0.0165 - dense_31_loss: 0.0166 - val_loss: 0.1129 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0292 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0282 - val_dense_31_loss: 0.0282\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0827 - dense_34_loss: 0.0169 - dense_22_loss: 0.0167 - dense_25_loss: 0.0170 - dense_28_loss: 0.0170 - dense_31_loss: 0.0171 - val_loss: 0.1134 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0287 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0296\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0816 - dense_34_loss: 0.0165 - dense_22_loss: 0.0165 - dense_25_loss: 0.0168 - dense_28_loss: 0.0170 - dense_31_loss: 0.0168 - val_loss: 0.1152 - val_dense_34_loss: 0.0292 - val_dense_22_loss: 0.0293 - val_dense_25_loss: 0.0306 - val_dense_28_loss: 0.0297 - val_dense_31_loss: 0.0295\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0842 - dense_34_loss: 0.0172 - dense_22_loss: 0.0168 - dense_25_loss: 0.0181 - dense_28_loss: 0.0176 - dense_31_loss: 0.0176 - val_loss: 0.1134 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0281\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0824 - dense_34_loss: 0.0170 - dense_22_loss: 0.0172 - dense_25_loss: 0.0177 - dense_28_loss: 0.0169 - dense_31_loss: 0.0173 - val_loss: 0.1137 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0298 - val_dense_28_loss: 0.0286 - val_dense_31_loss: 0.0287\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0815 - dense_34_loss: 0.0168 - dense_22_loss: 0.0165 - dense_25_loss: 0.0169 - dense_28_loss: 0.0166 - dense_31_loss: 0.0167 - val_loss: 0.1139 - val_dense_34_loss: 0.0294 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0299 - val_dense_28_loss: 0.0291 - val_dense_31_loss: 0.0290\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0805 - dense_34_loss: 0.0164 - dense_22_loss: 0.0167 - dense_25_loss: 0.0167 - dense_28_loss: 0.0165 - dense_31_loss: 0.0166 - val_loss: 0.1127 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0286\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0810 - dense_34_loss: 0.0164 - dense_22_loss: 0.0169 - dense_25_loss: 0.0166 - dense_28_loss: 0.0166 - dense_31_loss: 0.0167 - val_loss: 0.1163 - val_dense_34_loss: 0.0296 - val_dense_22_loss: 0.0315 - val_dense_25_loss: 0.0299 - val_dense_28_loss: 0.0296 - val_dense_31_loss: 0.0295\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0828 - dense_34_loss: 0.0169 - dense_22_loss: 0.0173 - dense_25_loss: 0.0172 - dense_28_loss: 0.0171 - dense_31_loss: 0.0171 - val_loss: 0.1139 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0287\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0815 - dense_34_loss: 0.0168 - dense_22_loss: 0.0170 - dense_25_loss: 0.0172 - dense_28_loss: 0.0171 - dense_31_loss: 0.0171 - val_loss: 0.1124 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0286 - val_dense_31_loss: 0.0283\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0808 - dense_34_loss: 0.0166 - dense_22_loss: 0.0169 - dense_25_loss: 0.0168 - dense_28_loss: 0.0169 - dense_31_loss: 0.0168 - val_loss: 0.1133 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0282 - val_dense_31_loss: 0.0284\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0826 - dense_34_loss: 0.0170 - dense_22_loss: 0.0178 - dense_25_loss: 0.0173 - dense_28_loss: 0.0173 - dense_31_loss: 0.0173 - val_loss: 0.1138 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0289\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0822 - dense_34_loss: 0.0168 - dense_22_loss: 0.0179 - dense_25_loss: 0.0171 - dense_28_loss: 0.0169 - dense_31_loss: 0.0170 - val_loss: 0.1124 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0282\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0827 - dense_34_loss: 0.0165 - dense_22_loss: 0.0174 - dense_25_loss: 0.0172 - dense_28_loss: 0.0165 - dense_31_loss: 0.0168 - val_loss: 0.1127 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0285 - val_dense_31_loss: 0.0286\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: LearningRateScheduler setting learning rate to 0.0008100000000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0829 - dense_34_loss: 0.0173 - dense_22_loss: 0.0176 - dense_25_loss: 0.0174 - dense_28_loss: 0.0175 - dense_31_loss: 0.0173 - val_loss: 0.1138 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0290 - val_dense_31_loss: 0.0293\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0823 - dense_34_loss: 0.0170 - dense_22_loss: 0.0172 - dense_25_loss: 0.0173 - dense_28_loss: 0.0172 - dense_31_loss: 0.0171 - val_loss: 0.1131 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0295 - val_dense_31_loss: 0.0290\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0824 - dense_34_loss: 0.0174 - dense_22_loss: 0.0176 - dense_25_loss: 0.0175 - dense_28_loss: 0.0174 - dense_31_loss: 0.0175 - val_loss: 0.1125 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0285 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0284\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0824 - dense_34_loss: 0.0173 - dense_22_loss: 0.0179 - dense_25_loss: 0.0177 - dense_28_loss: 0.0175 - dense_31_loss: 0.0174 - val_loss: 0.1129 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0285 - val_dense_31_loss: 0.0289\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0795 - dense_34_loss: 0.0160 - dense_22_loss: 0.0162 - dense_25_loss: 0.0163 - dense_28_loss: 0.0160 - dense_31_loss: 0.0162 - val_loss: 0.1126 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0286\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0797 - dense_34_loss: 0.0161 - dense_22_loss: 0.0165 - dense_25_loss: 0.0164 - dense_28_loss: 0.0162 - dense_31_loss: 0.0162 - val_loss: 0.1125 - val_dense_34_loss: 0.0284 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0286 - val_dense_31_loss: 0.0286\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0805 - dense_34_loss: 0.0165 - dense_22_loss: 0.0168 - dense_25_loss: 0.0169 - dense_28_loss: 0.0164 - dense_31_loss: 0.0167 - val_loss: 0.1120 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0285\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0820 - dense_34_loss: 0.0173 - dense_22_loss: 0.0178 - dense_25_loss: 0.0173 - dense_28_loss: 0.0172 - dense_31_loss: 0.0172 - val_loss: 0.1130 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0294 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0288\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0822 - dense_34_loss: 0.0170 - dense_22_loss: 0.0173 - dense_25_loss: 0.0174 - dense_28_loss: 0.0169 - dense_31_loss: 0.0171 - val_loss: 0.1123 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0286 - val_dense_31_loss: 0.0284\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0831 - dense_34_loss: 0.0170 - dense_22_loss: 0.0172 - dense_25_loss: 0.0172 - dense_28_loss: 0.0171 - dense_31_loss: 0.0171 - val_loss: 0.1145 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0284 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0280\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0837 - dense_34_loss: 0.0174 - dense_22_loss: 0.0176 - dense_25_loss: 0.0177 - dense_28_loss: 0.0174 - dense_31_loss: 0.0175 - val_loss: 0.1122 - val_dense_34_loss: 0.0280 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0281 - val_dense_31_loss: 0.0282\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0808 - dense_34_loss: 0.0164 - dense_22_loss: 0.0169 - dense_25_loss: 0.0167 - dense_28_loss: 0.0165 - dense_31_loss: 0.0166 - val_loss: 0.1139 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0297 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0292\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0826 - dense_34_loss: 0.0172 - dense_22_loss: 0.0177 - dense_25_loss: 0.0176 - dense_28_loss: 0.0174 - dense_31_loss: 0.0176 - val_loss: 0.1127 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0280\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0830 - dense_34_loss: 0.0177 - dense_22_loss: 0.0182 - dense_25_loss: 0.0182 - dense_28_loss: 0.0177 - dense_31_loss: 0.0180 - val_loss: 0.1163 - val_dense_34_loss: 0.0296 - val_dense_22_loss: 0.0296 - val_dense_25_loss: 0.0307 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0306\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0824 - dense_34_loss: 0.0171 - dense_22_loss: 0.0172 - dense_25_loss: 0.0176 - dense_28_loss: 0.0170 - dense_31_loss: 0.0175 - val_loss: 0.1123 - val_dense_34_loss: 0.0284 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0285 - val_dense_31_loss: 0.0282\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0810 - dense_34_loss: 0.0167 - dense_22_loss: 0.0168 - dense_25_loss: 0.0171 - dense_28_loss: 0.0169 - dense_31_loss: 0.0169 - val_loss: 0.1135 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0287 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0284\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0799 - dense_34_loss: 0.0164 - dense_22_loss: 0.0165 - dense_25_loss: 0.0168 - dense_28_loss: 0.0163 - dense_31_loss: 0.0164 - val_loss: 0.1153 - val_dense_34_loss: 0.0294 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0299 - val_dense_28_loss: 0.0297 - val_dense_31_loss: 0.0297\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0807 - dense_34_loss: 0.0164 - dense_22_loss: 0.0164 - dense_25_loss: 0.0171 - dense_28_loss: 0.0165 - dense_31_loss: 0.0165 - val_loss: 0.1138 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0298 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0294 - val_dense_31_loss: 0.0290\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0833 - dense_34_loss: 0.0173 - dense_22_loss: 0.0180 - dense_25_loss: 0.0171 - dense_28_loss: 0.0178 - dense_31_loss: 0.0173 - val_loss: 0.1122 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0286 - val_dense_31_loss: 0.0283\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0824 - dense_34_loss: 0.0168 - dense_22_loss: 0.0174 - dense_25_loss: 0.0167 - dense_28_loss: 0.0170 - dense_31_loss: 0.0169 - val_loss: 0.1156 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0291 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0285\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0836 - dense_34_loss: 0.0171 - dense_22_loss: 0.0179 - dense_25_loss: 0.0171 - dense_28_loss: 0.0174 - dense_31_loss: 0.0175 - val_loss: 0.1177 - val_dense_34_loss: 0.0310 - val_dense_22_loss: 0.0305 - val_dense_25_loss: 0.0301 - val_dense_28_loss: 0.0305 - val_dense_31_loss: 0.0304\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0809 - dense_34_loss: 0.0166 - dense_22_loss: 0.0167 - dense_25_loss: 0.0166 - dense_28_loss: 0.0165 - dense_31_loss: 0.0166 - val_loss: 0.1126 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0284 - val_dense_31_loss: 0.0283\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0820 - dense_34_loss: 0.0172 - dense_22_loss: 0.0177 - dense_25_loss: 0.0175 - dense_28_loss: 0.0175 - dense_31_loss: 0.0174 - val_loss: 0.1123 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0287 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0286\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0828 - dense_34_loss: 0.0168 - dense_22_loss: 0.0175 - dense_25_loss: 0.0170 - dense_28_loss: 0.0169 - dense_31_loss: 0.0169 - val_loss: 0.1184 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0291 - val_dense_31_loss: 0.0284\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0820 - dense_34_loss: 0.0165 - dense_22_loss: 0.0171 - dense_25_loss: 0.0166 - dense_28_loss: 0.0166 - dense_31_loss: 0.0165 - val_loss: 0.1115 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0284 - val_dense_31_loss: 0.0287\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: LearningRateScheduler setting learning rate to 0.0007290000000000002.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0800 - dense_34_loss: 0.0163 - dense_22_loss: 0.0168 - dense_25_loss: 0.0166 - dense_28_loss: 0.0164 - dense_31_loss: 0.0165 - val_loss: 0.1119 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0280 - val_dense_25_loss: 0.0285 - val_dense_28_loss: 0.0284 - val_dense_31_loss: 0.0282\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0803 - dense_34_loss: 0.0164 - dense_22_loss: 0.0168 - dense_25_loss: 0.0169 - dense_28_loss: 0.0166 - dense_31_loss: 0.0167 - val_loss: 0.1132 - val_dense_34_loss: 0.0291 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0291 - val_dense_31_loss: 0.0291\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0805 - dense_34_loss: 0.0163 - dense_22_loss: 0.0166 - dense_25_loss: 0.0165 - dense_28_loss: 0.0162 - dense_31_loss: 0.0164 - val_loss: 0.1134 - val_dense_34_loss: 0.0288 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0291 - val_dense_31_loss: 0.0292\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0831 - dense_34_loss: 0.0170 - dense_22_loss: 0.0174 - dense_25_loss: 0.0173 - dense_28_loss: 0.0171 - dense_31_loss: 0.0173 - val_loss: 0.1134 - val_dense_34_loss: 0.0279 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0283 - val_dense_28_loss: 0.0285 - val_dense_31_loss: 0.0279\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0825 - dense_34_loss: 0.0169 - dense_22_loss: 0.0172 - dense_25_loss: 0.0170 - dense_28_loss: 0.0170 - dense_31_loss: 0.0170 - val_loss: 0.1119 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0284 - val_dense_28_loss: 0.0283 - val_dense_31_loss: 0.0284\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0805 - dense_34_loss: 0.0165 - dense_22_loss: 0.0172 - dense_25_loss: 0.0166 - dense_28_loss: 0.0164 - dense_31_loss: 0.0164 - val_loss: 0.1143 - val_dense_34_loss: 0.0294 - val_dense_22_loss: 0.0297 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0294 - val_dense_31_loss: 0.0296\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0796 - dense_34_loss: 0.0161 - dense_22_loss: 0.0166 - dense_25_loss: 0.0164 - dense_28_loss: 0.0161 - dense_31_loss: 0.0163 - val_loss: 0.1139 - val_dense_34_loss: 0.0281 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0285 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0281\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0819 - dense_34_loss: 0.0168 - dense_22_loss: 0.0172 - dense_25_loss: 0.0172 - dense_28_loss: 0.0168 - dense_31_loss: 0.0168 - val_loss: 0.1136 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0287 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0294\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0795 - dense_34_loss: 0.0161 - dense_22_loss: 0.0166 - dense_25_loss: 0.0166 - dense_28_loss: 0.0164 - dense_31_loss: 0.0165 - val_loss: 0.1121 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0287 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0284 - val_dense_31_loss: 0.0284\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0804 - dense_34_loss: 0.0169 - dense_22_loss: 0.0171 - dense_25_loss: 0.0171 - dense_28_loss: 0.0169 - dense_31_loss: 0.0171 - val_loss: 0.1130 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0290 - val_dense_31_loss: 0.0287\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0798 - dense_34_loss: 0.0162 - dense_22_loss: 0.0165 - dense_25_loss: 0.0165 - dense_28_loss: 0.0162 - dense_31_loss: 0.0165 - val_loss: 0.1129 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0292 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0292\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0789 - dense_34_loss: 0.0157 - dense_22_loss: 0.0161 - dense_25_loss: 0.0161 - dense_28_loss: 0.0158 - dense_31_loss: 0.0161 - val_loss: 0.1126 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0291 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0291 - val_dense_31_loss: 0.0290\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0811 - dense_34_loss: 0.0169 - dense_22_loss: 0.0173 - dense_25_loss: 0.0172 - dense_28_loss: 0.0169 - dense_31_loss: 0.0170 - val_loss: 0.1125 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0285 - val_dense_31_loss: 0.0284\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0797 - dense_34_loss: 0.0165 - dense_22_loss: 0.0168 - dense_25_loss: 0.0167 - dense_28_loss: 0.0166 - dense_31_loss: 0.0166 - val_loss: 0.1122 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0287\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0793 - dense_34_loss: 0.0162 - dense_22_loss: 0.0166 - dense_25_loss: 0.0166 - dense_28_loss: 0.0163 - dense_31_loss: 0.0164 - val_loss: 0.1131 - val_dense_34_loss: 0.0288 - val_dense_22_loss: 0.0292 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0287\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0793 - dense_34_loss: 0.0163 - dense_22_loss: 0.0166 - dense_25_loss: 0.0166 - dense_28_loss: 0.0164 - dense_31_loss: 0.0165 - val_loss: 0.1125 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0286\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0798 - dense_34_loss: 0.0164 - dense_22_loss: 0.0166 - dense_25_loss: 0.0167 - dense_28_loss: 0.0166 - dense_31_loss: 0.0165 - val_loss: 0.1130 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0284\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0801 - dense_34_loss: 0.0164 - dense_22_loss: 0.0167 - dense_25_loss: 0.0166 - dense_28_loss: 0.0167 - dense_31_loss: 0.0165 - val_loss: 0.1121 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0285\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0797 - dense_34_loss: 0.0164 - dense_22_loss: 0.0167 - dense_25_loss: 0.0168 - dense_28_loss: 0.0166 - dense_31_loss: 0.0167 - val_loss: 0.1141 - val_dense_34_loss: 0.0293 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0294 - val_dense_28_loss: 0.0299 - val_dense_31_loss: 0.0297\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 00118: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0797 - dense_34_loss: 0.0166 - dense_22_loss: 0.0169 - dense_25_loss: 0.0168 - dense_28_loss: 0.0167 - dense_31_loss: 0.0168 - val_loss: 0.1122 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0287\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 00119: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0779 - dense_34_loss: 0.0157 - dense_22_loss: 0.0160 - dense_25_loss: 0.0160 - dense_28_loss: 0.0158 - dense_31_loss: 0.0157 - val_loss: 0.1131 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0286\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 00120: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0791 - dense_34_loss: 0.0164 - dense_22_loss: 0.0166 - dense_25_loss: 0.0167 - dense_28_loss: 0.0165 - dense_31_loss: 0.0166 - val_loss: 0.1127 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0289\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 00121: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0791 - dense_34_loss: 0.0163 - dense_22_loss: 0.0166 - dense_25_loss: 0.0167 - dense_28_loss: 0.0164 - dense_31_loss: 0.0166 - val_loss: 0.1143 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0284\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 00122: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0815 - dense_34_loss: 0.0169 - dense_22_loss: 0.0174 - dense_25_loss: 0.0174 - dense_28_loss: 0.0170 - dense_31_loss: 0.0171 - val_loss: 0.1134 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0290 - val_dense_31_loss: 0.0286\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 00123: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0830 - dense_34_loss: 0.0169 - dense_22_loss: 0.0174 - dense_25_loss: 0.0172 - dense_28_loss: 0.0169 - dense_31_loss: 0.0171 - val_loss: 0.1130 - val_dense_34_loss: 0.0292 - val_dense_22_loss: 0.0292 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0291\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 00124: LearningRateScheduler setting learning rate to 0.0006561000000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0812 - dense_34_loss: 0.0171 - dense_22_loss: 0.0175 - dense_25_loss: 0.0173 - dense_28_loss: 0.0173 - dense_31_loss: 0.0173 - val_loss: 0.1125 - val_dense_34_loss: 0.0288 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0291\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 00125: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0796 - dense_34_loss: 0.0164 - dense_22_loss: 0.0167 - dense_25_loss: 0.0166 - dense_28_loss: 0.0165 - dense_31_loss: 0.0166 - val_loss: 0.1124 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0287 - val_dense_31_loss: 0.0285\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 00126: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0804 - dense_34_loss: 0.0170 - dense_22_loss: 0.0173 - dense_25_loss: 0.0175 - dense_28_loss: 0.0170 - dense_31_loss: 0.0172 - val_loss: 0.1136 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0290 - val_dense_31_loss: 0.0285\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 00127: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0844 - dense_34_loss: 0.0171 - dense_22_loss: 0.0178 - dense_25_loss: 0.0172 - dense_28_loss: 0.0175 - dense_31_loss: 0.0172 - val_loss: 0.1139 - val_dense_34_loss: 0.0294 - val_dense_22_loss: 0.0297 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0295 - val_dense_31_loss: 0.0295\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 00128: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0808 - dense_34_loss: 0.0165 - dense_22_loss: 0.0167 - dense_25_loss: 0.0165 - dense_28_loss: 0.0169 - dense_31_loss: 0.0166 - val_loss: 0.1126 - val_dense_34_loss: 0.0284 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/200\n",
      "\n",
      "Epoch 00129: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0789 - dense_34_loss: 0.0162 - dense_22_loss: 0.0165 - dense_25_loss: 0.0164 - dense_28_loss: 0.0163 - dense_31_loss: 0.0164 - val_loss: 0.1114 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0285 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0285\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 00130: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0797 - dense_34_loss: 0.0163 - dense_22_loss: 0.0167 - dense_25_loss: 0.0168 - dense_28_loss: 0.0164 - dense_31_loss: 0.0164 - val_loss: 0.1126 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0285\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 00131: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0783 - dense_34_loss: 0.0159 - dense_22_loss: 0.0163 - dense_25_loss: 0.0163 - dense_28_loss: 0.0160 - dense_31_loss: 0.0159 - val_loss: 0.1125 - val_dense_34_loss: 0.0284 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0285\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 00132: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0812 - dense_34_loss: 0.0167 - dense_22_loss: 0.0173 - dense_25_loss: 0.0170 - dense_28_loss: 0.0169 - dense_31_loss: 0.0169 - val_loss: 0.1137 - val_dense_34_loss: 0.0292 - val_dense_22_loss: 0.0293 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0294 - val_dense_31_loss: 0.0295\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 00133: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0784 - dense_34_loss: 0.0158 - dense_22_loss: 0.0164 - dense_25_loss: 0.0164 - dense_28_loss: 0.0159 - dense_31_loss: 0.0162 - val_loss: 0.1127 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0285\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 00134: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0790 - dense_34_loss: 0.0162 - dense_22_loss: 0.0165 - dense_25_loss: 0.0166 - dense_28_loss: 0.0163 - dense_31_loss: 0.0162 - val_loss: 0.1132 - val_dense_34_loss: 0.0293 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0296 - val_dense_31_loss: 0.0292\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 00135: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0797 - dense_34_loss: 0.0166 - dense_22_loss: 0.0171 - dense_25_loss: 0.0171 - dense_28_loss: 0.0166 - dense_31_loss: 0.0166 - val_loss: 0.1129 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0291 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0285\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 00136: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0795 - dense_34_loss: 0.0164 - dense_22_loss: 0.0167 - dense_25_loss: 0.0166 - dense_28_loss: 0.0164 - dense_31_loss: 0.0166 - val_loss: 0.1122 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0280 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0284\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 00137: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0790 - dense_34_loss: 0.0162 - dense_22_loss: 0.0168 - dense_25_loss: 0.0166 - dense_28_loss: 0.0164 - dense_31_loss: 0.0165 - val_loss: 0.1119 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0281 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0285\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 00138: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0779 - dense_34_loss: 0.0157 - dense_22_loss: 0.0163 - dense_25_loss: 0.0161 - dense_28_loss: 0.0160 - dense_31_loss: 0.0160 - val_loss: 0.1129 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0292\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 00139: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0779 - dense_34_loss: 0.0159 - dense_22_loss: 0.0163 - dense_25_loss: 0.0163 - dense_28_loss: 0.0162 - dense_31_loss: 0.0162 - val_loss: 0.1130 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0294 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0291\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 00140: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0786 - dense_34_loss: 0.0160 - dense_22_loss: 0.0164 - dense_25_loss: 0.0163 - dense_28_loss: 0.0162 - dense_31_loss: 0.0161 - val_loss: 0.1134 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0285\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 00141: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0782 - dense_34_loss: 0.0160 - dense_22_loss: 0.0164 - dense_25_loss: 0.0164 - dense_28_loss: 0.0160 - dense_31_loss: 0.0161 - val_loss: 0.1127 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0289 - val_dense_31_loss: 0.0284\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 00142: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0809 - dense_34_loss: 0.0167 - dense_22_loss: 0.0171 - dense_25_loss: 0.0171 - dense_28_loss: 0.0166 - dense_31_loss: 0.0167 - val_loss: 0.1133 - val_dense_34_loss: 0.0291 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0294 - val_dense_31_loss: 0.0291\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 00143: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0791 - dense_34_loss: 0.0163 - dense_22_loss: 0.0168 - dense_25_loss: 0.0168 - dense_28_loss: 0.0163 - dense_31_loss: 0.0164 - val_loss: 0.1147 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0290 - val_dense_31_loss: 0.0286\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 00144: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0807 - dense_34_loss: 0.0168 - dense_22_loss: 0.0169 - dense_25_loss: 0.0172 - dense_28_loss: 0.0168 - dense_31_loss: 0.0166 - val_loss: 0.1139 - val_dense_34_loss: 0.0292 - val_dense_22_loss: 0.0290 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0296 - val_dense_31_loss: 0.0293\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 00145: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0779 - dense_34_loss: 0.0156 - dense_22_loss: 0.0161 - dense_25_loss: 0.0161 - dense_28_loss: 0.0157 - dense_31_loss: 0.0157 - val_loss: 0.1132 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0291 - val_dense_31_loss: 0.0291\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 00146: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0794 - dense_34_loss: 0.0164 - dense_22_loss: 0.0170 - dense_25_loss: 0.0167 - dense_28_loss: 0.0167 - dense_31_loss: 0.0165 - val_loss: 0.1140 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0286 - val_dense_28_loss: 0.0290 - val_dense_31_loss: 0.0284\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 00147: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0799 - dense_34_loss: 0.0165 - dense_22_loss: 0.0173 - dense_25_loss: 0.0167 - dense_28_loss: 0.0167 - dense_31_loss: 0.0165 - val_loss: 0.1122 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0290 - val_dense_31_loss: 0.0286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "\n",
      "Epoch 00148: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0790 - dense_34_loss: 0.0161 - dense_22_loss: 0.0169 - dense_25_loss: 0.0164 - dense_28_loss: 0.0163 - dense_31_loss: 0.0161 - val_loss: 0.1158 - val_dense_34_loss: 0.0299 - val_dense_22_loss: 0.0293 - val_dense_25_loss: 0.0297 - val_dense_28_loss: 0.0300 - val_dense_31_loss: 0.0304\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 00149: LearningRateScheduler setting learning rate to 0.00059049.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0828 - dense_34_loss: 0.0173 - dense_22_loss: 0.0174 - dense_25_loss: 0.0172 - dense_28_loss: 0.0174 - dense_31_loss: 0.0172 - val_loss: 0.1129 - val_dense_34_loss: 0.0282 - val_dense_22_loss: 0.0280 - val_dense_25_loss: 0.0287 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0281\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 00150: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0785 - dense_34_loss: 0.0161 - dense_22_loss: 0.0163 - dense_25_loss: 0.0163 - dense_28_loss: 0.0163 - dense_31_loss: 0.0163 - val_loss: 0.1124 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0290 - val_dense_31_loss: 0.0286\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 00151: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0786 - dense_34_loss: 0.0160 - dense_22_loss: 0.0165 - dense_25_loss: 0.0165 - dense_28_loss: 0.0162 - dense_31_loss: 0.0163 - val_loss: 0.1133 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0293\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 00152: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0777 - dense_34_loss: 0.0157 - dense_22_loss: 0.0160 - dense_25_loss: 0.0162 - dense_28_loss: 0.0158 - dense_31_loss: 0.0160 - val_loss: 0.1128 - val_dense_34_loss: 0.0283 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0288 - val_dense_31_loss: 0.0283\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 00153: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0776 - dense_34_loss: 0.0158 - dense_22_loss: 0.0163 - dense_25_loss: 0.0164 - dense_28_loss: 0.0159 - dense_31_loss: 0.0160 - val_loss: 0.1127 - val_dense_34_loss: 0.0288 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0291 - val_dense_31_loss: 0.0287\n",
      "Epoch 154/200\n",
      "\n",
      "Epoch 00154: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0778 - dense_34_loss: 0.0156 - dense_22_loss: 0.0160 - dense_25_loss: 0.0160 - dense_28_loss: 0.0158 - dense_31_loss: 0.0159 - val_loss: 0.1137 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0292 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0292\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 00155: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0787 - dense_34_loss: 0.0163 - dense_22_loss: 0.0167 - dense_25_loss: 0.0167 - dense_28_loss: 0.0166 - dense_31_loss: 0.0165 - val_loss: 0.1127 - val_dense_34_loss: 0.0288 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0289\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 00156: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0783 - dense_34_loss: 0.0161 - dense_22_loss: 0.0165 - dense_25_loss: 0.0166 - dense_28_loss: 0.0162 - dense_31_loss: 0.0164 - val_loss: 0.1135 - val_dense_34_loss: 0.0288 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0291 - val_dense_31_loss: 0.0286\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 00157: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0780 - dense_34_loss: 0.0158 - dense_22_loss: 0.0162 - dense_25_loss: 0.0163 - dense_28_loss: 0.0160 - dense_31_loss: 0.0162 - val_loss: 0.1143 - val_dense_34_loss: 0.0293 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0297 - val_dense_28_loss: 0.0297 - val_dense_31_loss: 0.0295\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 00158: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0774 - dense_34_loss: 0.0156 - dense_22_loss: 0.0162 - dense_25_loss: 0.0161 - dense_28_loss: 0.0158 - dense_31_loss: 0.0158 - val_loss: 0.1133 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0294 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0289\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 00159: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0768 - dense_34_loss: 0.0154 - dense_22_loss: 0.0160 - dense_25_loss: 0.0159 - dense_28_loss: 0.0156 - dense_31_loss: 0.0156 - val_loss: 0.1129 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0281 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0286\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 00160: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0792 - dense_34_loss: 0.0166 - dense_22_loss: 0.0171 - dense_25_loss: 0.0171 - dense_28_loss: 0.0168 - dense_31_loss: 0.0170 - val_loss: 0.1134 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0287\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 00161: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0784 - dense_34_loss: 0.0160 - dense_22_loss: 0.0166 - dense_25_loss: 0.0166 - dense_28_loss: 0.0162 - dense_31_loss: 0.0162 - val_loss: 0.1131 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0287\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 00162: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0784 - dense_34_loss: 0.0162 - dense_22_loss: 0.0166 - dense_25_loss: 0.0166 - dense_28_loss: 0.0165 - dense_31_loss: 0.0165 - val_loss: 0.1133 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0287 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0294 - val_dense_31_loss: 0.0290\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 00163: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0773 - dense_34_loss: 0.0156 - dense_22_loss: 0.0161 - dense_25_loss: 0.0161 - dense_28_loss: 0.0157 - dense_31_loss: 0.0158 - val_loss: 0.1136 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0294 - val_dense_31_loss: 0.0289\n",
      "Epoch 164/200\n",
      "\n",
      "Epoch 00164: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0770 - dense_34_loss: 0.0156 - dense_22_loss: 0.0162 - dense_25_loss: 0.0161 - dense_28_loss: 0.0159 - dense_31_loss: 0.0160 - val_loss: 0.1132 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0287\n",
      "Epoch 165/200\n",
      "\n",
      "Epoch 00165: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0777 - dense_34_loss: 0.0155 - dense_22_loss: 0.0163 - dense_25_loss: 0.0161 - dense_28_loss: 0.0158 - dense_31_loss: 0.0158 - val_loss: 0.1148 - val_dense_34_loss: 0.0296 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0300 - val_dense_28_loss: 0.0299 - val_dense_31_loss: 0.0294\n",
      "Epoch 166/200\n",
      "\n",
      "Epoch 00166: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0802 - dense_34_loss: 0.0169 - dense_22_loss: 0.0171 - dense_25_loss: 0.0172 - dense_28_loss: 0.0171 - dense_31_loss: 0.0169 - val_loss: 0.1137 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0280 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/200\n",
      "\n",
      "Epoch 00167: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0777 - dense_34_loss: 0.0158 - dense_22_loss: 0.0163 - dense_25_loss: 0.0164 - dense_28_loss: 0.0160 - dense_31_loss: 0.0160 - val_loss: 0.1134 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0289\n",
      "Epoch 168/200\n",
      "\n",
      "Epoch 00168: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0774 - dense_34_loss: 0.0154 - dense_22_loss: 0.0161 - dense_25_loss: 0.0163 - dense_28_loss: 0.0156 - dense_31_loss: 0.0157 - val_loss: 0.1141 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0295 - val_dense_25_loss: 0.0293 - val_dense_28_loss: 0.0295 - val_dense_31_loss: 0.0291\n",
      "Epoch 169/200\n",
      "\n",
      "Epoch 00169: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0806 - dense_34_loss: 0.0166 - dense_22_loss: 0.0173 - dense_25_loss: 0.0170 - dense_28_loss: 0.0168 - dense_31_loss: 0.0167 - val_loss: 0.1130 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0288 - val_dense_28_loss: 0.0291 - val_dense_31_loss: 0.0286\n",
      "Epoch 170/200\n",
      "\n",
      "Epoch 00170: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0783 - dense_34_loss: 0.0158 - dense_22_loss: 0.0163 - dense_25_loss: 0.0163 - dense_28_loss: 0.0161 - dense_31_loss: 0.0162 - val_loss: 0.1139 - val_dense_34_loss: 0.0286 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0294 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0287\n",
      "Epoch 171/200\n",
      "\n",
      "Epoch 00171: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0793 - dense_34_loss: 0.0164 - dense_22_loss: 0.0162 - dense_25_loss: 0.0162 - dense_28_loss: 0.0167 - dense_31_loss: 0.0165 - val_loss: 0.1144 - val_dense_34_loss: 0.0294 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0300 - val_dense_31_loss: 0.0293\n",
      "Epoch 172/200\n",
      "\n",
      "Epoch 00172: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0785 - dense_34_loss: 0.0163 - dense_22_loss: 0.0167 - dense_25_loss: 0.0165 - dense_28_loss: 0.0164 - dense_31_loss: 0.0164 - val_loss: 0.1140 - val_dense_34_loss: 0.0291 - val_dense_22_loss: 0.0288 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0294 - val_dense_31_loss: 0.0297\n",
      "Epoch 173/200\n",
      "\n",
      "Epoch 00173: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0817 - dense_34_loss: 0.0170 - dense_22_loss: 0.0171 - dense_25_loss: 0.0169 - dense_28_loss: 0.0168 - dense_31_loss: 0.0173 - val_loss: 0.1158 - val_dense_34_loss: 0.0285 - val_dense_22_loss: 0.0287 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0285\n",
      "Epoch 174/200\n",
      "\n",
      "Epoch 00174: LearningRateScheduler setting learning rate to 0.000531441.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0775 - dense_34_loss: 0.0155 - dense_22_loss: 0.0158 - dense_25_loss: 0.0159 - dense_28_loss: 0.0155 - dense_31_loss: 0.0158 - val_loss: 0.1129 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0294 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0291\n",
      "Epoch 175/200\n",
      "\n",
      "Epoch 00175: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0761 - dense_34_loss: 0.0152 - dense_22_loss: 0.0156 - dense_25_loss: 0.0156 - dense_28_loss: 0.0154 - dense_31_loss: 0.0155 - val_loss: 0.1129 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0281 - val_dense_25_loss: 0.0290 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0287\n",
      "Epoch 176/200\n",
      "\n",
      "Epoch 00176: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0763 - dense_34_loss: 0.0152 - dense_22_loss: 0.0158 - dense_25_loss: 0.0159 - dense_28_loss: 0.0156 - dense_31_loss: 0.0156 - val_loss: 0.1156 - val_dense_34_loss: 0.0295 - val_dense_22_loss: 0.0297 - val_dense_25_loss: 0.0297 - val_dense_28_loss: 0.0297 - val_dense_31_loss: 0.0299\n",
      "Epoch 177/200\n",
      "\n",
      "Epoch 00177: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0795 - dense_34_loss: 0.0158 - dense_22_loss: 0.0167 - dense_25_loss: 0.0162 - dense_28_loss: 0.0158 - dense_31_loss: 0.0162 - val_loss: 0.1139 - val_dense_34_loss: 0.0284 - val_dense_22_loss: 0.0280 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0286\n",
      "Epoch 178/200\n",
      "\n",
      "Epoch 00178: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0783 - dense_34_loss: 0.0160 - dense_22_loss: 0.0166 - dense_25_loss: 0.0165 - dense_28_loss: 0.0163 - dense_31_loss: 0.0162 - val_loss: 0.1137 - val_dense_34_loss: 0.0288 - val_dense_22_loss: 0.0280 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0289\n",
      "Epoch 179/200\n",
      "\n",
      "Epoch 00179: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0766 - dense_34_loss: 0.0155 - dense_22_loss: 0.0160 - dense_25_loss: 0.0159 - dense_28_loss: 0.0158 - dense_31_loss: 0.0156 - val_loss: 0.1133 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0281 - val_dense_25_loss: 0.0294 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0289\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 00180: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0764 - dense_34_loss: 0.0154 - dense_22_loss: 0.0159 - dense_25_loss: 0.0159 - dense_28_loss: 0.0156 - dense_31_loss: 0.0155 - val_loss: 0.1146 - val_dense_34_loss: 0.0293 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0297 - val_dense_31_loss: 0.0295\n",
      "Epoch 181/200\n",
      "\n",
      "Epoch 00181: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0794 - dense_34_loss: 0.0161 - dense_22_loss: 0.0169 - dense_25_loss: 0.0165 - dense_28_loss: 0.0164 - dense_31_loss: 0.0163 - val_loss: 0.1145 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0289 - val_dense_25_loss: 0.0291 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0289\n",
      "Epoch 182/200\n",
      "\n",
      "Epoch 00182: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0783 - dense_34_loss: 0.0158 - dense_22_loss: 0.0168 - dense_25_loss: 0.0164 - dense_28_loss: 0.0160 - dense_31_loss: 0.0160 - val_loss: 0.1138 - val_dense_34_loss: 0.0292 - val_dense_22_loss: 0.0281 - val_dense_25_loss: 0.0297 - val_dense_28_loss: 0.0296 - val_dense_31_loss: 0.0297\n",
      "Epoch 183/200\n",
      "\n",
      "Epoch 00183: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0789 - dense_34_loss: 0.0162 - dense_22_loss: 0.0168 - dense_25_loss: 0.0168 - dense_28_loss: 0.0163 - dense_31_loss: 0.0165 - val_loss: 0.1133 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0287\n",
      "Epoch 184/200\n",
      "\n",
      "Epoch 00184: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0786 - dense_34_loss: 0.0161 - dense_22_loss: 0.0165 - dense_25_loss: 0.0166 - dense_28_loss: 0.0165 - dense_31_loss: 0.0164 - val_loss: 0.1137 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0280 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0294 - val_dense_31_loss: 0.0287\n",
      "Epoch 185/200\n",
      "\n",
      "Epoch 00185: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0765 - dense_34_loss: 0.0153 - dense_22_loss: 0.0161 - dense_25_loss: 0.0159 - dense_28_loss: 0.0154 - dense_31_loss: 0.0155 - val_loss: 0.1144 - val_dense_34_loss: 0.0293 - val_dense_22_loss: 0.0295 - val_dense_25_loss: 0.0302 - val_dense_28_loss: 0.0297 - val_dense_31_loss: 0.0295\n",
      "Epoch 186/200\n",
      "\n",
      "Epoch 00186: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0772 - dense_34_loss: 0.0157 - dense_22_loss: 0.0163 - dense_25_loss: 0.0164 - dense_28_loss: 0.0161 - dense_31_loss: 0.0160 - val_loss: 0.1141 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0297 - val_dense_28_loss: 0.0299 - val_dense_31_loss: 0.0294\n",
      "Epoch 187/200\n",
      "\n",
      "Epoch 00187: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0762 - dense_34_loss: 0.0151 - dense_22_loss: 0.0155 - dense_25_loss: 0.0157 - dense_28_loss: 0.0155 - dense_31_loss: 0.0155 - val_loss: 0.1138 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0295 - val_dense_31_loss: 0.0290\n",
      "Epoch 188/200\n",
      "\n",
      "Epoch 00188: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0764 - dense_34_loss: 0.0155 - dense_22_loss: 0.0158 - dense_25_loss: 0.0159 - dense_28_loss: 0.0157 - dense_31_loss: 0.0155 - val_loss: 0.1137 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0295 - val_dense_31_loss: 0.0293\n",
      "Epoch 189/200\n",
      "\n",
      "Epoch 00189: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0779 - dense_34_loss: 0.0161 - dense_22_loss: 0.0166 - dense_25_loss: 0.0169 - dense_28_loss: 0.0163 - dense_31_loss: 0.0165 - val_loss: 0.1139 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0280 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0295 - val_dense_31_loss: 0.0289\n",
      "Epoch 190/200\n",
      "\n",
      "Epoch 00190: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0766 - dense_34_loss: 0.0154 - dense_22_loss: 0.0160 - dense_25_loss: 0.0160 - dense_28_loss: 0.0158 - dense_31_loss: 0.0156 - val_loss: 0.1139 - val_dense_34_loss: 0.0291 - val_dense_22_loss: 0.0284 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0299 - val_dense_31_loss: 0.0291\n",
      "Epoch 191/200\n",
      "\n",
      "Epoch 00191: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0757 - dense_34_loss: 0.0151 - dense_22_loss: 0.0158 - dense_25_loss: 0.0158 - dense_28_loss: 0.0154 - dense_31_loss: 0.0152 - val_loss: 0.1139 - val_dense_34_loss: 0.0291 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0296 - val_dense_31_loss: 0.0292\n",
      "Epoch 192/200\n",
      "\n",
      "Epoch 00192: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0772 - dense_34_loss: 0.0159 - dense_22_loss: 0.0166 - dense_25_loss: 0.0166 - dense_28_loss: 0.0162 - dense_31_loss: 0.0162 - val_loss: 0.1136 - val_dense_34_loss: 0.0288 - val_dense_22_loss: 0.0282 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0296 - val_dense_31_loss: 0.0291\n",
      "Epoch 193/200\n",
      "\n",
      "Epoch 00193: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0767 - dense_34_loss: 0.0156 - dense_22_loss: 0.0159 - dense_25_loss: 0.0164 - dense_28_loss: 0.0158 - dense_31_loss: 0.0158 - val_loss: 0.1137 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0278 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0290\n",
      "Epoch 194/200\n",
      "\n",
      "Epoch 00194: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0761 - dense_34_loss: 0.0151 - dense_22_loss: 0.0158 - dense_25_loss: 0.0159 - dense_28_loss: 0.0156 - dense_31_loss: 0.0153 - val_loss: 0.1141 - val_dense_34_loss: 0.0290 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0299 - val_dense_31_loss: 0.0292\n",
      "Epoch 195/200\n",
      "\n",
      "Epoch 00195: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0765 - dense_34_loss: 0.0152 - dense_22_loss: 0.0157 - dense_25_loss: 0.0159 - dense_28_loss: 0.0156 - dense_31_loss: 0.0153 - val_loss: 0.1137 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0279 - val_dense_25_loss: 0.0289 - val_dense_28_loss: 0.0292 - val_dense_31_loss: 0.0289\n",
      "Epoch 196/200\n",
      "\n",
      "Epoch 00196: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0778 - dense_34_loss: 0.0159 - dense_22_loss: 0.0163 - dense_25_loss: 0.0162 - dense_28_loss: 0.0162 - dense_31_loss: 0.0160 - val_loss: 0.1142 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0286 - val_dense_25_loss: 0.0297 - val_dense_28_loss: 0.0294 - val_dense_31_loss: 0.0291\n",
      "Epoch 197/200\n",
      "\n",
      "Epoch 00197: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0764 - dense_34_loss: 0.0154 - dense_22_loss: 0.0157 - dense_25_loss: 0.0158 - dense_28_loss: 0.0156 - dense_31_loss: 0.0157 - val_loss: 0.1143 - val_dense_34_loss: 0.0287 - val_dense_22_loss: 0.0294 - val_dense_25_loss: 0.0295 - val_dense_28_loss: 0.0294 - val_dense_31_loss: 0.0288\n",
      "Epoch 198/200\n",
      "\n",
      "Epoch 00198: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0764 - dense_34_loss: 0.0156 - dense_22_loss: 0.0159 - dense_25_loss: 0.0162 - dense_28_loss: 0.0160 - dense_31_loss: 0.0159 - val_loss: 0.1139 - val_dense_34_loss: 0.0291 - val_dense_22_loss: 0.0280 - val_dense_25_loss: 0.0294 - val_dense_28_loss: 0.0297 - val_dense_31_loss: 0.0290\n",
      "Epoch 199/200\n",
      "\n",
      "Epoch 00199: LearningRateScheduler setting learning rate to 0.0004782969000000001.\n",
      "305/305 [==============================] - 1s 4ms/step - loss: 0.0758 - dense_34_loss: 0.0152 - dense_22_loss: 0.0158 - dense_25_loss: 0.0158 - dense_28_loss: 0.0156 - dense_31_loss: 0.0156 - val_loss: 0.1141 - val_dense_34_loss: 0.0289 - val_dense_22_loss: 0.0283 - val_dense_25_loss: 0.0292 - val_dense_28_loss: 0.0293 - val_dense_31_loss: 0.0290\n",
      "Epoch 200/200\n",
      "\n",
      "Epoch 00200: LearningRateScheduler setting learning rate to 0.0004304672100000001.\n",
      "305/305 [==============================] - 1s 3ms/step - loss: 0.0762 - dense_34_loss: 0.0155 - dense_22_loss: 0.0159 - dense_25_loss: 0.0160 - dense_28_loss: 0.0159 - dense_31_loss: 0.0157 - val_loss: 0.1141 - val_dense_34_loss: 0.0291 - val_dense_22_loss: 0.0285 - val_dense_25_loss: 0.0296 - val_dense_28_loss: 0.0299 - val_dense_31_loss: 0.0295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26fdac30588>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.compile(loss=custom_loss_nn, optimizer='adam')\n",
    "nn.fit(ttr_X/113, [tr_y/113]*5, # [tr_y/113]*5\n",
    "      epochs=200,\n",
    "       validation_split=0.2,\n",
    "#        batch_size=32,\n",
    "       shuffle=True,\n",
    "      callbacks=[es, lrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4342179938250385"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_loss(val_y, np.clip((nn.predict(np.expand_dims(val_X/113, -1)))*113, 0, 113))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_y.flatten(), label='true')\n",
    "plt.plot(np.clip((nn.predict(np.expand_dims(val_X/113, -1))).flatten()*113, 0, 113), label='pred')\n",
    "plt.title('wavenet 24 validation result')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22744075931453203"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_loss(val_y, np.clip((nn.predict(np.expand_dims(val_X/113, -1)))[0]*113, 0, 113))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_y.flatten(), label='true')\n",
    "plt.plot(np.clip((nn.predict(np.expand_dims(val_X/113, -1)))[0].flatten()*113, 0, 113), label='pred')\n",
    "plt.title('CNN 24 validation result')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
