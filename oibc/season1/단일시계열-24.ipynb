{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "from scipy import stats\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from scipy.stats import norm, kurtosis\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lgb(x, y, init_points=15, n_iter=0, param=True, verbose=2, cv=None, criterion='mse'):\n",
    "    if cv is None:\n",
    "        print('eval using hold-out')\n",
    "        train_X, test_X, train_y, test_y = train_test_split(x, y, test_size=0.3, random_state=SEED, shuffle=True)\n",
    "    else:\n",
    "        splitter = KFold(cv)\n",
    "        print('eval using cv')\n",
    "    def LGB_bayesian(\n",
    "        # learning_rate,\n",
    "        num_leaves, \n",
    "        bagging_fraction,\n",
    "        feature_fraction,\n",
    "        min_child_weight, \n",
    "        min_data_in_leaf,\n",
    "        max_depth,\n",
    "        reg_alpha,\n",
    "        reg_lambda,\n",
    "        n_estimators,\n",
    "        cv=cv\n",
    "         ):\n",
    "        # LightGBM expects next three parameters need to be integer. \n",
    "        num_leaves = int(num_leaves)\n",
    "        min_data_in_leaf = int(min_data_in_leaf)\n",
    "        max_depth = int(max_depth)\n",
    "\n",
    "        assert type(num_leaves) == int\n",
    "        assert type(min_data_in_leaf) == int\n",
    "        assert type(max_depth) == int\n",
    "\n",
    "\n",
    "        params = {\n",
    "                  'num_leaves': num_leaves, \n",
    "                  'min_data_in_leaf': min_data_in_leaf,\n",
    "                  'min_child_weight': min_child_weight,\n",
    "                  'bagging_fraction' : bagging_fraction,\n",
    "                  'feature_fraction' : feature_fraction,\n",
    "                  'learning_rate' : 0.05,\n",
    "                  'max_depth': max_depth,\n",
    "                  'reg_alpha': reg_alpha,\n",
    "                  'reg_lambda': reg_lambda,\n",
    "                  'objective': 'regression',\n",
    "                  'save_binary': True,\n",
    "                  'seed': SEED,\n",
    "                  'feature_fraction_seed': SEED,\n",
    "                  'bagging_seed': SEED,\n",
    "                  'drop_seed': SEED,\n",
    "                  'data_random_seed': SEED,\n",
    "                  'boosting': 'gbdt', ## some get better result using 'dart'\n",
    "                  'verbose': 1,\n",
    "                  'metric':'mae',\n",
    "                  'n_estimators': int(n_estimators),\n",
    "                  'n_jobs': -1,\n",
    "        }    \n",
    "\n",
    "        ## set clf options\n",
    "        if cv is not None:\n",
    "            score = 0\n",
    "            for tr_idx, val_idx in splitter.split(x, y):\n",
    "                tr_X, tr_y = x[tr_idx], y[tr_idx]\n",
    "                val_X, val_y = x[val_idx], y[val_idx]\n",
    "                clf = MultiOutputRegressor(lgb.LGBMRegressor(**params), n_jobs=-1)\n",
    "                clf = clf.fit(tr_X, tr_y)\n",
    "                pred = clf.predict(val_X)\n",
    "                if criterion == 'mse':\n",
    "                    score += mean_squared_error(val_y, pred)\n",
    "                elif criterion == 'mae':\n",
    "                    score += mean_absolute_error(val_y, pred)\n",
    "\n",
    "            return -score/cv\n",
    "        else:\n",
    "            raise('eo')\n",
    "            clf = lgb.LGBMRegressor(**params).fit(train_X, train_y)\n",
    "            pred =  clf.predict(test_X)\n",
    "#             pred = np.where(pred<0, 0, pred)\n",
    "            score = custom_loss(test_y, pred)\n",
    "            \n",
    "            return -score\n",
    "\n",
    "    \n",
    "    optimizer = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42, verbose=verbose)\n",
    "    init_points = init_points\n",
    "    n_iter = n_iter\n",
    "\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "    \n",
    "    param_lgb = {\n",
    "        'min_data_in_leaf': int(optimizer.max['params']['min_data_in_leaf']), \n",
    "        'num_leaves': int(optimizer.max['params']['num_leaves']), \n",
    "        'learning_rate': 0.05,\n",
    "        'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "        'bagging_fraction': optimizer.max['params']['bagging_fraction'], \n",
    "        'feature_fraction': optimizer.max['params']['feature_fraction'],\n",
    "        'reg_lambda': optimizer.max['params']['reg_lambda'],\n",
    "        'reg_alpha': optimizer.max['params']['reg_alpha'],\n",
    "        'max_depth': int(optimizer.max['params']['max_depth']), \n",
    "        'objective': 'regression',\n",
    "        'save_binary': True,\n",
    "        'seed': SEED,\n",
    "        'feature_fraction_seed': SEED,\n",
    "        'bagging_seed': SEED,\n",
    "        'drop_seed': SEED,\n",
    "        'data_random_seed': SEED,\n",
    "        'boosting_type': 'dart',  # also consider 'dart'\n",
    "        'verbose': 1,\n",
    "        'metric':'mae',\n",
    "        'n_estimators': int(optimizer.max['params']['n_estimators']),\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    params = param_lgb.copy()\n",
    "    \n",
    "    lgb_reg = MultiOutputRegressor(lgb.LGBMRegressor(**params), n_jobs=-1)\n",
    "    lgb_reg.fit(x, y)\n",
    "    \n",
    "    if param:\n",
    "        return lgb_reg, params\n",
    "    else:\n",
    "        return lgb_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.executing_eagerly()\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import layers, models, optimizers, initializers, regularizers, constraints, losses\n",
    "from keras.models import Sequential, Model, load_model, Input\n",
    "from keras.layers import (Dense, Concatenate, BatchNormalization, Activation, Add,\n",
    "                          concatenate, Dropout, AlphaDropout, Reshape, Layer, Multiply, Lambda)\n",
    "from keras.layers import (Dense, Concatenate, BatchNormalization, Activation, Add,\n",
    "                          concatenate, Dropout, AlphaDropout, Reshape, Layer, Multiply)\n",
    "from keras.layers import (Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling1D, \n",
    "                          GlobalMaxPooling1D, SeparableConv1D, MaxPool1D, AveragePooling1D, \n",
    "                          SeparableConv1D, AtrousConvolution1D)\n",
    "from keras.layers import (Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, \n",
    "                          GlobalMaxPooling2D, SeparableConv2D, MaxPool2D, AveragePooling2D, \n",
    "                          SeparableConv2D, AtrousConvolution2D)\n",
    "from keras.layers import LSTM, GRU, Bidirectional\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras.losses import binary_crossentropy,  kullback_leibler_divergence, mean_squared_error\n",
    "\n",
    "def mish(x):\n",
    "    return x*K.tanh(K.softplus(x))\n",
    "\n",
    "def decay(epoch, steps=100):\n",
    "    initial_lrate = 1e-3\n",
    "    drop = 0.9\n",
    "    epochs_drop = 25\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    lrate = max(lrate, 5e-5)\n",
    "    return lrate\n",
    "\n",
    "es = EarlyStopping(patience=25, restore_best_weights=True, monitor='val_total_loss')\n",
    "lrs = LearningRateScheduler(decay, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grap_year(x):\n",
    "    y, _, _ = x.split('-')\n",
    "    return int(y)\n",
    "\n",
    "def grap_date(x):\n",
    "    y = x[5:10].replace('-','')\n",
    "    return y\n",
    "\n",
    "def grap_hour(x):\n",
    "    return x[11:13]\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    res = 0\n",
    "    cnt = 1\n",
    "    for i in range(0, len(y_true), 24):\n",
    "        yt = y_true[i:i+24]\n",
    "        yp = y_pred[i:i+24]\n",
    "        a = np.abs(yt-yp)\n",
    "        c = 113\n",
    "        S = np.sum(yt)\n",
    "        res += np.sum(a*yt/(S*c))\n",
    "        cnt += 1\n",
    "    return res/cnt\n",
    "\n",
    "def custom_loss_nn(y_true, y_pred):\n",
    "    res = K.abs(y_true*113-y_pred*113)*y_true/K.sum(y_true)\n",
    "    res = K.mean(res, axis=1)\n",
    "\n",
    "    return K.mean(res)\n",
    "\n",
    "class Lookahead(keras.optimizers.Optimizer):\n",
    "    def __init__(self, optimizer, sync_period=5, slow_step=0.5, **kwargs):\n",
    "        super(Lookahead, self).__init__(**kwargs)\n",
    "        self.optimizer = keras.optimizers.get(optimizer)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.sync_period = K.variable(sync_period, dtype='int64', name='sync_period')\n",
    "            self.slow_step = K.variable(slow_step, name='slow_step')\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.optimizer.lr\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, lr):\n",
    "        self.optimizer.lr = lr\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self.optimizer.learning_rate\n",
    "\n",
    "    @learning_rate.setter\n",
    "    def learning_rate(self, learning_rate):\n",
    "        self.optimizer.learning_rate = learning_rate\n",
    "\n",
    "    @property\n",
    "    def iterations(self):\n",
    "        return self.optimizer.iterations\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        sync_cond = K.equal((self.iterations + 1) // self.sync_period * self.sync_period, (self.iterations + 1))\n",
    "        slow_params = {p.name: K.variable(K.get_value(p), name='sp_{}'.format(i)) for i, p in enumerate(params)}\n",
    "        update_names = ['update', 'update_add', 'update_sub']\n",
    "        original_updates = [getattr(K, name) for name in update_names]\n",
    "        setattr(K, 'update', lambda x, new_x: ('update', x, new_x))\n",
    "        setattr(K, 'update_add', lambda x, new_x: ('update_add', x, new_x))\n",
    "        setattr(K, 'update_sub', lambda x, new_x: ('update_sub', x, new_x))\n",
    "        self.updates = self.optimizer.get_updates(loss, params)\n",
    "        for name, original_update in zip(update_names, original_updates):\n",
    "            setattr(K, name, original_update)\n",
    "        slow_updates = []\n",
    "        for i, update in enumerate(self.updates):\n",
    "            if isinstance(update, tuple):\n",
    "                name, x, new_x, adjusted = update + (update[-1],)\n",
    "                update_func = getattr(K, name)\n",
    "                if name == 'update_add':\n",
    "                    adjusted = x + new_x\n",
    "                if name == 'update_sub':\n",
    "                    adjusted = x - new_x\n",
    "                if x.name not in slow_params:\n",
    "                    self.updates[i] = update_func(x, new_x)\n",
    "                else:\n",
    "                    slow_param = slow_params[x.name]\n",
    "                    slow_param_t = slow_param + self.slow_step * (adjusted - slow_param)\n",
    "                    slow_updates.append(K.update(slow_param, K.switch(\n",
    "                        sync_cond,\n",
    "                        slow_param_t,\n",
    "                        slow_param,\n",
    "                    )))\n",
    "                    self.updates[i] = K.update(x, K.switch(\n",
    "                        sync_cond,\n",
    "                        slow_param_t,\n",
    "                        adjusted,\n",
    "                    ))\n",
    "        slow_params = list(slow_params.values())\n",
    "        self.updates += slow_updates\n",
    "        self.weights = self.optimizer.weights + slow_params\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'optimizer': keras.optimizers.serialize(self.optimizer),\n",
    "            'sync_period': int(K.get_value(self.sync_period)),\n",
    "            'slow_step': float(K.get_value(self.slow_step)),\n",
    "        }\n",
    "        base_config = super(Lookahead, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        optimizer = keras.optimizers.deserialize(config.pop('optimizer'))\n",
    "        return cls(optimizer, **config)\n",
    "    \n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('./data/SolarPV_Elec_Problem.csv', header=None)\n",
    "data = pd.read_csv('./data/features3.csv')#.iloc[0:-2,:] # 자료가 21시까지\n",
    "# sub = pd.read_csv('./data/제출양식_복원값.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target['year'] = target[0].map(grap_year)\n",
    "target['md'] = target[0].map(grap_date)\n",
    "target['hour'] = target[0].map(grap_hour)\n",
    "target = target.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.fillna(-10)\n",
    "y = target.groupby(['year', 'md', 'hour']).sum()[1].values[:8760]\n",
    "\n",
    "# y1 = y[:5112] # 0701~0130\n",
    "# y2 = y[5160:6552] # 0201~0330\n",
    "# y3 = y[6600:8016] # 0401~0530\n",
    "# y4 = y[8064:] # 0530~0630\n",
    "\n",
    "# val_y1 = y[5112:5136]\n",
    "# val_y2 = y[6552:6576]\n",
    "# val_y3 = y[8016:8040]\n",
    "\n",
    "# val_y = np.hstack([val_y1, val_y2, val_y3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zero_col = ['rain', 'sunlight', 'sunpower', 'snow', 'windpower']\n",
    "ukn_num = ['cloud2', 'cloud1']\n",
    "ukn_cat = ['cloud3']\n",
    "ctd = ['cloudheight', 'time']\n",
    "\n",
    "categorical = [\n",
    "    'winddirection', 'cloud1', 'cloud2', 'cloud3'\n",
    "]\n",
    "\n",
    "numerical = [\n",
    "    'temperatur', 'rain', 'windpower', 'humidity', 'steampower', 'dew', 'pressure1', 'pressure2', 'sunpower', 'snow',\n",
    "    'sijung', 'gourndtemperatur'\n",
    "]\n",
    "\n",
    "data[zero_col] = data[zero_col].fillna(0)\n",
    "data[ukn_cat] = data[ukn_cat].fillna('unknown')\n",
    "# data[categorical] = data[categorical].fillna('unknown')\n",
    "data[ukn_num] = data[ukn_num].fillna(-1)\n",
    "data[numerical] = data[numerical].fillna(0)\n",
    "# data = data.drop(ctd, axis=1)\n",
    "\n",
    "\n",
    "for c in categorical:\n",
    "    lbe = LabelEncoder()\n",
    "    data[c] = lbe.fit_transform(data[c])\n",
    "    temp = pd.get_dummies(data[c], prefix=c)\n",
    "    data = pd.concat([data, temp], axis=1).drop(c, axis=1)\n",
    "\n",
    "    \n",
    "d1 = data[data['code']==127] # 충주\n",
    "d1.columns = [c+'_'+str(127) for c in d1.columns]\n",
    "d1 = d1.drop('code_127', axis=1).reset_index().drop('index', axis=1)\n",
    "\n",
    "d2 = data[data['code']==131] # 청주\n",
    "d2.columns = [c+'_'+str(131) for c in d2.columns]\n",
    "d2 = d2.drop('code_131', axis=1).reset_index().drop('index', axis=1)\n",
    "\n",
    "d3 = data[data['code']==232] # 천안\n",
    "d3.columns = [c+'_'+str(232) for c in d3.columns]\n",
    "d3 = d3.drop('code_232', axis=1).reset_index().drop('index', axis=1)\n",
    "\n",
    "data = pd.concat([d1, d2, d3], axis=1).iloc[:8760,:]\n",
    "\n",
    "for c in data.columns:\n",
    "    if data[c].std() == 0:\n",
    "        data = data.drop(c, axis=1)\n",
    "        \n",
    "tot_num = []\n",
    "for i in [127, 131, 232]:\n",
    "    for c in numerical:\n",
    "        tot_num.append(c+'_'+str(i))\n",
    "\n",
    "for c in tot_num:\n",
    "    if c in data.columns:\n",
    "        data[c] /= data[c].max()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 24\n",
    "tr_X = []\n",
    "tr_y = []\n",
    "val_X = []\n",
    "val_y = []\n",
    "\n",
    "y1 = y[:5136]\n",
    "for i in range(steps, len(y1)-24, 1):\n",
    "    tr_X.append(y1[i-steps:i])\n",
    "    tr_y.append(y1[i:i+24])\n",
    "\n",
    "val_X.append(tr_X[-1])\n",
    "val_y.append(tr_y[-1])\n",
    "tr_X.pop(-1)\n",
    "tr_y.pop(-1)\n",
    "    \n",
    "    \n",
    "y1 = y[5160:6576]\n",
    "for i in range(steps, len(y1)-24, 1):\n",
    "    tr_X.append(y1[i-steps:i])\n",
    "    tr_y.append(y1[i:i+24])\n",
    "    \n",
    "val_X.append(tr_X[-1])\n",
    "val_y.append(tr_y[-1])\n",
    "tr_X.pop(-1)\n",
    "tr_y.pop(-1)\n",
    "    \n",
    "    \n",
    "y1 = y[6600:8040]\n",
    "for i in range(steps, len(y1)-24, 1):\n",
    "    tr_X.append(y1[i-steps:i])\n",
    "    tr_y.append(y1[i:i+24])\n",
    "\n",
    "val_X.append(tr_X[-1])\n",
    "val_y.append(tr_y[-1])\n",
    "tr_X.pop(-1)\n",
    "tr_y.pop(-1)\n",
    "    \n",
    "    \n",
    "y1 = y[8064:8760]\n",
    "for i in range(steps, len(y1)-24, 1):\n",
    "    tr_X.append(y1[i-steps:i])\n",
    "    tr_y.append(y1[i:i+24])\n",
    "    \n",
    "val_X.append(tr_X[-1])\n",
    "val_y.append(tr_y[-1])\n",
    "tr_X.pop(-1)\n",
    "tr_y.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X = np.array(tr_X)\n",
    "tr_y = np.array(tr_y)\n",
    "val_X = np.array(val_X)\n",
    "val_y = np.array(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X.shape, tr_y.shape, val_X.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_X1 = y[5136-24:5160-24]\n",
    "te_X2 = y[6576-24:6600-24]\n",
    "te_X3 = y[8040:8064]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling(model, x):\n",
    "    X = list(x)\n",
    "    res = []\n",
    "    for i in range(24):\n",
    "        res.append(model.predict(np.expand_dims(X, 0)).flatten()[0])\n",
    "        X.append(res[-1])\n",
    "        X.pop(0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf.fit(tr_X, tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_loss(val_y, rf.predict(val_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_loss(val_y, rf.predict(val_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(val_y[:-1].flatten(), label='true')\n",
    "plt.plot(rf.predict(val_X)[:-1].flatten(), label='pred')\n",
    "plt.title('RF 24 validation result')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_X = np.expand_dims(tr_X, -1)\n",
    "ttr_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_block(filters,kernel_size,n):\n",
    "    def f(x):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters=filters,\n",
    "                    kernel_size=1, \n",
    "                    padding='same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters=filters,\n",
    "                    kernel_size=kernel_size, \n",
    "                    padding='same',\n",
    "                    activation = 'tanh',\n",
    "                    dilation_rate=dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters=filters,\n",
    "                    kernel_size=kernel_size, \n",
    "                    padding='same',\n",
    "                    activation = 'sigmoid',\n",
    "                    dilation_rate=dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out,sigm_out])\n",
    "            x = Conv1D(filters = filters,\n",
    "                           kernel_size = 1,\n",
    "                           padding='same')(x)\n",
    "            # x = BatchNormalization()(x)\n",
    "    #             x = SpatialDropout1D(0.2)(x)\n",
    "            res_x = Add()([res_x,x])\n",
    "        return res_x\n",
    "    return f\n",
    "\n",
    "def conv1d_block(x, fs, ks, activation=None):\n",
    "    x = Conv1D(fs, ks, padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_flat():\n",
    "    inputs = Input(shape = ttr_X.shape[1:])\n",
    "    x1 = conv1d_block(inputs, 16, 2, mish)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = conv1d_block(x1, 32, 2, mish)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = Bidirectional(LSTM(32, return_sequences=True))(x1)\n",
    "    x1 = Flatten()(x1)\n",
    "    # x1 = Attention(24)(x1)\n",
    "\n",
    "    x1_out = Dense(512, kernel_initializer='he_normal')(x1)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(256, kernel_initializer='he_normal')(x1_out)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(128, kernel_initializer='he_normal')(x1_out)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(1, kernel_initializer='he_normal')(x1_out)\n",
    "\n",
    "    x2 = conv1d_block(inputs, 16, 3, mish)\n",
    "    x2 = MaxPooling1D(2)(x2)\n",
    "    x2 = conv1d_block(x2, 32, 3, mish)\n",
    "    x2 = MaxPooling1D(2)(x2)\n",
    "    x2 = Bidirectional(LSTM(32, return_sequences=True))(x2)\n",
    "    x2 = Flatten()(x2)\n",
    "    # x2 = Attention(24)(x2)\n",
    "\n",
    "    x2_out = Dense(512, kernel_initializer='he_normal')(x2)\n",
    "    x2_out = Activation(mish)(x2_out)\n",
    "    x2_out = Dense(256, kernel_initializer='he_normal')(x2_out)\n",
    "    x2_out = Activation(mish)(x2_out)\n",
    "    x2_out = Dense(128, kernel_initializer='he_normal')(x2_out)\n",
    "    x2_out = Activation(mish)(x2_out)\n",
    "    x2_out = Dense(24)(x2_out)\n",
    "\n",
    "    x3 = conv1d_block(inputs, 16, 5, mish)\n",
    "    x3 = MaxPooling1D(2)(x3)\n",
    "    x3 = conv1d_block(x3, 32, 5, mish)\n",
    "    x3 = MaxPooling1D(2)(x3)\n",
    "    x3 = Bidirectional(LSTM(32, return_sequences=True))(x3)\n",
    "    x3 = Flatten()(x3)\n",
    "    # x3 = Attention(24)(x3)\n",
    "\n",
    "    x3_out = Dense(512, kernel_initializer='he_normal')(x3)\n",
    "    x3_out = Activation(mish)(x3_out)\n",
    "    x3_out = Dense(256, kernel_initializer='he_normal')(x3_out)\n",
    "    x3_out = Activation(mish)(x3_out)\n",
    "    x3_out = Dense(128, kernel_initializer='he_normal')(x3_out)\n",
    "    x3_out = Activation(mish)(x3_out)\n",
    "    x3_out = Dense(24)(x3_out)\n",
    "\n",
    "    x4 = conv1d_block(inputs, 16, 7, mish)\n",
    "    x4 = MaxPooling1D(2)(x4)\n",
    "    x4 = conv1d_block(x4, 32, 7, mish)\n",
    "    x4 = MaxPooling1D(2)(x4)\n",
    "    x4 = Bidirectional(LSTM(32, return_sequences=True))(x4)\n",
    "    x4 = Flatten()(x4)\n",
    "    # x4 = Attention(24)(x4)\n",
    "\n",
    "    x4_out = Dense(512, kernel_initializer='he_normal')(x4)\n",
    "    x4_out = Activation(mish)(x4_out)\n",
    "    x4_out = Dense(256, kernel_initializer='he_normal')(x4_out)\n",
    "    x4_out = Activation(mish)(x4_out)\n",
    "    x4_out = Dense(128, kernel_initializer='he_normal')(x4_out)\n",
    "    x4_out = Activation(mish)(x4_out)\n",
    "    x4_out = Dense(24)(x4_out)\n",
    "\n",
    "    x = Add()([x1, x2, x3, x4])\n",
    "    x = Dense(512, kernel_initializer='he_normal')(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = Dense(256, kernel_initializer='he_normal')(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = Dense(24)(x)\n",
    "\n",
    "    nn = Model(inputs, [x, x1_out,x2_out,x3_out, x4_out])\n",
    "    return nn\n",
    "\n",
    "\n",
    "def build_att():\n",
    "    inputs = Input(shape = ttr_X.shape[1:])\n",
    "    x1 = conv1d_block(inputs, 16, 2, mish)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = conv1d_block(x1, 32, 2, mish)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = Bidirectional(LSTM(32, return_sequences=True))(x1)\n",
    "#     x1 = Flatten()(x1)\n",
    "    x1 = Attention(6)(x1)\n",
    "\n",
    "    x1_out = Dense(64, kernel_initializer='he_normal')(x1)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(32, kernel_initializer='he_normal')(x1_out)\n",
    "    x1_out = Activation(mish)(x1_out)\n",
    "    x1_out = Dense(24, kernel_initializer='he_normal')(x1_out)\n",
    "\n",
    "    x2 = conv1d_block(inputs, 16, 3, mish)\n",
    "    x2 = MaxPooling1D(2)(x2)\n",
    "    x2 = conv1d_block(x2, 32, 3, mish)\n",
    "    x2 = MaxPooling1D(2)(x2)\n",
    "    x2 = Bidirectional(LSTM(32, return_sequences=True))(x2)\n",
    "#     x2 = Flatten()(x2)\n",
    "    x2 = Attention(6)(x2)\n",
    "\n",
    "    x2_out = Dense(64, kernel_initializer='he_normal')(x2)\n",
    "    x2_out = Activation(mish)(x2_out)\n",
    "    x2_out = Dense(32, kernel_initializer='he_normal')(x2_out)\n",
    "    x2_out = Activation(mish)(x2_out)\n",
    "    x2_out = Dense(24)(x2_out)\n",
    "\n",
    "    x3 = conv1d_block(inputs, 16, 5, mish)\n",
    "    x3 = MaxPooling1D(2)(x3)\n",
    "    x3 = conv1d_block(x3, 32, 5, mish)\n",
    "    x3 = MaxPooling1D(2)(x3)\n",
    "    x3 = Bidirectional(LSTM(32, return_sequences=True))(x3)\n",
    "#     x3 = Flatten()(x3)\n",
    "    x3 = Attention(6)(x3)\n",
    "\n",
    "    x3_out = Dense(64, kernel_initializer='he_normal')(x3)\n",
    "    x3_out = Activation(mish)(x3_out)\n",
    "    x3_out = Dense(32, kernel_initializer='he_normal')(x3_out)\n",
    "    x3_out = Activation(mish)(x3_out)\n",
    "    x3_out = Dense(24)(x3_out)\n",
    "\n",
    "    x4 = conv1d_block(inputs, 16, 7, mish)\n",
    "    x4 = MaxPooling1D(2)(x4)\n",
    "    x4 = conv1d_block(x4, 32, 7, mish)\n",
    "    x4 = MaxPooling1D(2)(x4)\n",
    "    x4 = Bidirectional(LSTM(32, return_sequences=True))(x4)\n",
    "#     x4 = Flatten()(x4)\n",
    "    x4 = Attention(6)(x4)\n",
    "\n",
    "    x4_out = Dense(64, kernel_initializer='he_normal')(x4)\n",
    "    x4_out = Activation(mish)(x4_out)\n",
    "    x4_out = Dense(32, kernel_initializer='he_normal')(x4_out)\n",
    "    x4_out = Activation(mish)(x4_out)\n",
    "    x4_out = Dense(24)(x4_out)\n",
    "\n",
    "    x = Add()([x1, x2, x3, x4])\n",
    "    x = Dense(64, kernel_initializer='he_normal')(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = Dense(32, kernel_initializer='he_normal')(x)\n",
    "    x = Activation(mish)(x)\n",
    "    x = Dense(24)(x)\n",
    "\n",
    "    nn = Model(inputs, [x, x1_out,x2_out,x3_out, x4_out])\n",
    "    return nn\n",
    "\n",
    "def build_wav():\n",
    "    inputs = Input(shape = ttr_X.shape[1:])\n",
    "    x1 = wave_block(16, 3, 5)(inputs)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = wave_block(32, 3, 3)(x1)\n",
    "    x1 = MaxPooling1D(2)(x1)\n",
    "    x1 = Bidirectional(LSTM(32, return_sequences=True))(x1)\n",
    "    x1 = Attention(6)(x1)\n",
    "\n",
    "    x = Dense(64, kernel_initializer='he_normal')(x1)\n",
    "    x = Activation(mish)(x)\n",
    "    x = Dense(24)(x)\n",
    "\n",
    "    nn = Model(inputs, x)\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = build_att()\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.compile(loss=custom_loss_nn, optimizer='adam')\n",
    "nn.fit(ttr_X/113, [tr_y/113]*5, # [tr_y/113]*5\n",
    "      epochs=200,\n",
    "       validation_split=0.2,\n",
    "#        batch_size=32,\n",
    "       shuffle=True,\n",
    "      callbacks=[es, lrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_loss(val_y[:-1], np.clip((nn.predict(np.expand_dims(val_X/113, -1)))[0][:-1].flatten()*113, 0, 113))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_y[:-1].flatten(), label='true')\n",
    "plt.plot(np.clip((nn.predict(np.expand_dims(val_X/113, -1)))[0][:-1].flatten()*113, 0, 113), label='pred')\n",
    "plt.title('NN 24 validation result')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
