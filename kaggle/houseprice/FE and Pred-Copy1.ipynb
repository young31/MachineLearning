{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling 해보기\n",
    "adb + voting + stk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import norm, skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81) (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "training = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "sub = pd.read_csv('./data/sample_submission.csv')\n",
    "# making copies of original datasets for rest of this kernel\n",
    "df_train = training.copy()\n",
    "df_test = test.copy()\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (1460, 80), Target: (1460,), Test: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "target = df_train['SalePrice']  #target variable\n",
    "df_train = df_train.drop('SalePrice', axis=1) \n",
    "\n",
    "print(\"Training: {}, Target: {}, Test: {}\".format(df_train.shape, target.shape, df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tr = np.log1p(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def house_pipeline_v1(dataframe,\n",
    "                      impute_method = \"median\",\n",
    "                      feature_transform = \"yes\",\n",
    "                      feature_scaling = \"RobustScaler\", \n",
    "                      feature_selection = \"yes\"):\n",
    "    # 0. initialising dataframe\n",
    "    df_pipe = dataframe.copy()\n",
    "    print(\"Dataframe loaded.\")\n",
    "    \n",
    "    # Drop redundant columns\n",
    "    df_pipe.drop(['Id'], axis=1, inplace=True) # drop Id column\n",
    "    print(\"Dropped redundant column 'Id'.\")\n",
    "\n",
    "    # column types variables\n",
    "    numeric_features = list(df_pipe.select_dtypes(\n",
    "        include=[np.number]).columns.values)\n",
    "    categ_features = list(df_pipe.select_dtypes(\n",
    "        include=['object']).columns.values)\n",
    "    for col in numeric_features:\n",
    "        df_pipe[col] = df_pipe[col].astype(float)\n",
    "\n",
    "    # 1. Handling missing values\n",
    "    # replacing NaNs in categorical features with \"None\"\n",
    "    df_pipe[categ_features] = df_pipe[categ_features].apply(\n",
    "        lambda x: x.fillna(\"None\"), axis=0)\n",
    "\n",
    "    # imputing numerical features\n",
    "    for col in (\"LotFrontage\", 'GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "        df_pipe[col].fillna(0.0, inplace=True)\n",
    "        \n",
    "    if impute_method == \"median\": # replacing NaNs in numerical features with the median\n",
    "        df_pipe[numeric_features] = df_pipe[numeric_features].apply(\n",
    "            lambda x: x.fillna(x.median()), axis=0)\n",
    "        print(\"Missing values imputed with median.\")\n",
    "    \n",
    "    elif impute_method == \"mean\": # replacing NaNs in numerical features with the mean\n",
    "        df_pipe[numeric_features] = df_pipe[numeric_features].apply(\n",
    "            lambda x: x.fillna(x.mean()), axis=0)\n",
    "        print(\"Missing values imputed with mean.\")\n",
    "\n",
    "    # 2. Feature Engineering\n",
    "    # Examples: Discretize Continous Feature;\n",
    "    #           Decompose Features;\n",
    "    #           Add Combination of Feature\n",
    "    df_pipe['YrBltAndRemod']=df_pipe['YearBuilt']+df_pipe['YearRemodAdd']\n",
    "    df_pipe['TotalSF']=df_pipe['TotalBsmtSF'] + df_pipe['1stFlrSF'] + df_pipe['2ndFlrSF']\n",
    "\n",
    "    df_pipe['Total_sqr_footage'] = (df_pipe['BsmtFinSF1'] + df_pipe['BsmtFinSF2'] +\n",
    "                                     df_pipe['1stFlrSF'] + df_pipe['2ndFlrSF'])\n",
    "\n",
    "    df_pipe['Total_Bathrooms'] = (df_pipe['FullBath'] + (0.5 * df_pipe['HalfBath']) +\n",
    "                                   df_pipe['BsmtFullBath'] + (0.5 * df_pipe['BsmtHalfBath']))\n",
    "\n",
    "    df_pipe['Total_porch_sf'] = (df_pipe['OpenPorchSF'] + df_pipe['3SsnPorch'] +\n",
    "                                  df_pipe['EnclosedPorch'] + df_pipe['ScreenPorch'] + \n",
    "                                 df_pipe['WoodDeckSF'])\n",
    "    print(\"Feature enginering: added combination of features.\")\n",
    "    \n",
    "    df_pipe['haspool'] = df_pipe['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df_pipe['has2ndfloor'] = df_pipe['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df_pipe['hasgarage'] = df_pipe['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df_pipe['hasbsmt'] = df_pipe['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df_pipe['hasfireplace'] = df_pipe['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    print(\"Feature enginering: added boolean features.\")\n",
    "    \n",
    "    # 3. Feature Transformations (log(x), sqrt(x), x^2, etc.)\n",
    "    # Transform numerical features that should be considered as strings \n",
    "    df_pipe['MSSubClass'] = df_pipe['MSSubClass'].apply(str)\n",
    "    df_pipe['YrSold'] = df_pipe['YrSold'].astype(str)\n",
    "    df_pipe['MoSold'] = df_pipe['MoSold'].astype(str)\n",
    "    df_pipe['YrBltAndRemod'] = df_pipe['YrBltAndRemod'].astype(str)\n",
    "    print(\"Transformed numerical features that should be considered as strings.\")\n",
    "    \n",
    "    numeric_features = list(df_pipe.select_dtypes(\n",
    "        include=[np.number]).columns.values)\n",
    "    categ_features = list(df_pipe.select_dtypes(\n",
    "        include=['object']).columns.values)\n",
    "    \n",
    "    if feature_transform == \"yes\":\n",
    "        # Transform all numerical columns with skewness factor > 0.5\n",
    "        skew_features = df_pipe[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "        high_skew = skew_features[skew_features > 0.5]\n",
    "        skew_index = high_skew.index\n",
    "        for i in skew_index:\n",
    "            df_pipe[i] = boxcox1p(df_pipe[i], boxcox_normmax(df_pipe[i]+1))\n",
    "        print(\"Transformed numerical columns with high skewness factor.\")\n",
    "    elif feature_transform == \"no\":\n",
    "        pass\n",
    "\n",
    "    # 4. Label Encoding\n",
    "    df_pipe = pd.get_dummies(df_pipe)\n",
    "    print(\"Label Encoding: from {} cols to {} cols.\".format(\n",
    "        dataframe.shape[1], df_pipe.shape[1]))\n",
    "\n",
    "    # 5. Feature Scaling\n",
    "    #cols = df_pipe.select_dtypes([np.number]).columns\n",
    "    if feature_scaling == 'MinMaxScaler':\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        for col in numeric_features:\n",
    "            df_pipe[[col]] = scaler.fit_transform(df_pipe[[col]])\n",
    "        print(\"Performed feature Scaling with MinMaxScaler.\")\n",
    "\n",
    "    elif feature_scaling == 'StandardScaler':\n",
    "        scaler = StandardScaler()\n",
    "        for col in numeric_features:\n",
    "            df_pipe[[col]] = scaler.fit_transform(df_pipe[[col]])\n",
    "        print(\"Performed feature Scaling with StandardScaler.\")\n",
    "\n",
    "    elif feature_scaling == \"RobustScaler\":\n",
    "        scaler = RobustScaler()\n",
    "        for col in numeric_features:\n",
    "            df_pipe[[col]] = scaler.fit_transform(df_pipe[[col]])\n",
    "        print(\"Performed feature Scaling with RobustScaler.\")\n",
    "    \n",
    "    # 6. Feature Selection\n",
    "    ## let's remove columns with little variance (to reduce overfitting)\n",
    "    overfit = []\n",
    "    for i in df_pipe.columns:\n",
    "        counts = df_pipe[i].value_counts()\n",
    "        zeros = counts.iloc[0]\n",
    "        if zeros / len(df_pipe) * 100 > 99.9: # the threshold is set at 99.9%\n",
    "            overfit.append(i)\n",
    "    overfit = list(overfit)\n",
    "    # let's make sure to keep data processing columns needed later on\n",
    "    try:\n",
    "        overfit.remove('Dataset_Train')\n",
    "        overfit.remove('Dataset_Test')\n",
    "    except:\n",
    "        pass\n",
    "    df_pipe.drop(overfit, axis=1, inplace=True)\n",
    "    print(\"To prevent overfitting, {} columns were removed.\".format(len(overfit)))\n",
    "    \n",
    "    ## Summary\n",
    "    print(\"Shape of transformed dataset: {} (original: {})\".format(df_pipe.shape, dataframe.shape))\n",
    "    return df_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_transf(target, \n",
    "                  transform=\"log\"):\n",
    "    \n",
    "    if transform == \"log\":\n",
    "        target_tranf = np.log1p(target)\n",
    "        print(\"Target feature transformed with natural logarithm.\")\n",
    "    \n",
    "    elif transform == \"sqrt\":\n",
    "        target_tranf = np.sqrt(target)\n",
    "        print(\"Target feature transformed with sqrt.\")\n",
    "    \n",
    "    elif transform == \"square\":\n",
    "        target_tranf = np.square(target)\n",
    "        print(\"Target feature transformed with square.\")\n",
    "    \n",
    "    print(\"Shape of transformed target: {}\".format(target_tr.shape))\n",
    "    return target_tranf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded.\n",
      "Dropped redundant column 'Id'.\n",
      "Missing values imputed with median.\n",
      "Feature enginering: added combination of features.\n",
      "Feature enginering: added boolean features.\n",
      "Transformed numerical features that should be considered as strings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\scipy\\stats\\stats.py:3399: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "c:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\scipy\\stats\\stats.py:3429: PearsonRNearConstantInputWarning: An input array is nearly constant; the computed correlation coefficent may be inaccurate.\n",
      "  warnings.warn(PearsonRNearConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed numerical columns with high skewness factor.\n",
      "Label Encoding: from 80 cols to 505 cols.\n",
      "Performed feature Scaling with RobustScaler.\n",
      "To prevent overfitting, 35 columns were removed.\n",
      "Shape of transformed dataset: (1460, 470) (original: (1460, 80))\n",
      "\n",
      "\n",
      "Target feature transformed with natural logarithm.\n",
      "Shape of transformed target: (1460,)\n"
     ]
    }
   ],
   "source": [
    "# Test pipeline\n",
    "df_train_test = house_pipeline_v1(df_train)\n",
    "print(\"\\n\")\n",
    "target_tr = target_transf(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [missing_ratio]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check that we no longer have any missing values\n",
    "perc_na = (df_train_test.isnull().sum()/len(df_train_test))*100\n",
    "ratio_na = perc_na.sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'missing_ratio' :ratio_na})\n",
    "missing_data = missing_data.drop(missing_data[missing_data.missing_ratio == 0].index)\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined Dataframe shape: (2919, 81)\n"
     ]
    }
   ],
   "source": [
    "df_train_pipeline = df_train.copy()\n",
    "df_test_pipeline = df_test.copy()\n",
    "# Concat dataframes\n",
    "df_train_pipeline[\"Dataset\"] = \"Train\"\n",
    "df_test_pipeline[\"Dataset\"] = \"Test\"\n",
    "# Concat dataframes\n",
    "df_joined = pd.concat([df_train_pipeline, df_test_pipeline], \n",
    "                      sort=False)\n",
    "df_joined = df_joined.reset_index(drop=True) # reset index\n",
    "print(\"Joined Dataframe shape: {}\".format(df_joined.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded.\n",
      "Dropped redundant column 'Id'.\n",
      "Missing values imputed with median.\n",
      "Feature enginering: added combination of features.\n",
      "Feature enginering: added boolean features.\n",
      "Transformed numerical features that should be considered as strings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\scipy\\stats\\stats.py:3399: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "c:\\users\\young\\anaconda3\\envs\\study\\lib\\site-packages\\scipy\\stats\\stats.py:3429: PearsonRNearConstantInputWarning: An input array is nearly constant; the computed correlation coefficent may be inaccurate.\n",
      "  warnings.warn(PearsonRNearConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed numerical columns with high skewness factor.\n",
      "Label Encoding: from 81 cols to 521 cols.\n",
      "Performed feature Scaling with RobustScaler.\n",
      "To prevent overfitting, 44 columns were removed.\n",
      "Shape of transformed dataset: (2919, 477) (original: (2919, 81))\n",
      "----\n",
      "\n",
      "Target feature transformed with natural logarithm.\n",
      "Shape of transformed target: (1460,)\n",
      "----\n",
      "\n",
      "Transformed Joined Dataframe shape: (2919, 477), and target shape: (1460,)\n"
     ]
    }
   ],
   "source": [
    "df_joined_ml = house_pipeline_v1(df_joined,\n",
    "                                 impute_method = \"median\",\n",
    "                                 feature_transform = \"yes\",\n",
    "                                 feature_scaling = \"RobustScaler\", \n",
    "                                 feature_selection = \"yes\")\n",
    "print(\"----\\n\")\n",
    "target_ml = target_transf(target)\n",
    "print(\"----\\n\")\n",
    "print(\"Transformed Joined Dataframe shape: {}, and target shape: {}\".format(\n",
    "    df_joined_ml.shape, target_ml.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 475)\n"
     ]
    }
   ],
   "source": [
    "df_train_ml = df_joined_ml[df_joined_ml['Dataset_Train']==1].copy()\n",
    "# Remove redundant features\n",
    "df_train_ml.drop(['Dataset_Train'], axis=1, inplace=True)\n",
    "df_train_ml.drop(['Dataset_Test'], axis=1, inplace=True)\n",
    "# Reset index\n",
    "df_train_ml = df_train_ml.reset_index(drop=True) \n",
    "print(df_train_ml.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 475)\n"
     ]
    }
   ],
   "source": [
    "# Extract Testing data from joined transformed dataset\n",
    "df_test_ml = df_joined_ml[df_joined_ml['Dataset_Test']==1].copy()\n",
    "# Remove redundant features\n",
    "df_test_ml.drop(['Dataset_Train'], axis=1, inplace=True)\n",
    "df_test_ml.drop(['Dataset_Test'], axis=1, inplace=True)\n",
    "# Reset index\n",
    "df_test_ml = df_test_ml.reset_index(drop=True)\n",
    "print(df_test_ml.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df_train_ml.copy()\n",
    "target = target_ml.copy()\n",
    "test_X = df_test_ml.copy()\n",
    "\n",
    "n, s = np.mean(target), np.std(target)\n",
    "t = (target - n)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train_ml,\n",
    "                                                    target_ml,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=df_train_ml['OverallQual'],\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 256\n",
    "n_node = 3\n",
    "\n",
    "inputs = layers.Input(shape =(train_X.shape[1], ))\n",
    "\n",
    "x = layers.Dense(n_layers)(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "for i in range(n_node):\n",
    "    x = layers.Dense(n_layers//(2**(i+1)), kernel_regularizer=L1L2(l2=0.001), kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "m = models.Model(inputs, outputs)\n",
    "\n",
    "m.compile(optimizer = 'adam',\n",
    "            loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.fit(train_X, t, \n",
    "      epochs = 1000,\n",
    "     validation_split=0.2,\n",
    "     callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = m.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = np.exp(k*s+n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n, s = np.mean(target), np.std(target)\n",
    "# t = (target - n)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['SalePrice'] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('./sub/sub19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=1200,\n",
    "                               max_depth=15,\n",
    "                               min_samples_split=5,\n",
    "                               min_samples_leaf=5,\n",
    "                               max_features=None,\n",
    "                               oob_score=True,\n",
    "                               random_state=42)\n",
    "gbr = GradientBoostingRegressor(n_estimators=6000,\n",
    "                                learning_rate=0.01,\n",
    "                                max_depth=4,\n",
    "                                max_features='sqrt',\n",
    "                                min_samples_leaf=15,\n",
    "                                min_samples_split=10,\n",
    "                                loss='huber',\n",
    "                                random_state=42)\n",
    "svr = SVR()\n",
    "\n",
    "adb = AdaBoostRegressor(n_estimators=200,\n",
    "                       base_estimator=gbr,\n",
    "                       learning_rate=0.01,\n",
    "                       random_state=42\n",
    "                       )\n",
    "\n",
    "elastic = ElasticNet()\n",
    "\n",
    "vreg = VotingRegressor(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('gbr', gbr),\n",
    "    ('svr', svr),\n",
    "    ('adb', adb)\n",
    "],  n_jobs=-1\n",
    ")\n",
    "\n",
    "stk_reg = StackingRegressor(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('gbr', gbr),\n",
    "    ('svr', svr),\n",
    "    ('adb', adb)\n",
    "],  n_jobs=-1,\n",
    "#     final_estimator=rf\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "models = [rf, gbr, svr, adb, elastic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vreg\n",
    "model.fit(train_X, t)\n",
    "k = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = np.exp(k*s+n)\n",
    "# ans = np.exp(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([126319.14983499, 161675.02186024, 186104.38965289, ...,\n",
       "       167509.10587277, 110719.61178836, 217486.54948876])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['SalePrice'] = ans\n",
    "sub.to_csv('./sub/sub32.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = stk_reg\n",
    "model.fit(train_X, t)\n",
    "k = model.predict(test_X)\n",
    "\n",
    "ans = np.exp(k*s+n)\n",
    "# ans = np.exp(k)\n",
    "\n",
    "sub['SalePrice'] = ans\n",
    "sub.to_csv('./sub/sub33.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([124369.07109839, 160267.63926504, 189588.89374567, ...,\n",
       "       166217.21538395, 110963.38934735, 216509.40098854])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
